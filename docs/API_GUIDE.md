# API ä½¿ç”¨æŒ‡å—

è¯¦ç»†çš„ API æ–‡æ¡£å’Œä»£ç ç¤ºä¾‹ã€‚

> âš ï¸ **æ³¨æ„**: æœ¬æ–‡æ¡£éƒ¨åˆ†å†…å®¹å¯èƒ½å¼•ç”¨äº†å·²ç§»é™¤çš„é«˜çº§åŠŸèƒ½æ¨¡å—ï¼ˆå¦‚ `StatisticalAnalyzer`, `ScriptGenerator`, `SensitivityAnalyzer`, `ProjectManager`, `ResponseValidator`, `SimulationEstimator`ï¼‰ã€‚è¿™äº›åŠŸèƒ½çš„ä»£ç å·²ä»é¡¹ç›®ä¸­åˆ é™¤ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰UIæ”¯æŒã€‚æœ¬æ–‡æ¡£å°†åœ¨åç»­ç‰ˆæœ¬ä¸­æ›´æ–°ã€‚å½“å‰ç‰ˆæœ¬çš„ç³»ç»Ÿä¸“æ³¨äºæ ¸å¿ƒä»¿çœŸåŠŸèƒ½å’Œå·²å®ç°UIçš„ç‰¹æ€§ã€‚

---

## ğŸ“‹ ç›®å½•

- [æ ¸å¿ƒæ¨¡å—](#æ ¸å¿ƒæ¨¡å—)
  - [LLM å®¢æˆ·ç«¯](#llm-å®¢æˆ·ç«¯)
  - [è™šæ‹Ÿäººç‰©ç®¡ç†](#è™šæ‹Ÿäººç‰©ç®¡ç†)
  - [æ¨¡æ‹Ÿå¼•æ“](#æ¨¡æ‹Ÿå¼•æ“)
  - [ç»“æœå­˜å‚¨](#ç»“æœå­˜å‚¨)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
  - [A/B æµ‹è¯•](#ab-æµ‹è¯•)
  - [~~æ•æ„Ÿæ€§åˆ†æ~~](#æ•æ„Ÿæ€§åˆ†æ) *(å·²ç§»é™¤)*
  - [çºµå‘ç ”ç©¶](#çºµå‘ç ”ç©¶)
  - [æ‰¹é‡äººç‰©ç”Ÿæˆ](#æ‰¹é‡äººç‰©ç”Ÿæˆ)
- [å·¥å…·å’Œè¾…åŠ©](#å·¥å…·å’Œè¾…åŠ©)
- [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)

---

## æ ¸å¿ƒæ¨¡å—

### LLM å®¢æˆ·ç«¯

#### åŒæ­¥å®¢æˆ·ç«¯

```python
from src import LMStudioClient

# åˆå§‹åŒ–æœ¬åœ°å®¢æˆ·ç«¯
client = LMStudioClient(
    base_url="http://127.0.0.1:1234/v1",
    model="local-model"
)

# æˆ–åˆå§‹åŒ– API å®¢æˆ·ç«¯
client = LMStudioClient(
    base_url="https://api.deepseek.com/v1",
    api_key="your-api-key",
    model="deepseek-chat"
)

# ç”Ÿæˆå“åº”
response = client.generate(
    prompt="æ‚¨å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
    temperature=0.7,
    max_tokens=300
)
print(response)

# æµ‹è¯•è¿æ¥
is_connected = client.test_connection()
print(f"è¿æ¥çŠ¶æ€: {is_connected}")

# è·å–å¯ç”¨æ¨¡å‹
models = client.list_models()
print(f"å¯ç”¨æ¨¡å‹: {models}")
```

#### å¼‚æ­¥å®¢æˆ·ç«¯

```python
import asyncio
from src import AsyncLLMClient

async def main():
    # åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncLLMClient(
        base_url="https://api.deepseek.com/v1",
        api_key="your-api-key",
        model="deepseek-chat"
    )
    
    # å•ä¸ªè¯·æ±‚
    response = await client.generate_async(
        prompt="æ‚¨çš„å¹´é¾„æ˜¯ï¼Ÿ",
        temperature=0.7
    )
    
    # æ‰¹é‡è¯·æ±‚
    prompts = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
    responses = await client.generate_batch(
        prompts=prompts,
        temperature=0.7,
        max_concurrent=5
    )
    
    for i, response in enumerate(responses):
        print(f"å“åº” {i+1}: {response}")

asyncio.run(main())
```

---

### è™šæ‹Ÿäººç‰©ç®¡ç†

#### Persona ç±»

```python
from src import Persona

# åˆ›å»ºè™šæ‹Ÿäººç‰©
persona = Persona(
    name="ææ˜",
    age=32,
    gender="ç”·",
    occupation="è½¯ä»¶å·¥ç¨‹å¸ˆ",
    background="åœ¨åŒ—äº¬ä¸€å®¶äº’è”ç½‘å…¬å¸å·¥ä½œï¼Œç»å¸¸åŠ ç­...",
    personality_traits=["å†…å‘", "å®Œç¾ä¸»ä¹‰", "è´£ä»»å¿ƒå¼º"],
    values=["èŒä¸šå‘å±•", "å·¥ä½œç”Ÿæ´»å¹³è¡¡"],
    education="æœ¬ç§‘è®¡ç®—æœºç§‘å­¦",
    location="åŒ—äº¬"
)

# è½¬æ¢ä¸ºå­—å…¸
persona_dict = persona.to_dict()

# ä»å­—å…¸åˆ›å»º
persona_from_dict = Persona.from_dict(persona_dict)

# ç”Ÿæˆæç¤ºä¸Šä¸‹æ–‡
context = persona.to_prompt_context(use_json_format=True)
```

#### PersonaManager ç±»

```python
from src import PersonaManager

# åˆå§‹åŒ–ç®¡ç†å™¨
manager = PersonaManager()

# æ·»åŠ äººç‰©
manager.add_persona(persona)

# æ‰¹é‡æ·»åŠ 
personas = [persona1, persona2, persona3]
for p in personas:
    manager.add_persona(p)

# è·å–æ‰€æœ‰äººç‰©
all_personas = manager.get_all_personas()
print(f"æ€»å…± {len(all_personas)} ä¸ªäººç‰©")

# è·å–å•ä¸ªäººç‰©
persona = manager.get_persona("ææ˜")

# åˆ é™¤äººç‰©
manager.remove_persona("ææ˜")

# æŒ‰æ¡ä»¶ç­›é€‰
young_adults = manager.filter_personas(
    age_range=(18, 30)
)

engineers = manager.filter_personas(
    occupation="å·¥ç¨‹å¸ˆ"
)

female_doctors = manager.filter_personas(
    gender="å¥³",
    occupation="åŒ»ç”Ÿ"
)

# ä¿å­˜å’ŒåŠ è½½
manager.save_to_file("data/personas/my_personas.json")
manager.load_from_file("data/personas/my_personas.json")

# å¯¼å‡ºä¸º CSV
manager.export_to_csv("data/personas/personas.csv")

# ä» CSV å¯¼å…¥
manager.import_from_csv("data/personas/personas.csv")

# æ¸…ç©ºæ‰€æœ‰äººç‰©
manager.clear_all()
```

---

### æ¨¡æ‹Ÿå¼•æ“

#### åŸºç¡€å¼•æ“ï¼ˆåŒæ­¥ï¼‰

```python
from src import SimulationEngine, LMStudioClient

# åˆå§‹åŒ–
client = LMStudioClient(...)
engine = SimulationEngine(llm_client=client)

# è¿è¡Œè°ƒæŸ¥
questions = [
    "æ‚¨çš„å¹´é¾„æ˜¯ï¼Ÿ",
    "æ‚¨çš„èŒä¸šæ˜¯ï¼Ÿ",
    "æ‚¨æ¯å‘¨å·¥ä½œå¤šå°‘å°æ—¶ï¼Ÿ"
]

result = engine.run_survey(
    personas=personas,
    questions=questions,
    temperature=0.7,
    max_tokens=200
)

# è®¿é—®ç»“æœ
print(f"æ¨¡æ‹Ÿç±»å‹: {result.simulation_type}")
print(f"å®Œæˆæ—¶é—´: {result.timestamp}")
print(f"å“åº”æ•°é‡: {len(result.persona_responses)}")

# éå†å“åº”
for response in result.persona_responses:
    print(f"{response['persona_name']}: {response['response']}")

# è½¬æ¢ä¸ºå­—å…¸
result_dict = result.to_dict()
```

#### å¹¶è¡Œå¼•æ“ï¼ˆå¼‚æ­¥ï¼‰

```python
import asyncio
from src import ParallelSimulationEngine, AsyncLLMClient

async def run_parallel_simulation():
    # åˆå§‹åŒ–
    client = AsyncLLMClient(...)
    engine = ParallelSimulationEngine(
        llm_client=client,
        max_concurrent=10  # æœ€å¤§å¹¶å‘æ•°
    )
    
    # è¿è¡Œè°ƒæŸ¥
    result = await engine.run_survey_async(
        personas=personas,
        questions=questions,
        temperature=0.7,
        max_tokens=200,
        progress_callback=lambda current, total: print(f"è¿›åº¦: {current}/{total}")
    )
    
    return result

# æ‰§è¡Œ
result = asyncio.run(run_parallel_simulation())
```

#### å¹²é¢„æ¨¡æ‹Ÿ

```python
# å®šä¹‰å¹²é¢„æ–‡æœ¬
intervention = """
ç ”ç©¶è¡¨æ˜ï¼Œæ¯å¤©è¿›è¡Œ10åˆ†é’Ÿçš„æ­£å¿µå†¥æƒ³å¯ä»¥ï¼š
- é™ä½30%çš„å‹åŠ›æ°´å¹³
- æ”¹å–„ç¡çœ è´¨é‡
- æå‡ä¸“æ³¨åŠ›
"""

# åç»­é—®é¢˜
followup_questions = [
    "æ‚¨ä¼šå°è¯•è¿™ä¸ªæ–¹æ³•å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ",
    "ä»€ä¹ˆå› ç´ å¯èƒ½é˜»æ­¢æ‚¨å°è¯•ï¼Ÿ",
    "å¦‚ä½•è°ƒæ•´è¿™ä¸ªå»ºè®®èƒ½è®©æ‚¨æ›´æ„¿æ„å°è¯•ï¼Ÿ"
]

# è¿è¡Œå¹²é¢„æ¨¡æ‹Ÿ
result = engine.run_intervention(
    personas=personas,
    intervention_text=intervention,
    questions=followup_questions,
    temperature=0.7
)
```

#### ä½¿ç”¨ç¼“å­˜

```python
from src import ResponseCache, SimulationEngine

# åˆå§‹åŒ–ç¼“å­˜
cache = ResponseCache(cache_dir="data/cache")

# åˆ›å»ºå¸¦ç¼“å­˜çš„å¼•æ“
engine = SimulationEngine(
    llm_client=client,
    cache=cache
)

# ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆè°ƒç”¨ LLMï¼‰
result1 = engine.run_survey(personas, questions)

# ç¬¬äºŒæ¬¡è¿è¡Œç›¸åŒé—®é¢˜ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œå¿«é€Ÿè¿”å›ï¼‰
result2 = engine.run_survey(personas, questions)

# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
stats = cache.get_stats()
print(f"å‘½ä¸­ç‡: {stats['hit_rate']:.2%}")
print(f"æ€»è¯·æ±‚: {stats['total_requests']}")

# æ¸…ç©ºç¼“å­˜
cache.clear_cache()
```

#### ä½¿ç”¨æ£€æŸ¥ç‚¹

```python
from src import CheckpointManager, SimulationEngine

# åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
checkpoint_mgr = CheckpointManager(checkpoint_dir="data/checkpoints")

# åˆ›å»ºå¸¦æ£€æŸ¥ç‚¹çš„å¼•æ“
engine = SimulationEngine(
    llm_client=client,
    checkpoint_manager=checkpoint_mgr
)

# è¿è¡Œå¤§è§„æ¨¡æ¨¡æ‹Ÿï¼ˆè‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
try:
    result = engine.run_survey(
        personas=large_persona_list,  # 100+ äººç‰©
        questions=long_question_list,  # 20+ é—®é¢˜
        save_checkpoints=True,
        checkpoint_interval=10  # æ¯10ä¸ªå“åº”ä¿å­˜ä¸€æ¬¡
    )
except KeyboardInterrupt:
    print("æ¨¡æ‹Ÿè¢«ä¸­æ–­")

# æ¢å¤è¢«ä¸­æ–­çš„æ¨¡æ‹Ÿ
result = engine.resume_from_checkpoint(checkpoint_id="simulation_123")
```

---

### ç»“æœå­˜å‚¨

```python
from src import ResultsStorage

# åˆå§‹åŒ–å­˜å‚¨
storage = ResultsStorage(results_dir="data/results")

# ä¿å­˜ç»“æœ
storage.save_result(result)

# åŠ è½½æ‰€æœ‰ç»“æœ
all_results = storage.load_all_results()

# åŠ è½½ç‰¹å®šç»“æœ
result = storage.load_result(result_id="20231028_123456")

# å¯¼å‡ºä¸º CSV
storage.export_to_csv(
    result=result,
    output_path="output.csv",
    include_persona_details=True
)

# å¯¼å‡ºä¸º JSON
storage.export_to_json(
    result=result,
    output_path="output.json",
    pretty=True
)

# åˆ é™¤ç»“æœ
storage.delete_result(result_id="20231028_123456")

# åˆ—å‡ºæ‰€æœ‰ç»“æœ
result_list = storage.list_results()
for r in result_list:
    print(f"{r['id']}: {r['timestamp']} - {r['type']}")
```

---

## é«˜çº§åŠŸèƒ½

### A/B æµ‹è¯•

```python
from src import ABTestManager, Condition, ABTestConfig

# å®šä¹‰æµ‹è¯•æ¡ä»¶
condition_a = Condition(
    name="ç‰ˆæœ¬A - ç®€å•ä¿¡æ¯",
    intervention_text="æ¯å¤©å†¥æƒ³10åˆ†é’Ÿå¯ä»¥é™ä½å‹åŠ›ã€‚",
    questions=["æ‚¨ä¼šå°è¯•è¿™ä¸ªæ–¹æ³•å—ï¼Ÿ", "ä¸ºä»€ä¹ˆï¼Ÿ"]
)

condition_b = Condition(
    name="ç‰ˆæœ¬B - æ•°æ®æ”¯æŒ",
    intervention_text="ç ”ç©¶è¡¨æ˜ï¼Œæ¯å¤©å†¥æƒ³10åˆ†é’Ÿå¯ä»¥é™ä½30%çš„å‹åŠ›æ°´å¹³ã€‚",
    questions=["æ‚¨ä¼šå°è¯•è¿™ä¸ªæ–¹æ³•å—ï¼Ÿ", "ä¸ºä»€ä¹ˆï¼Ÿ"]
)

# é…ç½®æµ‹è¯•
config = ABTestConfig(
    name="å†¥æƒ³ä¿¡æ¯ A/B æµ‹è¯•",
    description="æµ‹è¯•ä¸åŒä¿¡æ¯æ¡†æ¶çš„æ•ˆæœ",
    randomize_assignment=True,  # éšæœºåˆ†é…äººç‰©
    balance_demographics=True   # å¹³è¡¡äººå£ç»Ÿè®¡ç‰¹å¾
)

# è¿è¡Œæµ‹è¯•
manager = ABTestManager(llm_client=client)
results = manager.run_test(
    conditions=[condition_a, condition_b],
    personas=personas,
    config=config
)

# åˆ†æç»“æœ
analysis = manager.analyze_results(results)
print(f"ç‰ˆæœ¬Aæ¥å—ç‡: {analysis['condition_a']['acceptance_rate']:.2%}")
print(f"ç‰ˆæœ¬Bæ¥å—ç‡: {analysis['condition_b']['acceptance_rate']:.2%}")
print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p = {analysis['p_value']:.4f}")

# å¯¼å‡ºæŠ¥å‘Š
manager.export_report(results, "ab_test_report.html")
```

### æ•æ„Ÿæ€§åˆ†æ

```python
from src import SensitivityAnalyzer

analyzer = SensitivityAnalyzer(llm_client=client)

# åˆ†ææ¸©åº¦å‚æ•°çš„å½±å“
temp_results = analyzer.analyze_temperature(
    personas=personas,
    questions=questions,
    temperatures=[0.0, 0.3, 0.5, 0.7, 0.9, 1.0]
)

# åˆ†ææœ€å¤§ä»¤ç‰Œæ•°çš„å½±å“
token_results = analyzer.analyze_max_tokens(
    personas=personas,
    questions=questions,
    max_tokens_values=[100, 200, 300, 500, 1000]
)

# åˆ†ææç¤ºè¯å˜åŒ–çš„å½±å“
prompt_variants = [
    "è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}",
    "ä½œä¸º {name}ï¼Œè¯·å›ç­”ï¼š{question}",
    "æ ¹æ®ä½ çš„èƒŒæ™¯å’Œç»å†ï¼Œå›ç­”ï¼š{question}"
]

prompt_results = analyzer.analyze_prompt_variants(
    personas=personas,
    questions=questions,
    prompt_templates=prompt_variants
)

# å¯è§†åŒ–ç»“æœ
analyzer.plot_sensitivity_results(temp_results, "temperature_sensitivity.png")
```

### çºµå‘ç ”ç©¶

```python
from src import (
    InterventionStudyBuilder,
    InterventionWave,
    InterventionStudyManager
)

# æ„å»ºç ”ç©¶è®¾è®¡
study = (InterventionStudyBuilder()
    # ç¬¬ä¸€æ³¢ï¼šåŸºçº¿è°ƒæŸ¥
    .add_wave(InterventionWave(
        name="åŸºçº¿",
        survey_config=baseline_survey,
        description="å¹²é¢„å‰çš„åŸºçº¿æµ‹é‡"
    ))
    
    # ç¬¬äºŒæ³¢ï¼šå¹²é¢„
    .add_wave(InterventionWave(
        name="å¹²é¢„å®æ–½",
        intervention_text="è¯·æ¯å¤©è¿›è¡Œ10åˆ†é’Ÿå†¥æƒ³ï¼ŒæŒç»­30å¤©",
        description="å¹²é¢„æªæ–½ä»‹ç»"
    ))
    
    # ç¬¬ä¸‰æ³¢ï¼š1ä¸ªæœˆåéšè®¿
    .add_wave(InterventionWave(
        name="1ä¸ªæœˆéšè®¿",
        survey_config=followup_survey,
        description="å¹²é¢„å1ä¸ªæœˆçš„æ•ˆæœè¯„ä¼°"
    ))
    
    # ç¬¬å››æ³¢ï¼š3ä¸ªæœˆåéšè®¿
    .add_wave(InterventionWave(
        name="3ä¸ªæœˆéšè®¿",
        survey_config=followup_survey,
        description="å¹²é¢„å3ä¸ªæœˆçš„æ•ˆæœè¯„ä¼°"
    ))
    
    .build()
)

# è¿è¡Œç ”ç©¶
manager = InterventionStudyManager(
    study=study,
    llm_client=client
)

results = manager.run_study(
    personas=personas,
    save_checkpoints=True
)

# åˆ†æçºµå‘å˜åŒ–
analysis = manager.analyze_longitudinal_changes(results)
print(f"å¹³å‡æ”¹å–„ç‡: {analysis['mean_improvement']:.2%}")

# å¯è§†åŒ–è¶‹åŠ¿
manager.plot_trends(results, "longitudinal_trends.png")
```

### æ‰¹é‡äººç‰©ç”Ÿæˆ

```python
from src import PersonaGenerator, DistributionConfig

# é…ç½®äººå£ç»Ÿè®¡åˆ†å¸ƒ
distribution = DistributionConfig(
    age_distribution={
        "18-30": 0.25,
        "31-50": 0.40,
        "51-70": 0.30,
        "71+": 0.05
    },
    gender_distribution={
        "ç”·": 0.48,
        "å¥³": 0.51,
        "å…¶ä»–": 0.01
    },
    education_distribution={
        "é«˜ä¸­": 0.30,
        "æœ¬ç§‘": 0.50,
        "ç ”ç©¶ç”Ÿ": 0.20
    },
    occupation_categories=[
        "å­¦ç”Ÿ", "æ•™å¸ˆ", "åŒ»ç”Ÿ", "æŠ¤å£«", "å·¥ç¨‹å¸ˆ",
        "é”€å”®", "ç®¡ç†", "æœåŠ¡ä¸š", "é€€ä¼‘", "å…¶ä»–"
    ]
)

# åˆå§‹åŒ–ç”Ÿæˆå™¨
generator = PersonaGenerator(llm_client=client)

# ç”Ÿæˆå•ä¸ªäººç‰©
persona = generator.generate_persona(
    age_range=(25, 35),
    gender="å¥³",
    occupation="åŒ»ç”Ÿ"
)

# æ‰¹é‡ç”Ÿæˆ
personas = generator.generate_batch(
    count=100,
    distribution_config=distribution,
    diversity_factor=0.8  # 0-1ï¼Œè¶Šé«˜è¶Šå¤šæ ·åŒ–
)

# ç”Ÿæˆå…·æœ‰ç‰¹å®šç‰¹å¾çš„äººç‰©
personas = generator.generate_with_constraints(
    count=50,
    constraints={
        "chronic_condition": True,  # æœ‰æ…¢æ€§ç–¾ç—…
        "income_level": "low",      # ä½æ”¶å…¥
        "urban": True               # åŸå¸‚å±…æ°‘
    }
)

# ä¿å­˜ç”Ÿæˆçš„äººç‰©
manager = PersonaManager()
for p in personas:
    manager.add_persona(p)
manager.save_to_file("generated_personas.json")
```

---

## å·¥å…·å’Œè¾…åŠ©

### é—®å·æ¨¡æ¿

```python
from src import SurveyTemplateLibrary, SurveyTemplate

# è·å–æ¨¡æ¿åº“
library = SurveyTemplateLibrary()

# åˆ—å‡ºæ‰€æœ‰æ¨¡æ¿
templates = library.list_templates()
for t in templates:
    print(f"{t['id']}: {t['name']}")

# è·å–ç‰¹å®šæ¨¡æ¿
phq9 = library.get_template("phq-9")

# ä½¿ç”¨æ¨¡æ¿
questions = phq9.get_questions()
scoring_rules = phq9.get_scoring_rules()

# åˆ›å»ºè‡ªå®šä¹‰æ¨¡æ¿
custom_template = SurveyTemplate(
    id="my-survey",
    name="æˆ‘çš„è‡ªå®šä¹‰é—®å·",
    sections=[
        {
            "name": "åŸºæœ¬ä¿¡æ¯",
            "questions": ["å¹´é¾„ï¼Ÿ", "èŒä¸šï¼Ÿ"]
        },
        {
            "name": "å¥åº·çŠ¶å†µ",
            "questions": ["æ‚¨çš„æ•´ä½“å¥åº·çŠ¶å†µå¦‚ä½•ï¼Ÿ"]
        }
    ]
)

# ä¿å­˜æ¨¡æ¿
library.add_template(custom_template)
library.save_to_file("data/survey_configs/custom_templates.json")
```

### è‡ªåŠ¨è¯„åˆ†

```python
from src import SurveyScorer

# åˆå§‹åŒ–è¯„åˆ†å™¨
scorer = SurveyScorer()

# åŠ è½½è¯„åˆ†è§„åˆ™
scorer.load_scoring_rules("phq-9")

# è¯„åˆ†å•ä¸ªå“åº”
response = {
    "question": "è¿‡å»ä¸¤å‘¨ï¼Œæ‚¨æœ‰å¤šå°‘å¤©æ„Ÿåˆ°å¿ƒæƒ…ä½è½ï¼Ÿ",
    "answer": "å‡ ä¹æ¯å¤©"
}
score = scorer.score_response(response)

# è¯„åˆ†æ•´ä¸ªé—®å·
responses = [
    {"question": "...", "answer": "..."},
    # ... æ›´å¤šå“åº”
]
total_score = scorer.score_survey(responses)
print(f"PHQ-9 æ€»åˆ†: {total_score}")

# è§£é‡Šåˆ†æ•°
interpretation = scorer.interpret_score(total_score, scale="phq-9")
print(f"è§£é‡Š: {interpretation}")
# è¾“å‡º: "è½»åº¦æŠ‘éƒ" æˆ– "ä¸­åº¦æŠ‘éƒ" ç­‰
```

### å“åº”éªŒè¯

```python
from src import ResponseValidator, ConsistencyChecker

# éªŒè¯å™¨
validator = ResponseValidator()

# éªŒè¯å•ä¸ªå“åº”
is_valid = validator.validate_response(
    response="æˆ‘ä»Šå¹´28å²",
    question_type="numeric",
    expected_range=(0, 120)
)

# éªŒè¯å“åº”æ ¼å¼
is_valid_format = validator.validate_format(
    response="A",
    allowed_values=["A", "B", "C", "D"]
)

# ä¸€è‡´æ€§æ£€æŸ¥
checker = ConsistencyChecker()

# æ£€æŸ¥äººç‰©çš„æ‰€æœ‰å“åº”æ˜¯å¦ä¸€è‡´
metrics = checker.check_consistency(persona_responses)
print(f"ä¸€è‡´æ€§å¾—åˆ†: {metrics.consistency_score:.2f}")
print(f"çŸ›ç›¾ç‚¹æ•°é‡: {metrics.num_contradictions}")

# æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§
is_consistent = checker.check_logical_consistency(
    responses={
        "age": "28å²",
        "has_children": "æ˜¯",
        "num_children": "3ä¸ª",
        "oldest_child_age": "35å²"  # çŸ›ç›¾ï¼
    }
)
```

### ç»Ÿè®¡åˆ†æ

```python
from src import StatisticalAnalyzer

# åˆå§‹åŒ–åˆ†æå™¨
analyzer = StatisticalAnalyzer()

# æè¿°ç»Ÿè®¡
desc_stats = analyzer.describe(result)
print(f"å“åº”æ•°é‡: {desc_stats['count']}")
print(f"å¹³å‡å“åº”é•¿åº¦: {desc_stats['mean_length']:.1f} å­—ç¬¦")

# ç»„é—´æ¯”è¾ƒ
comparison = analyzer.compare_groups(
    result,
    group_by="gender",
    metric="score"
)
print(f"ç”·æ€§å¹³å‡åˆ†: {comparison['male']['mean']:.2f}")
print(f"å¥³æ€§å¹³å‡åˆ†: {comparison['female']['mean']:.2f}")
print(f"å·®å¼‚æ˜¾è‘—æ€§: p = {comparison['p_value']:.4f}")

# ç›¸å…³åˆ†æ
correlation = analyzer.correlate(
    result,
    var1="age",
    var2="score"
)
print(f"ç›¸å…³ç³»æ•°: r = {correlation['r']:.3f}")

# å›å½’åˆ†æ
regression = analyzer.regression(
    result,
    dependent_var="score",
    independent_vars=["age", "education", "income"]
)
print(f"RÂ²: {regression['r_squared']:.3f}")
```

### æ•°æ®å¯¼å‡º

```python
from src import ScriptGenerator

# ç”Ÿæˆ Python åˆ†æè„šæœ¬
generator = ScriptGenerator()

python_script = generator.generate_python_script(
    result=result,
    include_visualization=True,
    include_statistics=True
)

# ä¿å­˜è„šæœ¬
with open("analyze_results.py", "w") as f:
    f.write(python_script)

# ç”Ÿæˆ R è„šæœ¬
r_script = generator.generate_r_script(
    result=result,
    include_visualization=True
)

with open("analyze_results.R", "w") as f:
    f.write(r_script)

# ç”Ÿæˆ Jupyter Notebook
notebook = generator.generate_notebook(
    result=result,
    format="ipynb"
)

with open("analysis.ipynb", "w") as f:
    f.write(notebook)
```

---

## å®Œæ•´ç¤ºä¾‹

### ç«¯åˆ°ç«¯å·¥ä½œæµ

```python
import asyncio
from src import (
    AsyncLLMClient,
    PersonaManager,
    ParallelSimulationEngine,
    ResultsStorage,
    ResponseCache,
    StatisticalAnalyzer,
    SurveyTemplateLibrary
)

async def complete_workflow():
    # 1. åˆå§‹åŒ–ç»„ä»¶
    client = AsyncLLMClient(
        base_url="https://api.deepseek.com/v1",
        api_key="your-api-key",
        model="deepseek-chat"
    )
    
    cache = ResponseCache()
    storage = ResultsStorage()
    
    engine = ParallelSimulationEngine(
        llm_client=client,
        cache=cache,
        max_concurrent=10
    )
    
    # 2. åˆ›å»ºæˆ–åŠ è½½äººç‰©
    persona_manager = PersonaManager()
    persona_manager.load_from_file("data/personas/my_personas.json")
    personas = persona_manager.get_all_personas()
    
    # 3. é€‰æ‹©é—®å·æ¨¡æ¿
    library = SurveyTemplateLibrary()
    phq9 = library.get_template("phq-9")
    questions = phq9.get_questions()
    
    # 4. è¿è¡Œæ¨¡æ‹Ÿ
    print("å¼€å§‹æ¨¡æ‹Ÿ...")
    result = await engine.run_survey_async(
        personas=personas,
        questions=questions,
        temperature=0.7,
        max_tokens=200,
        progress_callback=lambda c, t: print(f"è¿›åº¦: {c}/{t}")
    )
    
    # 5. ä¿å­˜ç»“æœ
    storage.save_result(result)
    print(f"ç»“æœå·²ä¿å­˜: {result.timestamp}")
    
    # 6. åˆ†æç»“æœ
    analyzer = StatisticalAnalyzer()
    stats = analyzer.describe(result)
    print(f"\nç»Ÿè®¡æ‘˜è¦:")
    print(f"- å“åº”æ•°é‡: {stats['count']}")
    print(f"- å¹³å‡é•¿åº¦: {stats['mean_length']:.1f} å­—ç¬¦")
    
    # 7. å¯¼å‡ºæ•°æ®
    storage.export_to_csv(result, "results.csv")
    storage.export_to_json(result, "results.json")
    
    print("\nå·¥ä½œæµå®Œæˆï¼")
    return result

# è¿è¡Œ
result = asyncio.run(complete_workflow())
```

### é¡¹ç›®ç®¡ç†å™¨ç¤ºä¾‹

```python
from src import ProjectManager

# åˆ›å»ºæ–°é¡¹ç›®
project = ProjectManager.create_project(
    name="å¥åº·å¹²é¢„ç ”ç©¶",
    description="æµ‹è¯•å†¥æƒ³å¹²é¢„å¯¹å‹åŠ›çš„å½±å“",
    output_dir="projects/meditation_study"
)

# é…ç½®é¡¹ç›®
project.set_llm_config({
    "provider": "deepseek",
    "model": "deepseek-chat",
    "api_key": "your-api-key"
})

# æ·»åŠ äººç‰©
project.add_personas_from_file("personas.json")

# æ·»åŠ é—®å·é…ç½®
project.add_survey_config("baseline", baseline_questions)
project.add_survey_config("followup", followup_questions)

# è¿è¡Œæ¨¡æ‹Ÿ
result = project.run_simulation(
    simulation_type="intervention",
    personas="all",
    intervention_text=intervention,
    questions="followup"
)

# é¡¹ç›®ä¼šè‡ªåŠ¨ä¿å­˜æ‰€æœ‰é…ç½®ã€ç»“æœå’Œåˆ†æ

# å¯¼å‡ºæ•´ä¸ªé¡¹ç›®
project.export_project("meditation_study.zip")

# åŠ è½½å·²æœ‰é¡¹ç›®
loaded_project = ProjectManager.load_project("projects/meditation_study")
```

---

## ğŸ“Œ æœ€ä½³å®è·µ

### é”™è¯¯å¤„ç†

```python
from src import SimulationEngine
from openai import APIError, RateLimitError

try:
    result = engine.run_survey(personas, questions)
except RateLimitError:
    print("API é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•")
except APIError as e:
    print(f"API é”™è¯¯: {e}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

### æ€§èƒ½ä¼˜åŒ–

```python
# ä½¿ç”¨å¼‚æ­¥å¼•æ“å¤„ç†å¤§è§„æ¨¡æ¨¡æ‹Ÿ
engine = ParallelSimulationEngine(
    llm_client=async_client,
    max_concurrent=15,  # æ ¹æ® API é™åˆ¶è°ƒæ•´
    retry_on_failure=True,
    max_retries=3
)

# å¯ç”¨ç¼“å­˜
cache = ResponseCache(cache_dir="data/cache")
engine.set_cache(cache)

# ä½¿ç”¨æ£€æŸ¥ç‚¹ä¿æŠ¤é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
checkpoint_mgr = CheckpointManager()
engine.set_checkpoint_manager(checkpoint_mgr)
```

### æ—¥å¿—è®°å½•

```python
from src import setup_logging, get_logger

# é…ç½®æ—¥å¿—
setup_logging(
    level="INFO",
    log_file="logs/simulation.log",
    console=True
)

# ä½¿ç”¨æ—¥å¿—
logger = get_logger(__name__)
logger.info("å¼€å§‹æ¨¡æ‹Ÿ")
logger.warning("ç¼“å­˜æœªå‘½ä¸­")
logger.error("API è°ƒç”¨å¤±è´¥")
```

---

## ğŸ“ è·å–å¸®åŠ©

- ğŸ“– æŸ¥çœ‹ [README.md](README.md) äº†è§£æ¦‚è¿°
- ğŸš€ æŸ¥çœ‹ [QUICKSTART.md](QUICKSTART.md) å¿«é€Ÿä¸Šæ‰‹
- ğŸ› [æŠ¥å‘Šé—®é¢˜](https://github.com/jason-jj-li/auto_sim_ai/issues)

---

**Happy Coding! ğŸš€**
