# é¡¹ç›®æ¶æ„æ–‡æ¡£

LLM Simulation Survey System çš„è¯¦ç»†æ¶æ„è®¾è®¡å’ŒæŠ€æœ¯è¯´æ˜ã€‚

---

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿæ¦‚è§ˆ](#ç³»ç»Ÿæ¦‚è§ˆ)
- [æŠ€æœ¯æ ˆ](#æŠ€æœ¯æ ˆ)
- [æ ¸å¿ƒæ¶æ„](#æ ¸å¿ƒæ¶æ„)
- [æ¨¡å—è®¾è®¡](#æ¨¡å—è®¾è®¡)
- [æ•°æ®æµ](#æ•°æ®æµ)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æ‰©å±•æ€§](#æ‰©å±•æ€§)

---

## ğŸ›ï¸ ç³»ç»Ÿæ¦‚è§ˆ

### è®¾è®¡ç†å¿µ

æœ¬ç³»ç»Ÿé‡‡ç”¨**æ¨¡å—åŒ–ã€å¯æ‰©å±•ã€å¼‚æ­¥ä¼˜å…ˆ**çš„æ¶æ„è®¾è®¡ï¼Œæ ¸å¿ƒç›®æ ‡ï¼š

1. **æ˜“ç”¨æ€§** - Streamlit UIï¼Œé›¶é…ç½®å¯åŠ¨
2. **çµæ´»æ€§** - æ”¯æŒå¤šç§ LLM æä¾›å•†ï¼Œå¯æ’æ‹”ç»„ä»¶
3. **æ€§èƒ½** - å¼‚æ­¥å¹¶å‘ï¼Œæ™ºèƒ½ç¼“å­˜
4. **å¯é æ€§** - æ£€æŸ¥ç‚¹æ¢å¤ï¼Œé”™è¯¯é‡è¯•
5. **å¯æ‰©å±•** - å·¥å…·ç³»ç»Ÿï¼Œæ’ä»¶æ¶æ„

### ç³»ç»Ÿåˆ†å±‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Presentation Layer (UI)         â”‚  Streamlit Web App
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Application Layer               â”‚  Business Logic
â”‚  - Simulation Engine                â”‚
â”‚  - Analysis & Scoring               â”‚
â”‚  - A/B Testing                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Domain Layer                    â”‚  Core Models
â”‚  - Persona                          â”‚
â”‚  - Survey Config                    â”‚
â”‚  - Results                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Infrastructure Layer            â”‚  External Services
â”‚  - LLM Client                       â”‚
â”‚  - Storage                          â”‚
â”‚  - Cache                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ æŠ€æœ¯æ ˆ

### å‰ç«¯

- **Streamlit** 1.32.0 - Web UI æ¡†æ¶
- **Plotly** 5.18.0 - äº¤äº’å¼å¯è§†åŒ–
- **Matplotlib/Seaborn** - é™æ€å›¾è¡¨

### åç«¯

- **Python** 3.8+ - ä¸»è¯­è¨€
- **asyncio** - å¼‚æ­¥ç¼–ç¨‹
- **aiohttp** - å¼‚æ­¥ HTTP å®¢æˆ·ç«¯

### æ•°æ®å¤„ç†

- **Pandas** 2.2.0 - æ•°æ®åˆ†æ
- **NumPy** 1.26.4 - æ•°å€¼è®¡ç®—
- **SciPy** 1.12.0 - ç§‘å­¦è®¡ç®—

### LLM é›†æˆ

- **OpenAI SDK** 1.30.0 - API å®¢æˆ·ç«¯ï¼ˆå…¼å®¹å¤šæä¾›å•†ï¼‰
- **httpx** 0.25.0 - HTTP æ”¯æŒ

### å­˜å‚¨

- **JSON** - é…ç½®å’Œç»“æœå­˜å‚¨
- **CSV** - æ•°æ®å¯¼å‡º
- **SQLite** - ç¼“å­˜å­˜å‚¨ï¼ˆå¯é€‰ï¼‰

### æµ‹è¯•

- **pytest** - æµ‹è¯•æ¡†æ¶
- **pytest-asyncio** - å¼‚æ­¥æµ‹è¯•
- **pytest-cov** - è¦†ç›–ç‡æŠ¥å‘Š

### ä»£ç è´¨é‡

- **black** - ä»£ç æ ¼å¼åŒ–
- **isort** - å¯¼å…¥æ’åº
- **mypy** - ç±»å‹æ£€æŸ¥
- **pylint** - ä»£ç æ£€æŸ¥

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„

### ç›®å½•ç»“æ„è¯¦è§£

```
auto_sim_ai/
â”‚
â”œâ”€â”€ app.py                          # ä¸»åº”ç”¨å…¥å£
â”‚
â”œâ”€â”€ pages/                          # Streamlit å¤šé¡µé¢
â”‚   â”œâ”€â”€ 1_Setup.py                 # äººç‰©ç®¡ç†é¡µé¢
â”‚   â”œâ”€â”€ 2_Simulation.py            # æ¨¡æ‹Ÿæ‰§è¡Œé¡µé¢
â”‚   â””â”€â”€ 3_Results.py               # ç»“æœåˆ†æé¡µé¢
â”‚
â”œâ”€â”€ src/                            # æ ¸å¿ƒæºä»£ç 
â”‚   â”‚
â”‚   â”œâ”€â”€ __init__.py                # æ¨¡å—å¯¼å‡º
â”‚   â”‚
â”‚   â”œâ”€â”€ llm_client.py              # LLM å®¢æˆ·ç«¯
â”‚   â”‚   â”œâ”€â”€ LMStudioClient         # åŒæ­¥å®¢æˆ·ç«¯
â”‚   â”‚   â””â”€â”€ AsyncLLMClient         # å¼‚æ­¥å®¢æˆ·ç«¯
â”‚   â”‚
â”‚   â”œâ”€â”€ persona.py                 # äººç‰©æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ Persona                # äººç‰©æ•°æ®ç±»
â”‚   â”‚   â””â”€â”€ PersonaManager         # äººç‰©ç®¡ç†å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ simulation.py              # æ¨¡æ‹Ÿå¼•æ“
â”‚   â”‚   â”œâ”€â”€ SimulationResult       # ç»“æœå®¹å™¨
â”‚   â”‚   â”œâ”€â”€ SimulationEngine       # åŒæ­¥å¼•æ“
â”‚   â”‚   â””â”€â”€ ParallelSimulationEngine # å¼‚æ­¥å¹¶è¡Œå¼•æ“
â”‚   â”‚
â”‚   â”œâ”€â”€ storage.py                 # æ•°æ®å­˜å‚¨
â”‚   â”‚   â””â”€â”€ ResultsStorage         # ç»“æœå­˜å‚¨ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ cache.py                   # ç¼“å­˜ç³»ç»Ÿ
â”‚   â”‚   â””â”€â”€ ResponseCache          # å“åº”ç¼“å­˜
â”‚   â”‚
â”‚   â”œâ”€â”€ checkpoint.py              # æ£€æŸ¥ç‚¹
â”‚   â”‚   â”œâ”€â”€ Checkpoint             # æ£€æŸ¥ç‚¹æ•°æ®
â”‚   â”‚   â””â”€â”€ CheckpointManager      # æ£€æŸ¥ç‚¹ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ survey_config.py           # é—®å·é…ç½®
â”‚   â”‚   â”œâ”€â”€ SurveyConfig           # é…ç½®æ¨¡å‹
â”‚   â”‚   â””â”€â”€ SurveyConfigManager    # é…ç½®ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ survey_templates.py        # é—®å·æ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ QuestionMetadata       # é—®é¢˜å…ƒæ•°æ®
â”‚   â”‚   â”œâ”€â”€ SurveySection          # é—®å·åˆ†èŠ‚
â”‚   â”‚   â”œâ”€â”€ SurveyTemplate         # æ¨¡æ¿ç±»
â”‚   â”‚   â””â”€â”€ SurveyTemplateLibrary  # æ¨¡æ¿åº“
â”‚   â”‚
â”‚   â”œâ”€â”€ scoring.py                 # è‡ªåŠ¨è¯„åˆ†
â”‚   â”‚   â””â”€â”€ SurveyScorer           # è¯„åˆ†å¼•æ“
â”‚   â”‚
â”‚   â”œâ”€â”€ validation.py              # å“åº”éªŒè¯
â”‚   â”‚   â”œâ”€â”€ ResponseValidator      # æ ¼å¼éªŒè¯
â”‚   â”‚   â”œâ”€â”€ ConsistencyChecker     # ä¸€è‡´æ€§æ£€æŸ¥
â”‚   â”‚   â””â”€â”€ ConsistencyMetrics     # ä¸€è‡´æ€§æŒ‡æ ‡
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis.py                # ç»Ÿè®¡åˆ†æ
â”‚   â”‚   â”œâ”€â”€ StatisticalAnalyzer    # åˆ†æå™¨
â”‚   â”‚   â””â”€â”€ StatisticalResult      # åˆ†æç»“æœ
â”‚   â”‚
â”‚   â”œâ”€â”€ export.py                  # æ•°æ®å¯¼å‡º
â”‚   â”‚   â””â”€â”€ ScriptGenerator        # åˆ†æè„šæœ¬ç”Ÿæˆ
â”‚   â”‚
â”‚   â”œâ”€â”€ ab_testing.py              # A/B æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ Condition              # æµ‹è¯•æ¡ä»¶
â”‚   â”‚   â”œâ”€â”€ ABTestConfig           # æµ‹è¯•é…ç½®
â”‚   â”‚   â””â”€â”€ ABTestManager          # æµ‹è¯•ç®¡ç†å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ sensitivity.py             # æ•æ„Ÿæ€§åˆ†æ
â”‚   â”‚   â””â”€â”€ SensitivityAnalyzer    # åˆ†æå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ intervention_study.py      # çºµå‘ç ”ç©¶
â”‚   â”‚   â”œâ”€â”€ InterventionWave       # ç ”ç©¶æ³¢æ¬¡
â”‚   â”‚   â”œâ”€â”€ InterventionStudyConfig # ç ”ç©¶é…ç½®
â”‚   â”‚   â”œâ”€â”€ InterventionStudyBuilder # æ„å»ºå™¨
â”‚   â”‚   â””â”€â”€ InterventionStudyManager # ç®¡ç†å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ persona_generator.py       # äººç‰©ç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ DistributionConfig     # åˆ†å¸ƒé…ç½®
â”‚   â”‚   â””â”€â”€ PersonaGenerator       # ç”Ÿæˆå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ estimation.py              # æ—¶é—´ä¼°ç®—
â”‚   â”‚   â”œâ”€â”€ SimulationEstimate     # ä¼°ç®—ç»“æœ
â”‚   â”‚   â””â”€â”€ SimulationEstimator    # ä¼°ç®—å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ project.py                 # é¡¹ç›®ç®¡ç†
â”‚   â”‚   â””â”€â”€ ProjectManager         # é¡¹ç›®ç®¡ç†å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ tools.py                   # å·¥å…·ç³»ç»Ÿ
â”‚   â”‚   â””â”€â”€ ToolRegistry           # å·¥å…·æ³¨å†Œ
â”‚   â”‚
â”‚   â”œâ”€â”€ ui_components.py           # UI ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ render_navigation      # å¯¼èˆªæ 
â”‚   â”‚   â”œâ”€â”€ render_page_header     # é¡µé¢å¤´éƒ¨
â”‚   â”‚   â””â”€â”€ render_system_status   # ç³»ç»ŸçŠ¶æ€
â”‚   â”‚
â”‚   â”œâ”€â”€ styles.py                  # æ ·å¼ç³»ç»Ÿ
â”‚   â”‚   â””â”€â”€ apply_global_styles    # å…¨å±€æ ·å¼
â”‚   â”‚
â”‚   â”œâ”€â”€ validators.py              # è¾“å…¥éªŒè¯
â”‚   â”‚   â”œâ”€â”€ InputValidator         # éªŒè¯å™¨
â”‚   â”‚   â””â”€â”€ ValidationError        # éªŒè¯å¼‚å¸¸
â”‚   â”‚
â”‚   â”œâ”€â”€ connection_manager.py      # è¿æ¥ç®¡ç†
â”‚   â”‚   â””â”€â”€ ConnectionManager      # è¿æ¥ç®¡ç†å™¨
â”‚   â”‚
â”‚   â””â”€â”€ logging_config.py          # æ—¥å¿—é…ç½®
â”‚       â”œâ”€â”€ setup_logging          # æ—¥å¿—è®¾ç½®
â”‚       â””â”€â”€ get_logger             # è·å–æ—¥å¿—å™¨
â”‚
â”œâ”€â”€ tests/                          # æµ‹è¯•å¥—ä»¶
â”‚   â”œâ”€â”€ conftest.py                # pytest é…ç½®
â”‚   â”œâ”€â”€ test_llm_client.py         # LLM å®¢æˆ·ç«¯æµ‹è¯•
â”‚   â”œâ”€â”€ test_persona.py            # äººç‰©æµ‹è¯•
â”‚   â”œâ”€â”€ test_storage.py            # å­˜å‚¨æµ‹è¯•
â”‚   â””â”€â”€ test_validators.py         # éªŒè¯å™¨æµ‹è¯•
â”‚
â”œâ”€â”€ data/                           # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ personas/                  # äººç‰©æ•°æ®
â”‚   â”œâ”€â”€ results/                   # æ¨¡æ‹Ÿç»“æœ
â”‚   â”œâ”€â”€ cache/                     # ç¼“å­˜æ•°æ®
â”‚   â”œâ”€â”€ checkpoints/               # æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ survey_configs/            # é—®å·é…ç½®
â”‚   â”œâ”€â”€ calibration_targets/       # æ ¡å‡†ç›®æ ‡ï¼ˆé¢„ç•™ï¼‰
â”‚   â””â”€â”€ samples/                   # ç¤ºä¾‹æ•°æ®
â”‚
â”œâ”€â”€ docs/                           # æ–‡æ¡£
â”‚
â”œâ”€â”€ requirements.txt                # è¿è¡Œæ—¶ä¾èµ–
â”œâ”€â”€ requirements-dev.txt            # å¼€å‘ä¾èµ–
â”œâ”€â”€ pyproject.toml                 # é¡¹ç›®é…ç½®
â”œâ”€â”€ pytest.ini                     # pytest é…ç½®
â””â”€â”€ setup.sh                       # å®‰è£…è„šæœ¬
```

---

## ğŸ¯ æ¨¡å—è®¾è®¡

### LLM å®¢æˆ·ç«¯ (`llm_client.py`)

#### è®¾è®¡æ¨¡å¼

- **é€‚é…å™¨æ¨¡å¼** - ç»Ÿä¸€ä¸åŒ LLM æä¾›å•†çš„æ¥å£
- **å•ä¾‹æ¨¡å¼** - å¤ç”¨è¿æ¥æ± 

#### æ ¸å¿ƒåŠŸèƒ½

```python
class LMStudioClient:
    """åŒæ­¥ LLM å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str, model: str, api_key: Optional[str] = None):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = model
    
    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆå“åº”"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            **kwargs
        )
        return response.choices[0].message.content

class AsyncLLMClient:
    """å¼‚æ­¥ LLM å®¢æˆ·ç«¯"""
    
    async def generate_async(self, prompt: str, **kwargs) -> str:
        """å¼‚æ­¥ç”Ÿæˆå•ä¸ªå“åº”"""
        
    async def generate_batch(self, prompts: List[str], **kwargs) -> List[str]:
        """æ‰¹é‡å¼‚æ­¥ç”Ÿæˆ"""
        tasks = [self.generate_async(p, **kwargs) for p in prompts]
        return await asyncio.gather(*tasks)
```

#### é”™è¯¯å¤„ç†

- **è‡ªåŠ¨é‡è¯•** - ç½‘ç»œé”™è¯¯ã€è¶…æ—¶
- **é€Ÿç‡é™åˆ¶** - éµå®ˆ API é™åˆ¶
- **é™çº§ç­–ç•¥** - å¤±è´¥æ—¶é™ä½å¹¶å‘æ•°

---

### æ¨¡æ‹Ÿå¼•æ“ (`simulation.py`)

#### æ¶æ„

```
SimulationEngine (åŸºç¡€)
    â”œâ”€â”€ run_survey()        # è¿è¡Œè°ƒæŸ¥
    â”œâ”€â”€ run_intervention()  # è¿è¡Œå¹²é¢„
    â””â”€â”€ _generate_response() # ç”Ÿæˆå•ä¸ªå“åº”

ParallelSimulationEngine (é«˜çº§)
    â”œâ”€â”€ run_survey_async()       # å¼‚æ­¥è°ƒæŸ¥
    â”œâ”€â”€ run_intervention_async() # å¼‚æ­¥å¹²é¢„
    â””â”€â”€ _batch_generate()        # æ‰¹é‡ç”Ÿæˆ
```

#### æ‰§è¡Œæµç¨‹

```
1. å‡†å¤‡é˜¶æ®µ
   â”œâ”€â”€ éªŒè¯è¾“å…¥
   â”œâ”€â”€ åŠ è½½ç¼“å­˜
   â””â”€â”€ åˆå§‹åŒ–æ£€æŸ¥ç‚¹

2. æ‰§è¡Œé˜¶æ®µ
   â”œâ”€â”€ éå†äººç‰©
   â”‚   â”œâ”€â”€ æ„å»ºæç¤ºè¯
   â”‚   â”œâ”€â”€ æ£€æŸ¥ç¼“å­˜
   â”‚   â”œâ”€â”€ è°ƒç”¨ LLM (å¦‚éœ€è¦)
   â”‚   â””â”€â”€ ä¿å­˜å“åº”
   â””â”€â”€ æ›´æ–°è¿›åº¦

3. å®Œæˆé˜¶æ®µ
   â”œâ”€â”€ èšåˆç»“æœ
   â”œâ”€â”€ ä¿å­˜æ£€æŸ¥ç‚¹
   â””â”€â”€ è¿”å› SimulationResult
```

#### å¹¶è¡Œç­–ç•¥

```python
async def run_survey_async(self, personas, questions, max_concurrent=10):
    """å¹¶è¡Œæ‰§è¡Œæ¨¡æ‹Ÿ"""
    
    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    tasks = []
    for persona in personas:
        for question in questions:
            task = self._generate_response_async(persona, question)
            tasks.append(task)
    
    # ä½¿ç”¨ Semaphore æ§åˆ¶å¹¶å‘
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def bounded_task(task):
        async with semaphore:
            return await task
    
    # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await asyncio.gather(*[bounded_task(t) for t in tasks])
    
    return results
```

---

### ç¼“å­˜ç³»ç»Ÿ (`cache.py`)

#### ç¼“å­˜é”®ç”Ÿæˆ

```python
def _generate_cache_key(self, persona: Persona, question: str) -> str:
    """åŸºäºå†…å®¹çš„å“ˆå¸Œé”®"""
    content = {
        "persona": persona.to_dict(),
        "question": question,
        "model": self.model_name,
        "temperature": self.temperature
    }
    content_str = json.dumps(content, sort_keys=True)
    return hashlib.sha256(content_str.encode()).hexdigest()
```

#### å­˜å‚¨ç»“æ„

```
data/cache/
â”œâ”€â”€ 00/
â”‚   â”œâ”€â”€ 00a1b2c3...json  # ç¼“å­˜æ¡ç›®
â”‚   â””â”€â”€ 00d4e5f6...json
â”œâ”€â”€ 01/
â”œâ”€â”€ 02/
...
â””â”€â”€ ff/
```

#### ç¼“å­˜ç­–ç•¥

- **LRU** - æœ€è¿‘æœ€å°‘ä½¿ç”¨æ·˜æ±°
- **TTL** - å¯é…ç½®è¿‡æœŸæ—¶é—´
- **å¤§å°é™åˆ¶** - è‡ªåŠ¨æ¸…ç†æ—§ç¼“å­˜

---

### æ£€æŸ¥ç‚¹ç³»ç»Ÿ (`checkpoint.py`)

#### æ£€æŸ¥ç‚¹æ•°æ®

```python
@dataclass
class Checkpoint:
    simulation_id: str
    timestamp: str
    completed_count: int
    total_count: int
    partial_result: SimulationResult
    state: Dict[str, Any]
```

#### æ¢å¤æœºåˆ¶

```python
def resume_from_checkpoint(self, checkpoint_id: str) -> SimulationResult:
    """ä»æ£€æŸ¥ç‚¹æ¢å¤"""
    checkpoint = self.checkpoint_mgr.load(checkpoint_id)
    
    # æ¢å¤çŠ¶æ€
    remaining_tasks = checkpoint.state['remaining_tasks']
    partial_result = checkpoint.partial_result
    
    # ç»§ç»­æ‰§è¡Œ
    for task in remaining_tasks:
        response = self._execute_task(task)
        partial_result.add_response(response)
        
    return partial_result
```

---

### é—®å·æ¨¡æ¿ç³»ç»Ÿ (`survey_templates.py`)

#### æ¨¡æ¿ç»“æ„

```python
class SurveyTemplate:
    id: str
    name: str
    description: str
    sections: List[SurveySection]
    scoring_rules: Optional[Dict]
    metadata: Dict

class SurveySection:
    name: str
    questions: List[QuestionMetadata]

class QuestionMetadata:
    text: str
    type: str  # "multiple_choice", "scale", "open_ended"
    options: Optional[List[str]]
    required: bool
```

#### æ¨¡æ¿ç¤ºä¾‹

```json
{
    "id": "phq-9",
    "name": "PHQ-9 Depression Screening",
    "sections": [
        {
            "name": "ç—‡çŠ¶è¯„ä¼°",
            "questions": [
                {
                    "text": "è¿‡å»ä¸¤å‘¨ï¼Œæ‚¨æœ‰å¤šå°‘å¤©æ„Ÿåˆ°å¿ƒæƒ…ä½è½ï¼Ÿ",
                    "type": "scale",
                    "options": ["ä»ä¸", "å‡ å¤©", "ä¸€åŠä»¥ä¸Š", "å‡ ä¹æ¯å¤©"],
                    "score_map": [0, 1, 2, 3]
                }
            ]
        }
    ],
    "scoring_rules": {
        "total": "sum",
        "interpretation": {
            "0-4": "æ— æŠ‘éƒ",
            "5-9": "è½»åº¦",
            "10-14": "ä¸­åº¦",
            "15-27": "é‡åº¦"
        }
    }
}
```

---

## ğŸ”„ æ•°æ®æµ

### è°ƒæŸ¥æ¨¡æ‹Ÿæµç¨‹

```
ç”¨æˆ·è¾“å…¥
  â†“
UI éªŒè¯ (validators.py)
  â†“
æ¨¡æ‹Ÿé…ç½® (survey_config.py)
  â†“
æ¨¡æ‹Ÿå¼•æ“ (simulation.py)
  â”œâ†’ äººç‰©ç®¡ç†å™¨ (persona.py)
  â”œâ†’ ç¼“å­˜ç³»ç»Ÿ (cache.py)
  â”œâ†’ LLM å®¢æˆ·ç«¯ (llm_client.py)
  â”‚   â†“
  â”‚   å¤–éƒ¨ LLM API
  â”‚   â†“
  â”œâ† å“åº”æ•°æ®
  â”œâ†’ éªŒè¯å™¨ (validation.py)
  â””â†’ è¯„åˆ†å™¨ (scoring.py)
  â†“
ç»“æœå¯¹è±¡ (SimulationResult)
  â†“
å­˜å‚¨ç³»ç»Ÿ (storage.py)
  â”œâ†’ JSON æ–‡ä»¶
  â””â†’ CSV å¯¼å‡º
  â†“
åˆ†ææ¨¡å— (analysis.py)
  â†“
UI å±•ç¤º
```

### æ•°æ®æ ¼å¼

#### äººç‰© JSON

```json
{
    "name": "ææ˜",
    "age": 32,
    "gender": "ç”·",
    "occupation": "è½¯ä»¶å·¥ç¨‹å¸ˆ",
    "background": "...",
    "personality_traits": ["å†…å‘", "å®Œç¾ä¸»ä¹‰"],
    "values": ["èŒä¸šå‘å±•", "å¥åº·"],
    "education": "æœ¬ç§‘",
    "location": "åŒ—äº¬"
}
```

#### ç»“æœ JSON

```json
{
    "simulation_type": "survey",
    "timestamp": "2023-10-28T10:30:00",
    "persona_responses": [
        {
            "persona_name": "ææ˜",
            "persona_age": 32,
            "question": "æ‚¨çš„å¹´é¾„æ˜¯ï¼Ÿ",
            "response": "æˆ‘ä»Šå¹´32å²",
            "conversation_history": [...]
        }
    ],
    "metadata": {
        "model": "deepseek-chat",
        "temperature": 0.7,
        "total_tokens": 1500
    }
}
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å¼‚æ­¥å¹¶å‘

```python
# åŒæ­¥ï¼šé¡ºåºæ‰§è¡Œ
for persona in personas:
    for question in questions:
        response = llm.generate(...)  # ç­‰å¾…
# æ—¶é—´ï¼šN Ã— M Ã— T

# å¼‚æ­¥ï¼šå¹¶å‘æ‰§è¡Œ
tasks = [llm.generate_async(...) for p in personas for q in questions]
responses = await asyncio.gather(*tasks, return_exceptions=True)
# æ—¶é—´ï¼šâ‰ˆ T (å¦‚æœå¹¶å‘æ•°è¶³å¤Ÿ)
```

### 2. æ™ºèƒ½ç¼“å­˜

```python
# ç¬¬ä¸€æ¬¡è¿è¡Œ
result1 = engine.run_survey(personas, questions)
# API è°ƒç”¨ï¼š100 æ¬¡
# æ—¶é—´ï¼š5 åˆ†é’Ÿ

# ç¬¬äºŒæ¬¡è¿è¡Œç›¸åŒå†…å®¹
result2 = engine.run_survey(personas, questions)
# API è°ƒç”¨ï¼š0 æ¬¡ï¼ˆå…¨éƒ¨å‘½ä¸­ç¼“å­˜ï¼‰
# æ—¶é—´ï¼š2 ç§’
```

### 3. æ‰¹å¤„ç†

```python
# åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
batch_size = 50
for i in range(0, len(tasks), batch_size):
    batch = tasks[i:i+batch_size]
    results = await process_batch(batch)
    save_checkpoint(results)
```

### 4. è¿æ¥æ± 

```python
# å¤ç”¨ HTTP è¿æ¥
client = AsyncLLMClient(
    connection_pool_size=20,
    keep_alive=True
)
```

---

## ğŸ”Œ æ‰©å±•æ€§

### æ’ä»¶æ¶æ„

#### å·¥å…·æ³¨å†Œç³»ç»Ÿ

```python
from src import ToolRegistry

registry = ToolRegistry()

# æ³¨å†Œè‡ªå®šä¹‰å·¥å…·
@registry.register_tool("my_analyzer")
class MyAnalyzer:
    def analyze(self, data):
        return custom_analysis(data)

# ä½¿ç”¨å·¥å…·
tool = registry.get_tool("my_analyzer")
result = tool.analyze(data)
```

#### è‡ªå®šä¹‰ LLM æä¾›å•†

```python
from src.llm_client import BaseLLMClient

class MyLLMProvider(BaseLLMClient):
    def generate(self, prompt: str, **kwargs) -> str:
        # å®ç°è‡ªå®šä¹‰é€»è¾‘
        return my_api_call(prompt, **kwargs)

# ä½¿ç”¨è‡ªå®šä¹‰æä¾›å•†
client = MyLLMProvider(...)
engine = SimulationEngine(llm_client=client)
```

#### è‡ªå®šä¹‰è¯„åˆ†è§„åˆ™

```python
from src import SurveyScorer

scorer = SurveyScorer()

# æ·»åŠ è‡ªå®šä¹‰è§„åˆ™
scorer.add_scoring_rule(
    scale_id="my-scale",
    rules={
        "questions": [...],
        "scoring": lambda responses: custom_score(responses)
    }
)
```

### å¤šè¯­è¨€æ”¯æŒ

```python
# é…ç½®è¯­è¨€
config = {
    "language": "zh-CN",  # æˆ– "en-US", "es-ES"
    "translations": load_translations("zh-CN")
}

# ä½¿ç”¨ç¿»è¯‘
ui_text = config["translations"]["welcome_message"]
```

---

## ğŸ” å®‰å…¨æ€§

### API å¯†é’¥ç®¡ç†

```python
# ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
import os
api_key = os.getenv("DEEPSEEK_API_KEY")

# .env æ–‡ä»¶
from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv("DEEPSEEK_API_KEY")

# ä¸è¦ç¡¬ç¼–ç ï¼
# api_key = "sk-xxx"  # âŒ æ°¸è¿œä¸è¦è¿™æ ·åš
```

### æ•°æ®éšç§

- æœ¬åœ°æ¨¡å¼ï¼šæ•°æ®å®Œå…¨ä¸ç¦»å¼€æœ¬åœ°
- API æ¨¡å¼ï¼šéµå¾ªæä¾›å•†éšç§æ”¿ç­–
- æ•æ„Ÿæ•°æ®ï¼šä½¿ç”¨æœ¬åœ° LM Studio

### è¾“å…¥éªŒè¯

```python
from src import InputValidator, ValidationError

validator = InputValidator()

try:
    validated_age = validator.validate_age(user_input)
except ValidationError as e:
    st.error(f"æ— æ•ˆè¾“å…¥: {e}")
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—ç³»ç»Ÿ

```python
from src import setup_logging, get_logger

# é…ç½®æ—¥å¿—
setup_logging(
    level="INFO",
    log_file="logs/app.log",
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

# ä½¿ç”¨æ—¥å¿—
logger = get_logger(__name__)
logger.info("æ¨¡æ‹Ÿå¼€å§‹")
logger.warning("ç¼“å­˜æœªå‘½ä¸­")
logger.error("API è°ƒç”¨å¤±è´¥", exc_info=True)
```

### æ€§èƒ½ç›‘æ§

```python
import time

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
    
    def track(self, operation_name):
        def decorator(func):
            def wrapper(*args, **kwargs):
                start = time.time()
                result = func(*args, **kwargs)
                duration = time.time() - start
                
                self.metrics[operation_name] = {
                    "duration": duration,
                    "timestamp": time.time()
                }
                return result
            return wrapper
        return decorator

monitor = PerformanceMonitor()

@monitor.track("run_simulation")
def run_simulation(...):
    ...
```

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### æµ‹è¯•é‡‘å­—å¡”

```
      /\
     /  \    E2E Tests (å°‘é‡)
    /â”€â”€â”€â”€\
   /      \  Integration Tests (ä¸­ç­‰)
  /â”€â”€â”€â”€â”€â”€â”€â”€\
 /          \ Unit Tests (å¤§é‡)
/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
```

### æµ‹è¯•ç±»å‹

#### å•å…ƒæµ‹è¯•

```python
# tests/test_persona.py
def test_persona_creation():
    persona = Persona(name="Test", age=25, ...)
    assert persona.name == "Test"
    assert persona.age == 25

def test_persona_to_dict():
    persona = Persona(...)
    data = persona.to_dict()
    assert isinstance(data, dict)
    assert "name" in data
```

#### é›†æˆæµ‹è¯•

```python
# tests/test_simulation.py
@pytest.mark.integration
async def test_full_simulation(mock_llm_client):
    engine = SimulationEngine(llm_client=mock_llm_client)
    result = await engine.run_survey_async(personas, questions)
    assert len(result.persona_responses) == len(personas) * len(questions)
```

#### Mock ç­–ç•¥

```python
# tests/conftest.py
@pytest.fixture
def mock_llm_client():
    client = Mock(spec=LMStudioClient)
    client.generate.return_value = "Mock response"
    return client
```

---

## ğŸš€ éƒ¨ç½²

### æœ¬åœ°éƒ¨ç½²

```bash
streamlit run app.py
```

### Docker éƒ¨ç½²ï¼ˆè§„åˆ’ä¸­ï¼‰

```dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8501
CMD ["streamlit", "run", "app.py"]
```

### äº‘éƒ¨ç½²

- **Streamlit Cloud** - ä¸€é”®éƒ¨ç½²
- **AWS/Azure** - å®¹å™¨éƒ¨ç½²
- **Heroku** - PaaS éƒ¨ç½²

---

## ğŸ“ˆ æœªæ¥è·¯çº¿å›¾

### çŸ­æœŸï¼ˆ1-3ä¸ªæœˆï¼‰

- [ ] å¢å¼ºæ•°æ®å¯è§†åŒ–
- [ ] æ”¯æŒæ›´å¤šé—®å·æ¨¡æ¿
- [ ] æ”¹è¿›é”™è¯¯å¤„ç†
- [ ] æ€§èƒ½ä¼˜åŒ–

### ä¸­æœŸï¼ˆ3-6ä¸ªæœˆï¼‰

- [ ] æ•°æ®åº“åç«¯æ”¯æŒ
- [ ] å¤šç”¨æˆ·ç³»ç»Ÿ
- [ ] API æœåŠ¡åŒ–
- [ ] æ›´å¤š LLM æä¾›å•†

### é•¿æœŸï¼ˆ6-12ä¸ªæœˆï¼‰

- [ ] æœºå™¨å­¦ä¹ æ¨¡å‹é›†æˆ
- [ ] å®æ—¶åä½œåŠŸèƒ½
- [ ] ç§»åŠ¨ç«¯æ”¯æŒ
- [ ] ä¼ä¸šç‰ˆåŠŸèƒ½

---

## ğŸ“š å‚è€ƒèµ„æº

- [Streamlit æ–‡æ¡£](https://docs.streamlit.io)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
- [Python asyncio æ–‡æ¡£](https://docs.python.org/3/library/asyncio.html)
- [pytest æ–‡æ¡£](https://docs.pytest.org)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
**æœ€åæ›´æ–°**: 2025-10-28  
**ç»´æŠ¤è€…**: Jason Li
