# Auto Sim AI - LLM Survey Simulation System



<div align="center">



**English Version | [中文版](./README_zh.md)**<div align="center">



---<div align="center">



🔬 **AI-Powered Survey and Intervention Simulation System****English Version | [中文版](./README_zh.md)**



Simulate real survey research and intervention effects using LLM-driven virtual personas



[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)---

[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)**English Version | [中文版](./README_zh.md)**<div align="center">



📖 **[View Complete English Documentation](./docs/en/README.md)**🔬 **AI-Powered Survey and Intervention Simulation System**



[Quick Start](./docs/en/quickstart/README.md) •

[Features](#features) •

[API Reference](./docs/en/api/README.md) •Simulate real survey research and intervention effects using LLM-driven virtual personas

[Contributing](./docs/en/contributing/README.md)

---

</div>

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

---

[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)

## 📋 Table of Contents

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

- [Overview](#overview)

- [Features](#features)🔬 **AI-Powered Survey and Intervention Simulation System****English Version | [中文版](./README_zh.md)**<div align="center"><div align="right">

- [Quick Start](#quick-start)

- [User Guide](#user-guide)📖 **[View Complete English Documentation](./docs/en/README.md)**

- [📚 Complete Documentation](./docs/README.md)

  - [Quick Start Guide](./docs/quickstart/README.md)

  - [API Documentation](./docs/api/README.md)

  - [Architecture Design](./docs/architecture/README.md)[Quick Start](#quick-start) •

  - [Longitudinal Studies Guide](./docs/longitudinal/README.md)

  - [Contributing Guide](./docs/contributing/README.md)[Features](#features) •Simulate real survey research and intervention effects using LLM-driven virtual personas

- [FAQ](#faq)

- [License](#license)[API Reference](./docs/en/api/README.md) •



---[Contributing](./docs/en/contributing/README.md)



## 🎯 Overview



**LLM Simulation Survey System** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions.</div>📖 **[View Complete English Documentation](./docs/en/README.md)**---



### Use Cases



- 🏥 **Health Intervention Research** - Test the impact of health messaging on different populations---

- 📊 **Market Research** - Rapidly evaluate user feedback for products or services

- 🎓 **Educational Research** - Assess teaching method effectiveness across different learner types

- 💡 **Policy Analysis** - Predict potential policy impacts on diverse populations

- 🧪 **A/B Testing** - Compare effectiveness across different approaches## 📋 Table of Contents[Quick Start](./docs/en/quickstart/README.md) •

- 📈 **Prototype Validation** - Rapidly iterate designs before real-world research



### Core Advantages

- [Overview](#overview)[Features](#features) •

✅ **Fast Iteration** - Complete survey simulations with hundreds of participants in minutes  

✅ **Cost-Effective** - No need to recruit real participants  - [Features](#features)

✅ **Reproducible** - Precise variable control ensures experimental repeatability  

✅ **Diverse** - Easily create virtual personas with varied backgrounds, ages, and cultures  - [Quick Start](#quick-start)[API Reference](./docs/en/api/README.md) •🔬 **AI-Powered Survey and Intervention Simulation System****English Version | [中文版](./README_zh.md)**[![English](https://img.shields.io/badge/docs-English-blue?style=flat-square)](./docs/en/README.md)

✅ **Deep Insights** - Obtain detailed qualitative and quantitative data  

✅ **Flexible Deployment** - Support for local deployment and cloud API- [User Guide](#user-guide)



---- [📚 Complete Documentation](./docs/README.md)[Contributing](./docs/en/contributing/README.md)



## 🚀 Features  - [Quick Start Guide](./docs/quickstart/README.md)



### Core Capabilities  - [API Documentation](./docs/api/README.md)



#### 1️⃣ Virtual Persona Management  - [Architecture Design](./docs/architecture/README.md)



- **Rich Persona Attributes**: Age, gender, occupation, education, personality traits, values, etc.  - [Longitudinal Studies Guide](./docs/longitudinal/README.md)</div>

- **Batch Creation**: Auto-generate virtual samples matching real population distributions using demographic statistics

- **CSV Import/Export**: Bulk import personas from Excel or databases  - [Contributing Guide](./docs/contributing/README.md)

- **Demo Templates**: Built-in typical persona templates, ready to use

- [FAQ](#faq)Simulate real survey research and intervention effects using LLM-driven virtual personas[![中文文档](https://img.shields.io/badge/文档-中文-red?style=flat-square)](./docs/zh/README.md)

#### 2️⃣ Multiple Simulation Modes

- [License](#license)

- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)

- **Intervention Mode**: Test health messaging, advertising copy, etc. on different populations---

- **A/B Testing**: Test multiple versions simultaneously and compare effectiveness

- **Longitudinal Studies**: Simulate multi-wave surveys to track changes over time---

- **Sensitivity Analysis**: Systematically test how parameter changes affect results



#### 3️⃣ LLM Integration

## 🎯 Overview

- **Local Deployment**: LM Studio (free, completely private)

- **Commercial APIs**:## 📋 Table of Contents

  - DeepSeek (cost-effective, Chinese-optimized)

  - OpenAI (GPT-4, GPT-3.5)**LLM Simulation Survey System** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions.

  - Other OpenAI-compatible services

- **Flexible Switching**: Change models or providers anytime[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)---



#### 4️⃣ Advanced Analysis### Use Cases



- **Auto-Scoring**: Built-in automatic scoring system for standardized scales- [Overview](#overview)

- **Statistical Analysis**: Descriptive statistics, correlation analysis, group comparisons

- **Consistency Checks**: Validate internal consistency and logic of responses- 🏥 **Health Intervention Research** - Test the impact of health messaging on different populations

- **Visualization**: Interactive charts, word clouds, distribution plots

- **Export Functions**: CSV, JSON, Python/R analysis scripts- 📊 **Market Research** - Rapidly evaluate user feedback for products or services- [Features](#features)[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)



#### 5️⃣ Performance Optimization- 🎓 **Educational Research** - Assess teaching method effectiveness across different learner types



- **Parallel Execution**: Async processing for multiple persona responses- 💡 **Policy Analysis** - Predict potential policy impacts on diverse populations- [Quick Start](#quick-start)

- **Smart Caching**: Avoid redundant LLM calls, save time and cost

- **Checkpoint Resume**: Support pausing and resuming large-scale simulations- 🧪 **A/B Testing** - Compare effectiveness across different approaches

- **Progress Tracking**: Real-time progress display and estimated completion time

- 📈 **Prototype Validation** - Rapidly iterate designs before real-world research- [User Guide](#user-guide)[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)</div>

---



## ⚡ Quick Start

### Core Advantages- [📚 Complete Documentation](./docs/README.md)

### System Requirements



- **Python**: 3.8 or higher

- **Memory**: 8GB+ recommended✅ **Fast Iteration** - Complete survey simulations with hundreds of participants in minutes    - [Quick Start Guide](./docs/quickstart/README.md)

- **LLM Provider** (choose one):

  - LM Studio (local, free)✅ **Cost-Effective** - No need to recruit real participants  

  - DeepSeek/OpenAI API key

✅ **Reproducible** - Precise variable control ensures experimental repeatability    - [API Documentation](./docs/api/README.md)

### Installation Steps

✅ **Diverse** - Easily create virtual personas with varied backgrounds, ages, and cultures  

#### Step 1: Clone Repository

✅ **Deep Insights** - Obtain detailed qualitative and quantitative data    - [Architecture Design](./docs/architecture/README.md)📖 **[Complete English Documentation](./docs/en/README.md)**🔬 **AI-Powered Survey and Intervention Simulation System**

```bash

git clone https://github.com/jason-jj-li/auto_sim_ai.git✅ **Flexible Deployment** - Support for local deployment and cloud API

cd auto_sim_ai

```  - [Longitudinal Studies Guide](./docs/longitudinal/README.md)



#### Step 2: Install Dependencies---



```bash  - [Contributing Guide](./docs/contributing/README.md)

# Create virtual environment (recommended)

python -m venv venv## 🚀 Features

source venv/bin/activate  # Windows: venv\Scripts\activate

- [FAQ](#faq)

# Install dependencies

pip install -r requirements.txt### Core Capabilities

```

- [License](#license)[Quick Start](#-quick-start) •<div align="center">

Or use the setup script:

#### 1️⃣ Virtual Persona Management

```bash

chmod +x setup.sh

./setup.sh

```- **Rich Persona Attributes**: Age, gender, occupation, education, personality traits, values, etc.



#### Step 3: Configure LLM- **Batch Creation**: Auto-generate virtual samples matching real population distributions using demographic statistics---[Features](#-features) •



**Option A: Local LM Studio (Recommended for Learning and Development)**- **CSV Import/Export**: Bulk import personas from Excel or databases



1. Download [LM Studio](https://lmstudio.ai/)- **Demo Templates**: Built-in typical persona templates, ready to use

2. Download a model in LM Studio:

   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`

   - Minimum: 7B parameter model

3. Start local server:#### 2️⃣ Multiple Simulation Modes## 🎯 Overview[API Reference](./docs/en/api/README.md) •Simulate real survey research and intervention effects using LLM-driven virtual personas

   - Click "Local Server" tab

   - Select model

   - Click "Start Server"

   - Confirm address is `http://localhost:1234`- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)



**Option B: Online API (Recommended for Production)**- **Intervention Mode**: Test health messaging, advertising copy, etc. on different populations



```bash- **A/B Testing**: Test multiple versions simultaneously and compare effectiveness**LLM Simulation Survey System** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions.[Contributing](./docs/en/contributing/README.md)

# Copy environment template

cp env.example .env- **Longitudinal Studies**: Simulate multi-wave surveys to track changes over time



# Edit .env file and add API key- **Sensitivity Analysis**: Systematically test how parameter changes affect results

# DEEPSEEK_API_KEY=your_api_key_here

# or

# OPENAI_API_KEY=your_api_key_here

```#### 3️⃣ LLM Integration### Use Cases**Language / 语言:** [English](#english) | [中文](#中文)



#### Step 4: Launch Application



```bash- **Local Deployment**: LM Studio (free, completely private)

streamlit run app.py

```- **Commercial APIs**:



The app will automatically open in your browser: `http://localhost:8501`  - DeepSeek (cost-effective, Chinese-optimized)- 🏥 **Health Intervention Research** - Test the impact of health messaging on different populations</div>



### First-Time User Guide  - OpenAI (GPT-4, GPT-3.5)



1. **Connect LLM** (Home Page)  - Other OpenAI-compatible services- 📊 **Market Research** - Rapidly evaluate user feedback for products or services

   - Select LLM provider

   - Test connection- **Flexible Switching**: Change models or providers anytime

   - Wait for "System Ready" message

- 🎓 **Educational Research** - Assess teaching method effectiveness across different learner types[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

2. **Create Virtual Personas** (Setup Page)

   - Click "Create Demo Personas" for quick start#### 4️⃣ Advanced Analysis

   - Or manually create custom personas

   - Or upload CSV for bulk import- 💡 **Policy Analysis** - Predict potential policy impacts on diverse populations



3. **Run Simulation** (Simulation Page)- **Auto-Scoring**: Built-in automatic scoring system for standardized scales

   - Choose simulation type (Survey/Intervention)

   - Select personas to participate- **Statistical Analysis**: Descriptive statistics, correlation analysis, group comparisons- 🧪 **A/B Testing** - Compare effectiveness across different approaches---

   - Choose questionnaire template or enter custom questions

   - Click "Run Simulation"- **Consistency Checks**: Validate internal consistency and logic of responses



4. **View Results** (Results Page)- **Visualization**: Interactive charts, word clouds, distribution plots- 📈 **Prototype Validation** - Rapidly iterate designs before real-world research

   - Browse response data

   - View statistical analysis- **Export Functions**: CSV, JSON, Python/R analysis scripts

   - Export results for further analysis

[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)---

---

#### 5️⃣ Performance Optimization

## 📖 User Guide

### Core Advantages

### Persona Design Best Practices

- **Parallel Execution**: Async processing for multiple persona responses

#### Creating High-Quality Personas

- **Smart Caching**: Avoid redundant LLM calls, save time and cost## 📋 Table of Contents

```python

# Good Example: Specific, detailed, realistic- **Checkpoint Resume**: Support pausing and resuming large-scale simulations

{

    "name": "Li Ming",- **Progress Tracking**: Real-time progress display and estimated completion time✅ **Fast Iteration** - Complete survey simulations with hundreds of participants in minutes  

    "age": 32,

    "gender": "Male",

    "occupation": "Software Engineer at Startup",

    "education": "Bachelor's in Computer Science",---✅ **Cost-Effective** - No need to recruit real participants  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

    "location": "Beijing",

    "background": "Works at a fast-growing tech company, often works overtime. Recently feeling work stress and declining sleep quality. Enjoys relieving stress through exercise but often too busy to do so.",

    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],

    "values": ["Career development", "Work-life balance", "Family health"]## ⚡ Quick Start✅ **Reproducible** - Precise variable control ensures experimental repeatability  

}



# Bad Example: Vague, generic

{### System Requirements✅ **Diverse** - Easily create virtual personas with varied backgrounds, ages, and cultures  - [Overview](#-overview)

    "name": "John Doe",

    "age": 30,

    "gender": "Male",

    "occupation": "Engineer",- **Python**: 3.8 or higher✅ **Deep Insights** - Obtain detailed qualitative and quantitative data  

    "background": "Regular person",

    "personality_traits": ["Normal"],- **Memory**: 8GB+ recommended

    "values": ["Happiness"]

}- **LLM Provider** (choose one):✅ **Flexible Deployment** - Support for local deployment and cloud API- [Features](#-features)## English

```

  - LM Studio (local, free)

#### Persona Diversity

  - DeepSeek/OpenAI API key

Ensure virtual samples reflect real population diversity:



- **Age**: Cover different age groups (18-80 years)

- **Gender**: Male, female, non-binary### Installation Steps---- [Quick Start](#-quick-start)

- **Occupation**: Different industries and job levels

- **Education**: High school to graduate degrees

- **Geography**: Urban, rural, different regions

- **Cultural Background**: Different ethnicities, religions, cultural traditions#### Step 1: Clone Repository



### Questionnaire Design Tips



#### Good Question Characteristics```bash## 🚀 Features- [User Guide](#-user-guide)📖 **[View Full English Documentation](./docs/en/README.md)**



✅ **Clear and Specific**git clone https://github.com/jason-jj-li/auto_sim_ai.git



```cd auto_sim_ai

Good: In the past two weeks, how many days have you felt down or depressed?

Bad: How have you been feeling lately?```

```

### Core Capabilities- [Architecture](#-architecture)

✅ **Avoid Compound Questions**

#### Step 2: Install Dependencies

```

Good: How many times per week do you exercise? How long is each exercise session?

Bad: How often do you exercise, for how long, and at what intensity?

``````bash



✅ **Use Standardized Scales**# Create virtual environment (recommended)#### 1️⃣ Virtual Persona Management- [Advanced Features](#-advanced-features)🔬 **AI-Powered Survey and Intervention Simulation System**



```python -m venv venv

Never(0) - Sometimes(1) - Often(2) - Always(3)

```source venv/bin/activate  # Windows: venv\Scripts\activate



#### Use Built-in Templates



The system includes multiple validated standardized scales:# Install dependencies- **Rich Persona Attributes**: Age, gender, occupation, education, personality traits, values, etc.- [API Documentation](#-api-documentation)



- **PHQ-9**: Depression screening scalepip install -r requirements.txt

- **GAD-7**: Anxiety screening scale

- **PSS-10**: Perceived Stress Scale```- **Batch Creation**: Auto-generate virtual samples matching real population distributions using demographic statistics

- More templates continuously being added...



### Simulation Settings Optimization

Or use the setup script:- **CSV Import/Export**: Bulk import personas from Excel or databases- [FAQ](#-faq)[Quick Start](#quick-start) •

#### Temperature Parameter



Controls response randomness and creativity:

```bash- **Demo Templates**: Built-in typical persona templates, ready to use

- **0.0 - 0.3**: High consistency, suitable for standardized responses

- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)chmod +x setup.sh

- **0.8 - 1.0**: More diverse, suitable for exploratory research and creative testing

./setup.sh- [Contributing](#-contributing)

#### Max Tokens

```

- **150-300**: Short answers (multiple choice, scale ratings)

- **300-500**: Medium length (short answer questions)#### 2️⃣ Multiple Simulation Modes

- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)

#### Step 3: Configure LLM

#### Parallel Settings

- [License](#-license)[Features](#features) •Simulate real survey research and intervention effects using LLM-driven virtual personas

- **Small Scale** (<10 personas): Concurrency 2-3

- **Medium Scale** (10-50 personas): Concurrency 5-10**Option A: Local LM Studio (Recommended for Learning and Development)**

- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)

- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)

---

1. Download [LM Studio](https://lmstudio.ai/)

## 🏗️ Architecture Design

2. Download a model in LM Studio:- **Intervention Mode**: Test health messaging, advertising copy, etc. on different populations

For detailed architecture documentation, see **[Architecture Guide](./docs/architecture/README.md)**

   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`

### Project Structure

   - Minimum: 7B parameter model- **A/B Testing**: Test multiple versions simultaneously and compare effectiveness

```

auto_sim_ai/3. Start local server:

├── app.py                      # Streamlit main application

├── pages/                      # Multi-page application   - Click "Local Server" tab- **Longitudinal Studies**: Simulate multi-wave surveys to track changes over time---[API Reference](./docs/en/api/README.md) •

│   ├── 1_Setup.py             # Persona management page

│   ├── 2_Simulation.py        # Simulation execution page   - Select model

│   └── 3_Results.py           # Results analysis page

├── src/                        # Core modules   - Click "Start Server"- **Sensitivity Analysis**: Systematically test how parameter changes affect results

│   ├── llm_client.py          # LLM client (sync/async)

│   ├── persona.py             # Persona management   - Confirm address is `http://localhost:1234`

│   ├── simulation.py          # Simulation engine (single-thread/parallel)

│   ├── storage.py             # Results storage

│   ├── cache.py               # Response cache

│   ├── checkpoint.py          # Checkpoint management**Option B: Online API (Recommended for Production)**

│   ├── scoring.py             # Auto-scoring

│   ├── ab_testing.py          # A/B testing#### 3️⃣ LLM Integration

│   ├── intervention_study.py  # Intervention studies (legacy)

│   ├── longitudinal_study.py  # Longitudinal studies (new, recommended)```bash

│   ├── persona_generator.py   # Persona generator

│   ├── survey_templates.py    # Survey template library# Copy environment template## 🎯 Overview[Contributing](./docs/en/contributing/README.md)[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

│   ├── survey_config.py       # Survey configuration

│   ├── tools.py               # Tool registration systemcp env.example .env

│   ├── ui_components.py       # UI components

│   ├── styles.py              # Design system- **Local Deployment**: LM Studio (free, completely private)

│   └── validators.py          # Input validation

├── tests/                      # Test suite# Edit .env file and add API key

├── data/                       # Data directory

│   ├── personas/              # Persona data# DEEPSEEK_API_KEY=your_api_key_here- **Commercial APIs**:

│   ├── results/               # Simulation results

│   ├── cache/                 # Cache data# or

│   ├── checkpoints/           # Checkpoints

│   └── survey_configs/        # Survey configurations# OPENAI_API_KEY=your_api_key_here  - DeepSeek (cost-effective, Chinese-optimized)

├── docs/                       # Documentation

├── requirements.txt            # Dependencies```

└── pytest.ini                 # Test configuration

```  - OpenAI (GPT-4, GPT-3.5)**Auto Sim AI** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions.[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)



### Core Module Descriptions#### Step 4: Launch Application



#### LLM Client (`llm_client.py`)  - Other OpenAI-compatible services



Supports both synchronous and asynchronous modes:```bash



- **LMStudioClient**: Sync client, suitable for simple scenariosstreamlit run app.py- **Flexible Switching**: Change models or providers anytime

- **AsyncLLMClient**: Async client, supports high concurrency

```

Compatible with OpenAI API format, seamless switching between providers.



#### Simulation Engine (`simulation.py`)

The app will automatically open in your browser: `http://localhost:8501`

- **SimulationEngine**: Base engine, sequential execution

- **ParallelSimulationEngine**: Parallel engine, supports async batch processing#### 4️⃣ Advanced Analysis### Use Cases</div>[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)



Automatically handles error retry, progress tracking, and result aggregation.### First-Time User Guide



#### Cache System (`cache.py`)



Content-hash based smart caching:1. **Connect LLM** (Home Page)



- Same persona + same question = directly return cached result   - Select LLM provider- **Auto-Scoring**: Built-in automatic scoring system for standardized scales

- Support cache export and import

- Significantly reduce LLM API call costs   - Test connection



#### Scoring System (`scoring.py`)   - Wait for "System Ready" message- **Statistical Analysis**: Descriptive statistics, correlation analysis, group comparisons



Automated scoring features:



- Support multiple standardized scales2. **Create Virtual Personas** (Setup Page)- **Consistency Checks**: Validate internal consistency and logic of responses- 🏥 **Health Intervention Research** - Test health messaging impact on different populations

- Configurable custom scoring rules

- Auto-calculate total and subscale scores   - Click "Create Demo Personas" for quick start



---   - Or manually create custom personas- **Visualization**: Interactive charts, word clouds, distribution plots



## 🔬 Advanced Features   - Or upload CSV for bulk import



> 💡 **Tip**: For detailed API documentation and advanced features, see [API Guide](./docs/api/README.md)- **Export Functions**: CSV, JSON, Python/R analysis scripts- 📊 **Market Research** - Rapidly evaluate product/service user feedback



### 1. A/B Testing3. **Run Simulation** (Simulation Page)



Compare intervention effectiveness across different versions:   - Choose simulation type (Survey/Intervention)



```python   - Select personas to participate

from src import ABTestManager, Condition

   - Choose questionnaire template or enter custom questions#### 5️⃣ Performance Optimization- 🎓 **Educational Research** - Assess teaching method effectiveness across learner types---📖 **[View Full English Documentation](./docs/en/README.md)**

# Define test conditions

condition_a = Condition(   - Click "Run Simulation"

    name="Version A",

    intervention_text="Meditating 10 minutes daily can reduce stress.",

    questions=["Would you try this method?"]

)4. **View Results** (Results Page)



condition_b = Condition(   - Browse response data- **Parallel Execution**: Async processing for multiple persona responses- 💡 **Policy Analysis** - Predict policy impact on diverse populations

    name="Version B", 

    intervention_text="Research shows that meditating 10 minutes daily can reduce stress levels by 30%.",   - View statistical analysis

    questions=["Would you try this method?"]

)   - Export results for further analysis- **Smart Caching**: Avoid redundant LLM calls, save time and cost



# Run A/B test

ab_manager = ABTestManager()

results = ab_manager.run_test([condition_a, condition_b], personas)---- **Checkpoint Resume**: Support pausing and resuming large-scale simulations- 🧪 **A/B Testing** - Compare effectiveness of different approaches

```



### 2. Longitudinal Studies (Multi-Wave Tracking)

## 📖 User Guide- **Progress Tracking**: Real-time progress display and estimated completion time

Implement realistic longitudinal tracking using conversation memory:



```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig### Persona Design Best Practices- 📈 **Prototype Validation** - Rapidly iterate designs before real-world research



# Configure study waves

waves = [

    WaveConfig(#### Creating High-Quality Personas---

        wave_number=1,

        wave_name="Baseline",

        questions=["What is your current stress level? (1-10)"],

        days_from_baseline=0```python## 📋 Table of Contents[Quick Start](./docs/en/quickstart/README.md) •

    ),

    WaveConfig(# Good Example: Specific, detailed, realistic

        wave_number=2,

        wave_name="1 Month Follow-up",{## ⚡ Quick Start

        questions=["What is your stress level now? (1-10)"],

        days_from_baseline=30,    "name": "Li Ming",

        intervention_text="Practice 10 minutes of meditation daily"

    )    "age": 32,### Key Advantages

]

    "gender": "Male",

# Run longitudinal study

config = LongitudinalStudyConfig(    "occupation": "Software Engineer at Startup",### System Requirements

    study_id="stress_study",

    study_name="Stress Intervention Study",    "education": "Bachelor's in Computer Science",

    waves=waves

)    "location": "Beijing",[Features](#features-en) •



engine = LongitudinalStudyEngine(llm_client)    "background": "Works at a fast-growing tech company, often works overtime. Recently feeling work stress and declining sleep quality. Enjoys relieving stress through exercise but often too busy to do so.",

results = engine.run_study(personas, config)

```    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],- **Python**: 3.8 or higher



For detailed longitudinal study guide, see **[Longitudinal Study Guide](./docs/longitudinal/README.md)**    "values": ["Career development", "Work-life balance", "Family health"]



### 3. Batch Persona Generation}- **Memory**: 8GB+ recommended✅ **Fast Iteration** - Complete hundreds of survey simulations in minutes  



Generate virtual samples based on real demographic distributions:



```python# Bad Example: Vague, generic- **LLM Provider** (choose one):

from src import PersonaGenerator, DistributionConfig

{

# Configure distribution

config = DistributionConfig(    "name": "John Doe",  - LM Studio (local, free)✅ **Cost-Effective** - No need to recruit real participants  - [Overview](#overview)[API Reference](./docs/en/api/README.md) •

    age_distribution={

        "18-30": 0.3,    "age": 30,

        "31-50": 0.4,

        "51-70": 0.3    "gender": "Male",  - DeepSeek/OpenAI API key

    },

    gender_distribution={    "occupation": "Engineer",

        "Male": 0.48,

        "Female": 0.52    "background": "Regular person",✅ **Reproducible** - Precise variable control ensures repeatability  

    }

)    "personality_traits": ["Normal"],



# Generate 100 personas    "values": ["Happiness"]### Installation Steps

generator = PersonaGenerator()

personas = generator.generate_batch(}

    count=100,

    distribution_config=config,```✅ **Diverse** - Easily create personas with varied backgrounds, ages, cultures  - [Features](#features)[Contributing](./docs/en/contributing/README.md)

    llm_client=client

)

```

#### Persona Diversity#### 1. Clone Repository

### 4. Response Validation



Automatically check response quality and consistency:

Ensure virtual samples reflect real population diversity:✅ **Deep Insights** - Obtain detailed qualitative and quantitative data  

```python

from src import ResponseValidator, ConsistencyChecker



validator = ResponseValidator()- **Age**: Cover different age groups (18-80 years)```bash

checker = ConsistencyChecker()

- **Gender**: Male, female, non-binary

# Validate response format

is_valid = validator.validate_response(response, question_type)- **Occupation**: Different industries and job levelsgit clone https://github.com/jason-jj-li/auto_sim_ai.git✅ **Flexible Deployment** - Support for local and cloud API deployments- [Quick Start](#quick-start)



# Check consistency- **Education**: High school to graduate degrees

metrics = checker.check_consistency(persona_responses)

print(f"Consistency score: {metrics.consistency_score}")- **Geography**: Urban, rural, different regionscd auto_sim_ai

```

- **Cultural Background**: Different ethnicities, religions, cultural traditions

---

```

## 📚 API Documentation

### Questionnaire Design Tips

### PersonaManager



```python

from src import PersonaManager#### Good Question Characteristics



manager = PersonaManager()#### 2. Install Dependencies---- [User Guide](#user-guide)### Features (EN)



# Add persona✅ **Clear and Specific**

manager.add_persona(persona)



# Get all personas

personas = manager.get_all_personas()```



# Filter by criteriaGood: In the past two weeks, how many days have you felt down or depressed?```bash

young_adults = manager.filter_personas(

    age_range=(18, 30),Bad: How have you been feeling lately?

    gender="Female"

)```# Create virtual environment (recommended)



# Save/load

manager.save_to_file("personas.json")

manager.load_from_file("personas.json")✅ **Avoid Compound Questions**python -m venv venv## 🚀 Features- [📚 Complete Documentation](./docs/en/README.md)

```



### SimulationEngine

```source venv/bin/activate  # Windows: venv\Scripts\activate

```python

from src import SimulationEngineGood: How many times per week do you exercise? How long is each exercise session?



engine = SimulationEngine(Bad: How often do you exercise, for how long, and at what intensity?

    llm_client=client,

    cache=cache,```

    checkpoint_manager=checkpoint_mgr

)# Install dependencies



# Run survey✅ **Use Standardized Scales**

result = engine.run_survey(

    personas=personas,pip install -r requirements.txt### Core Capabilities  - [Quick Start Guide](./docs/en/quickstart/README.md)- **Three Research Modes**: Survey Testing, Message Testing, A/B Testing

    questions=questions,

    temperature=0.7,```

    max_tokens=300

)Never(0) - Sometimes(1) - Often(2) - Always(3)```



# Run intervention```

result = engine.run_intervention(

    personas=personas,

    intervention_text="Health intervention text",

    questions=followup_questions#### Use Built-in Templates

)

```Or use the setup script:



### ResultsStorageThe system includes multiple validated standardized scales:



```python#### 1. Virtual Persona Management  - [API Documentation](./docs/en/api/README.md)- **Longitudinal Studies**: Multi-wave research with persona memory

from src import ResultsStorage

- **PHQ-9**: Depression screening scale

storage = ResultsStorage()

- **GAD-7**: Anxiety screening scale```bash

# Save result

storage.save_result(simulation_result)- **PSS-10**: Perceived Stress Scale



# Load results- More templates continuously being added...chmod +x setup.sh

results = storage.load_all_results()



# Export to CSV

storage.export_to_csv(result, "output.csv")### Simulation Settings Optimization./setup.sh



# Export analysis script

storage.export_analysis_script(result, "analysis.py", language="python")

```#### Temperature Parameter```- **Rich Attributes**: Age, gender, occupation, education, personality traits, values, etc.  - [Architecture Design](./docs/en/architecture/README.md)- **Async Processing**: High-performance parallel simulations



---



## ❓ FAQControls response randomness and creativity:



### How many LLM API calls are needed?



Call count = Number of personas × Number of questions. For example:- **0.0 - 0.3**: High consistency, suitable for standardized responses#### 3. Configure LLM- **Batch Creation**: Auto-generate samples matching real population distributions



- 10 personas × 9 questions = 90 calls- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)

- Using cache can significantly reduce repeat calls

- **0.8 - 1.0**: More diverse, suitable for exploratory research and creative testing

### How long does simulation take?



Depends on:

#### Max Tokens**Option A: Local LM Studio (Recommended for Learning and Development)**- **CSV Import/Export**: Bulk import personas from Excel or databases  - [Longitudinal Studies](./docs/en/longitudinal/README.md)- **Flexible LLM Support**: Local (LM Studio) or API (DeepSeek, OpenAI)

- **Local model**: ~5-15 seconds/response

- **Online API**: ~1-3 seconds/response

- **Parallel execution**: Can reduce time by 50-80%

- **150-300**: Short answers (multiple choice, scale ratings)

### How reliable are the results?

- **300-500**: Medium length (short answer questions)

LLM simulation is an exploratory research tool, suitable for:

- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)1. Download [LM Studio](https://lmstudio.ai/)- **Demo Templates**: Built-in templates for common persona types, ready to use

- ✅ Rapid prototype testing

- ✅ Hypothesis generation

- ✅ Questionnaire pre-testing

- ❌ **Cannot** replace real human research#### Parallel Settings2. Download a model in LM Studio:



### How to improve response quality?



1. Create detailed, realistic persona backgrounds- **Small Scale** (<10 personas): Concurrency 2-3   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`  - [Contributing Guide](./docs/en/contributing/README.md)- **Web Interface**: User-friendly Streamlit UI

2. Use clear, specific questions

3. Choose appropriate temperature parameters- **Medium Scale** (10-50 personas): Concurrency 5-10

4. Use more powerful models (e.g., GPT-4)

5. Enable response validation and consistency checks- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)   - Minimum: 7B parameter model



### What are the costs?



- **Local LM Studio**: Completely free (requires GPU)---3. Start local server:#### 2. Multiple Simulation Modes

- **DeepSeek API**: ~$0.0001/1k tokens, extremely low cost

- **OpenAI GPT-3.5**: ~$0.002/1k tokens

- **OpenAI GPT-4**: ~$0.03/1k tokens

## 🏗️ Architecture Design   - Click "Local Server" tab

### Is my data secure?



- Local mode: Data never leaves your machine

- API mode: Follows each provider's privacy policyFor detailed architecture documentation, see **[Architecture Guide](./docs/architecture/README.md)**   - Select model- [FAQ](#faq)- **Complete Data Export**: CSV, JSON formats for statistical analysis

- Recommendation: Use local mode for sensitive data



---

### Project Structure   - Click "Start Server"

## 🤝 Contributing



Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

```   - Confirm address is `http://localhost:1234`- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)

### Development Setup

auto_sim_ai/

```bash

# Install development dependencies├── app.py                      # Streamlit main application

pip install -r requirements-dev.txt

├── pages/                      # Multi-page application

# Run tests

pytest│   ├── 1_Setup.py             # Persona management page**Option B: Online API (Recommended for Production)**- **Intervention Mode**: Test health messages, ad copy, etc. on different populations- [License](#license)



# Code formatting│   ├── 2_Simulation.py        # Simulation execution page

black src/ tests/

isort src/ tests/│   └── 3_Results.py           # Results analysis page



# Type checking├── src/                        # Core modules

mypy src/

```│   ├── llm_client.py          # LLM client (sync/async)```bash- **A/B Testing**: Test multiple versions simultaneously and compare effects



### Report Issues│   ├── persona.py             # Persona management



Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).│   ├── simulation.py          # Simulation engine (single-thread/parallel)# Copy environment template



---│   ├── storage.py             # Results storage



## 📄 License│   ├── cache.py               # Response cachecp env.example .env- **Longitudinal Studies**: Multi-wave research to track changes over time### Quick Start (EN)



This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.│   ├── checkpoint.py          # Checkpoint management



---│   ├── scoring.py             # Auto-scoring



## 🙏 Acknowledgments│   ├── ab_testing.py          # A/B testing



- [Streamlit](https://streamlit.io/) - Excellent Python web framework│   ├── intervention_study.py  # Intervention studies (legacy)# Edit .env file and add API key- **Sensitivity Analysis**: Systematically test parameter impact on results

- [LM Studio](https://lmstudio.ai/) - Local LLM runtime environment

- [OpenAI](https://openai.com/) - API standards│   ├── longitudinal_study.py  # Longitudinal studies (new, recommended)

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM service

│   ├── persona_generator.py   # Persona generator# DEEPSEEK_API_KEY=your_api_key_here

---

│   ├── survey_templates.py    # Survey template library

## 📞 Contact

│   ├── survey_config.py       # Survey configuration# or---

- **Maintainer**: Jason Li

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)│   ├── tools.py               # Tool registration system

- **Email**: [Contact via GitHub Issues]

│   ├── ui_components.py       # UI components# OPENAI_API_KEY=your_api_key_here

---

│   ├── styles.py              # Design system

<div align="center">

│   └── validators.py          # Input validation```#### 3. LLM Integration

**⭐ If this project helps you, please give it a star!**

├── tests/                      # Test suite

Made with ❤️ by Jason Li

├── data/                       # Data directory

</div>

│   ├── personas/              # Persona data

│   ├── results/               # Simulation results#### 4. Launch Application```bash

│   ├── cache/                 # Cache data

│   ├── checkpoints/           # Checkpoints

│   └── survey_configs/        # Survey configurations

├── docs/                       # Documentation```bash- **Local Deployment**: LM Studio (free, completely private)

├── requirements.txt            # Dependencies

└── pytest.ini                 # Test configurationstreamlit run app.py

```

```- **Commercial APIs**:## 🎯 Overviewgit clone https://github.com/jason-jj-li/auto_sim_ai.git

### Core Module Descriptions



#### LLM Client (`llm_client.py`)

The app will automatically open in your browser: `http://localhost:8501`  - DeepSeek (cost-effective, Chinese-optimized)

Supports both synchronous and asynchronous modes:



- **LMStudioClient**: Sync client, suitable for simple scenarios

- **AsyncLLMClient**: Async client, supports high concurrency### First-Time User Guide  - OpenAI (GPT-4, GPT-3.5)cd auto_sim_ai



Compatible with OpenAI API format, seamless switching between providers.



#### Simulation Engine (`simulation.py`)1. **Connect LLM** (Home Page)  - Other OpenAI-compatible services



- **SimulationEngine**: Base engine, sequential execution   - Select LLM provider

- **ParallelSimulationEngine**: Parallel engine, supports async batch processing

   - Test connection- **Flexible Switching**: Change models or providers anytime**Auto Sim AI** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions../setup.sh

Automatically handles error retry, progress tracking, and result aggregation.

   - Wait for "System Ready" message

#### Cache System (`cache.py`)



Content-hash based smart caching:

2. **Create Virtual Personas** (Setup Page)

- Same persona + same question = directly return cached result

- Support cache export and import   - Click "Create Demo Personas" for quick start#### 4. Advanced Analysisstreamlit run app.py

- Significantly reduce LLM API call costs

   - Or manually create custom personas

#### Scoring System (`scoring.py`)

   - Or upload CSV for bulk import

Automated scoring features:



- Support multiple standardized scales

- Configurable custom scoring rules3. **Run Simulation** (Simulation Page)- **Auto-Scoring**: Built-in scoring for standardized scales### Use Cases```

- Auto-calculate total and subscale scores

   - Choose simulation type (Survey/Intervention)

---

   - Select personas to participate- **Statistical Analysis**: Descriptive stats, correlation, group comparisons

## 🔬 Advanced Features

   - Choose questionnaire template or enter custom questions

> 💡 **Tip**: For detailed API documentation and advanced features, see [API Guide](./docs/api/README.md)

   - Click "Run Simulation"- **Consistency Checks**: Validate response internal consistency and logic

### 1. A/B Testing



Compare intervention effectiveness across different versions:

4. **View Results** (Results Page)- **Visualization**: Interactive charts, word clouds, distributions

```python

from src import ABTestManager, Condition   - Browse response data



# Define test conditions   - View statistical analysis- **Export Features**: CSV, JSON, Python/R analysis scripts- 🏥 **Health Intervention Research** - Test health messaging impact on different populations📘 **[Complete English Documentation →](./docs/en/README.md)**

condition_a = Condition(

    name="Version A",   - Export results for further analysis

    intervention_text="Meditating 10 minutes daily can reduce stress.",

    questions=["Would you try this method?"]

)

---

condition_b = Condition(

    name="Version B", #### 5. Performance Optimization- 📊 **Market Research** - Rapidly evaluate product/service user feedback

    intervention_text="Research shows that meditating 10 minutes daily can reduce stress levels by 30%.",

    questions=["Would you try this method?"]## 📖 User Guide

)



# Run A/B test

ab_manager = ABTestManager()### Persona Design Best Practices

results = ab_manager.run_test([condition_a, condition_b], personas)

```- **Parallel Execution**: Async processing for multiple persona responses- 🎓 **Educational Research** - Assess teaching method effectiveness across learner types---



### 2. Longitudinal Studies (Multi-Wave Tracking)#### Creating High-Quality Personas



Implement realistic longitudinal tracking using conversation memory:- **Smart Caching**: Avoid redundant LLM calls, save time and cost



```python```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig

# Good Example: Specific, detailed, realistic- **Resume Capability**: Pause and resume large-scale simulations- 💡 **Policy Analysis** - Predict policy impact on diverse populations

# Configure study waves

waves = [{

    WaveConfig(

        wave_number=1,    "name": "Li Ming",- **Progress Tracking**: Real-time progress and estimated completion time

        wave_name="Baseline",

        questions=["What is your current stress level? (1-10)"],    "age": 32,

        days_from_baseline=0

    ),    "gender": "Male",- 🧪 **A/B Testing** - Compare effectiveness of different approaches## 中文

    WaveConfig(

        wave_number=2,    "occupation": "Software Engineer at Startup",

        wave_name="1 Month Follow-up",

        questions=["What is your stress level now? (1-10)"],    "education": "Bachelor's in Computer Science",---

        days_from_baseline=30,

        intervention_text="Practice 10 minutes of meditation daily"    "location": "Beijing",

    )

]    "background": "Works at a fast-growing tech company, often works overtime. Recently feeling work stress and declining sleep quality. Enjoys relieving stress through exercise but often too busy to do so.",- 📈 **Prototype Validation** - Rapidly iterate designs before real-world research



# Run longitudinal study    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],

config = LongitudinalStudyConfig(

    study_id="stress_study",    "values": ["Career development", "Work-life balance", "Family health"]## ⚡ Quick Start

    study_name="Stress Intervention Study",

    waves=waves}

)

🔬 **基于大语言模型的调查与干预模拟系统**

engine = LongitudinalStudyEngine(llm_client)

results = engine.run_study(personas, config)# Bad Example: Vague, generic

```

{### System Requirements

For detailed longitudinal study guide, see **[Longitudinal Study Guide](./docs/longitudinal/README.md)**

    "name": "John Doe",

### 3. Batch Persona Generation

    "age": 30,### Key Advantages

Generate virtual samples based on real demographic distributions:

    "gender": "Male",

```python

from src import PersonaGenerator, DistributionConfig    "occupation": "Engineer",- **Python**: 3.8 or higher



# Configure distribution    "background": "Regular person",

config = DistributionConfig(

    age_distribution={    "personality_traits": ["Normal"],- **Memory**: 8GB+ recommended使用 AI 驱动的虚拟人物模拟真实的调查研究和干预效果

        "18-30": 0.3,

        "31-50": 0.4,    "values": ["Happiness"]

        "51-70": 0.3

    },}- **LLM Provider** (choose one):

    gender_distribution={

        "Male": 0.48,```

        "Female": 0.52

    }  - LM Studio (local, free)✅ **Fast Iteration** - Complete hundreds of survey simulations in minutes  

)

#### Persona Diversity

# Generate 100 personas

generator = PersonaGenerator()  - DeepSeek/OpenAI API key

personas = generator.generate_batch(

    count=100,Ensure virtual samples reflect real population diversity:

    distribution_config=config,

    llm_client=client✅ **Cost-Effective** - No need to recruit real participants  📗 **[查看完整中文文档](./docs/zh/README.md)**

)

```- **Age**: Cover different age groups (18-80 years)



### 4. Response Validation- **Gender**: Male, female, non-binary### Installation Steps



Automatically check response quality and consistency:- **Occupation**: Different industries and job levels



```python- **Education**: High school to graduate degrees✅ **Reproducible** - Precise variable control ensures repeatability  

from src import ResponseValidator, ConsistencyChecker

- **Geography**: Urban, rural, different regions

validator = ResponseValidator()

checker = ConsistencyChecker()- **Cultural Background**: Different ethnicities, religions, cultural traditions#### Step 1: Clone Repository



# Validate response format

is_valid = validator.validate_response(response, question_type)

### Questionnaire Design Tips✅ **Diverse** - Easily create personas with varied backgrounds, ages, cultures  [快速开始](./docs/zh/quickstart/README.md) •

# Check consistency

metrics = checker.check_consistency(persona_responses)

print(f"Consistency score: {metrics.consistency_score}")

```#### Good Question Characteristics```bash



---



## 📚 API Documentation✅ **Clear and Specific**git clone https://github.com/jason-jj-li/auto_sim_ai.git✅ **Deep Insights** - Obtain detailed qualitative and quantitative data  [功能特性](#功能特性) •



### PersonaManager



```python```cd auto_sim_ai

from src import PersonaManager

Good: In the past two weeks, how many days have you felt down or depressed?

manager = PersonaManager()

Bad: How have you been feeling lately?```✅ **Flexible Deployment** - Support for local and cloud API deployments[API 参考](./docs/zh/api/README.md) •

# Add persona

manager.add_persona(persona)```



# Get all personas

personas = manager.get_all_personas()

✅ **Avoid Compound Questions**

# Filter by criteria

young_adults = manager.filter_personas(#### Step 2: Install Dependencies[贡献指南](./docs/zh/contributing/README.md)

    age_range=(18, 30),

    gender="Female"```

)

Good: How many times per week do you exercise? How long is each exercise session?

# Save/load

manager.save_to_file("personas.json")Bad: How often do you exercise, for how long, and at what intensity?

manager.load_from_file("personas.json")

``````**Option A: Automated Setup (Recommended)**---



### SimulationEngine



```python✅ **Use Standardized Scales**

from src import SimulationEngine



engine = SimulationEngine(

    llm_client=client,``````bash</div>

    cache=cache,

    checkpoint_manager=checkpoint_mgrNever(0) - Sometimes(1) - Often(2) - Always(3)

)

```chmod +x setup.sh

# Run survey

result = engine.run_survey(

    personas=personas,

    questions=questions,#### Use Built-in Templates./setup.sh## 🚀 Features

    temperature=0.7,

    max_tokens=300

)

The system includes multiple validated standardized scales:```

# Run intervention

result = engine.run_intervention(

    personas=personas,

    intervention_text="Health intervention text",- **PHQ-9**: Depression screening scale---

    questions=followup_questions

)- **GAD-7**: Anxiety screening scale

```

- **PSS-10**: Perceived Stress Scale**Option B: Manual Installation**

### ResultsStorage

- More templates continuously being added...

```python

from src import ResultsStorage### Core Capabilities



storage = ResultsStorage()### Simulation Settings Optimization



# Save result```bash

storage.save_result(simulation_result)

#### Temperature Parameter

# Load results

results = storage.load_all_results()# Create virtual environment## 📋 目录



# Export to CSVControls response randomness and creativity:

storage.export_to_csv(result, "output.csv")

python -m venv venv

# Export analysis script

storage.export_analysis_script(result, "analysis.py", language="python")- **0.0 - 0.3**: High consistency, suitable for standardized responses

```

- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)source venv/bin/activate  # Windows: venv\Scripts\activate#### 1️⃣ Virtual Persona Management

---

- **0.8 - 1.0**: More diverse, suitable for exploratory research and creative testing

## ❓ FAQ



### How many LLM API calls are needed?

#### Max Tokens

Call count = Number of personas × Number of questions. For example:

# Install dependencies- [项目简介](#项目简介)

- 10 personas × 9 questions = 90 calls

- Using cache can significantly reduce repeat calls- **150-300**: Short answers (multiple choice, scale ratings)



### How long does simulation take?- **300-500**: Medium length (short answer questions)pip install -r requirements.txt



Depends on:- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)



- **Local model**: ~5-15 seconds/response```- **Rich Attributes**: Age, gender, occupation, education, personality traits, values, etc.- [功能特性](#功能特性)

- **Online API**: ~1-3 seconds/response

- **Parallel execution**: Can reduce time by 50-80%#### Parallel Settings



### How reliable are the results?



LLM simulation is an exploratory research tool, suitable for:- **Small Scale** (<10 personas): Concurrency 2-3



- ✅ Rapid prototype testing- **Medium Scale** (10-50 personas): Concurrency 5-10#### Step 3: Configure LLM- **Batch Creation**: Auto-generate samples matching real population distributions- [快速开始](#快速开始)

- ✅ Hypothesis generation

- ✅ Questionnaire pre-testing- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)

- ❌ **Cannot** replace real human research



### How to improve response quality?

---

1. Create detailed, realistic persona backgrounds

2. Use clear, specific questions**Option A: Local LM Studio (Recommended for Learning)**- **CSV Import/Export**: Bulk import personas from Excel or databases- [使用指南](#使用指南)

3. Choose appropriate temperature parameters

4. Use more powerful models (e.g., GPT-4)## 🏗️ Architecture Design

5. Enable response validation and consistency checks



### What are the costs?

For detailed architecture documentation, see **[Architecture Guide](./docs/architecture/README.md)**

- **Local LM Studio**: Completely free (requires GPU)

- **DeepSeek API**: ~$0.0001/1k tokens, extremely low cost1. Download [LM Studio](https://lmstudio.ai/)- **Demo Templates**: Built-in templates for common persona types, ready to use- [📚 完整文档](./docs/README.md)

- **OpenAI GPT-3.5**: ~$0.002/1k tokens

- **OpenAI GPT-4**: ~$0.03/1k tokens### Project Structure



### Is my data secure?2. Download a model in LM Studio:



- Local mode: Data never leaves your machine```

- API mode: Follows each provider's privacy policy

- Recommendation: Use local mode for sensitive dataauto_sim_ai/   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`  - [快速开始指南](./docs/quickstart/README.md)



---├── app.py                      # Streamlit main application



## 🤝 Contributing├── pages/                      # Multi-page application   - Minimum: 7B parameter model



Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.│   ├── 1_Setup.py             # Persona management page



### Development Setup│   ├── 2_Simulation.py        # Simulation execution page3. Start local server:#### 2️⃣ Multiple Simulation Modes  - [API文档](./docs/api/README.md)



```bash│   └── 3_Results.py           # Results analysis page

# Install development dependencies

pip install -r requirements-dev.txt├── src/                        # Core modules   - Click "Local Server" tab



# Run tests│   ├── llm_client.py          # LLM client (sync/async)

pytest

│   ├── persona.py             # Persona management   - Select model  - [架构设计](./docs/architecture/README.md)

# Code formatting

black src/ tests/│   ├── simulation.py          # Simulation engine (single-thread/parallel)

isort src/ tests/

│   ├── storage.py             # Results storage   - Click "Start Server"

# Type checking

mypy src/│   ├── cache.py               # Response cache

```

│   ├── checkpoint.py          # Checkpoint management   - Confirm address is `http://localhost:1234`- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)  - [纵向研究指南](./docs/longitudinal/README.md)

### Report Issues

│   ├── scoring.py             # Auto-scoring

Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).

│   ├── ab_testing.py          # A/B testing

---

│   ├── intervention_study.py  # Intervention studies (legacy)

## 📄 License

│   ├── longitudinal_study.py  # Longitudinal studies (new, recommended)**Option B: Online API (Recommended for Production)**- **Intervention Mode**: Test health messages, ad copy, etc. on different populations  - [贡献指南](./docs/contributing/README.md)

This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.

│   ├── persona_generator.py   # Persona generator

---

│   ├── survey_templates.py    # Survey template library

## 🙏 Acknowledgments

│   ├── survey_config.py       # Survey configuration

- [Streamlit](https://streamlit.io/) - Excellent Python web framework

- [LM Studio](https://lmstudio.ai/) - Local LLM runtime environment│   ├── tools.py               # Tool registration system```bash- **A/B Testing**: Test multiple versions simultaneously and compare effects- [常见问题](#常见问题)

- [OpenAI](https://openai.com/) - API standards

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM service│   ├── ui_components.py       # UI components



---│   ├── styles.py              # Design system# Copy environment template



## 📞 Contact│   └── validators.py          # Input validation



- **Maintainer**: Jason Li├── tests/                      # Test suitecp env.example .env- **Longitudinal Studies**: Multi-wave research to track changes over time- [许可证](#许可证)

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)

- **Email**: [Contact via GitHub Issues]├── data/                       # Data directory



---│   ├── personas/              # Persona data



<div align="center">│   ├── results/               # Simulation results



**⭐ If this project helps you, please give it a star!**│   ├── cache/                 # Cache data# Edit .env file and add your API key- **Sensitivity Analysis**: Systematically test parameter impact on results



Made with ❤️ by Jason Li│   ├── checkpoints/           # Checkpoints



</div>│   └── survey_configs/        # Survey configurations# DEEPSEEK_API_KEY=your_api_key_here

├── docs/                       # Documentation

├── requirements.txt            # Dependencies# or---

└── pytest.ini                 # Test configuration

```# OPENAI_API_KEY=your_api_key_here



### Core Module Descriptions```#### 3️⃣ LLM Integration



#### LLM Client (`llm_client.py`)



Supports both synchronous and asynchronous modes:#### Step 4: Launch Application## 🎯 项目简介



- **LMStudioClient**: Sync client, suitable for simple scenarios

- **AsyncLLMClient**: Async client, supports high concurrency

```bash- **Local Deployment**: LM Studio (free, completely private)

Compatible with OpenAI API format, seamless switching between providers.

streamlit run app.py

#### Simulation Engine (`simulation.py`)

```- **Commercial APIs**:**LLM Simulation Survey System** 是一个创新的研究工具，利用大语言模型（LLM）生成虚拟人物（Personas），模拟真实人群对调查问卷和干预措施的响应。

- **SimulationEngine**: Base engine, sequential execution

- **ParallelSimulationEngine**: Parallel engine, supports async batch processing



Automatically handles error retry, progress tracking, and result aggregation.The app will automatically open in your browser at: `http://localhost:8501`  - DeepSeek (cost-effective, Chinese-optimized)



#### Cache System (`cache.py`)



Content-hash based smart caching:### First-Time User Guide  - OpenAI (GPT-4, GPT-3.5)### 适用场景



- Same persona + same question = directly return cached result

- Support cache export and import

- Significantly reduce LLM API call costs**1. Connect LLM (Home Page)**  - Other OpenAI-compatible services



#### Scoring System (`scoring.py`)   - Select your LLM provider



Automated scoring features:   - Test the connection- **Flexible Switching**: Change models or providers anytime- 🏥 **健康干预研究** - 测试健康信息对不同人群的影响



- Support multiple standardized scales   - Wait for "System Ready" message

- Configurable custom scoring rules

- Auto-calculate total and subscale scores- 📊 **市场调研** - 快速评估产品或服务的用户反馈



---**2. Create Virtual Personas (Setup Page)**



## 🔬 Advanced Features   - Click "Create Demo Personas" for quick start#### 4️⃣ Advanced Analysis- 🎓 **教育研究** - 评估教学方法对不同学习者的效果



> 💡 **Tip**: For detailed API documentation and advanced features, see [API Guide](./docs/api/README.md)   - Or manually create custom personas



### 1. A/B Testing   - Or upload CSV for bulk import- 💡 **政策分析** - 预测政策对多元群体的潜在影响



Compare intervention effectiveness across different versions:



```python**3. Run Simulation (Simulation Page)**- **Auto-Scoring**: Built-in scoring for standardized scales- 🧪 **A/B 测试** - 比较不同方案的效果差异

from src import ABTestManager, Condition

   - Choose simulation type (Survey/Intervention/A/B Test)

# Define test conditions

condition_a = Condition(   - Select personas to participate- **Statistical Analysis**: Descriptive stats, correlation, group comparisons- 📈 **原型验证** - 在真实调研前快速迭代设计

    name="Version A",

    intervention_text="Meditating 10 minutes daily can reduce stress.",   - Choose questionnaire template or enter custom questions

    questions=["Would you try this method?"]

)   - Click "Run Simulation"- **Consistency Checks**: Validate response internal consistency and logic



condition_b = Condition(

    name="Version B", 

    intervention_text="Research shows that meditating 10 minutes daily can reduce stress levels by 30%.",**4. View Results (Results Page)**- **Visualization**: Interactive charts, word clouds, distributions### 核心优势

    questions=["Would you try this method?"]

)   - Browse response data



# Run A/B test   - View statistical analysis- **Export Features**: CSV, JSON, Python/R analysis scripts

ab_manager = ABTestManager()

results = ab_manager.run_test([condition_a, condition_b], personas)   - Export results for further analysis

```

✅ **快速迭代** - 几分钟内完成数百人的调查模拟  

### 2. Longitudinal Studies (Multi-Wave Tracking)

---

Implement realistic longitudinal tracking using conversation memory:

#### 5️⃣ Performance Optimization✅ **成本低廉** - 无需招募真实参与者  

```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig## 📖 User Guide



# Configure study waves✅ **可重复性** - 精确控制变量，确保实验可重复  

waves = [

    WaveConfig(### Persona Design Best Practices

        wave_number=1,

        wave_name="Baseline",- **Parallel Execution**: Async processing for multiple persona responses✅ **多样化** - 轻松创建不同背景、年龄、文化的虚拟人物  

        questions=["What is your current stress level? (1-10)"],

        days_from_baseline=0#### Creating High-Quality Personas

    ),

    WaveConfig(- **Smart Caching**: Avoid redundant LLM calls, save time and cost✅ **深度洞察** - 获得详细的质性和量化数据  

        wave_number=2,

        wave_name="1 Month Follow-up",**Good Example: Specific, detailed, realistic**

        questions=["What is your stress level now? (1-10)"],

        days_from_baseline=30,- **Resume Capability**: Pause and resume large-scale simulations✅ **灵活部署** - 支持本地运行和云端API

        intervention_text="Practice 10 minutes of meditation daily"

    )```python

]

{- **Progress Tracking**: Real-time progress and estimated completion time

# Run longitudinal study

config = LongitudinalStudyConfig(    "name": "Sarah Chen",

    study_id="stress_study",

    study_name="Stress Intervention Study",    "age": 32,---

    waves=waves

)    "gender": "Female",



engine = LongitudinalStudyEngine(llm_client)    "occupation": "Software Engineer at startup",---

results = engine.run_study(personas, config)

```    "education": "Bachelor's in Computer Science",



For detailed longitudinal study guide, see **[Longitudinal Study Guide](./docs/longitudinal/README.md)**    "location": "San Francisco, CA",## 🚀 功能特性



### 3. Batch Persona Generation    "background": "Works at a fast-growing tech company, often works overtime. Recently experiencing work stress and sleep quality decline. Likes to relieve stress through exercise but often too busy.",



Generate virtual samples based on real demographic distributions:    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],## ⚡ Quick Start



```python    "values": ["Career development", "Work-life balance", "Family health"]

from src import PersonaGenerator, DistributionConfig

}### 核心功能

# Configure distribution

config = DistributionConfig(```

    age_distribution={

        "18-30": 0.3,### System Requirements

        "31-50": 0.4,

        "51-70": 0.3**Bad Example: Vague, generic**

    },

    gender_distribution={#### 1️⃣ 虚拟人物管理

        "Male": 0.48,

        "Female": 0.52```python

    }

){- **Python**: 3.8 or higher- **丰富的人物属性**：年龄、性别、职业、教育背景、性格特征、价值观等



# Generate 100 personas    "name": "John Doe",

generator = PersonaGenerator()

personas = generator.generate_batch(    "age": 30,- **Memory**: 8GB+ recommended- **批量创建**：使用人口统计分布自动生成符合真实人口的虚拟样本

    count=100,

    distribution_config=config,    "gender": "Male",

    llm_client=client

)    "occupation": "Engineer",- **LLM Provider** (choose one):- **CSV 导入/导出**：支持从Excel或数据库批量导入人物

```

    "background": "Regular person",

### 4. Response Validation

    "personality_traits": ["Normal"],  - LM Studio (local, free)- **演示模板**：内置多种典型人物模板，即开即用

Automatically check response quality and consistency:

    "values": ["Happiness"]

```python

from src import ResponseValidator, ConsistencyChecker}  - DeepSeek/OpenAI API key



validator = ResponseValidator()```

checker = ConsistencyChecker()

#### 2️⃣ 多种模拟模式

# Validate response format

is_valid = validator.validate_response(response, question_type)#### Persona Diversity Guidelines



# Check consistency### Installation- **调查模式**：运行标准化问卷（PHQ-9、GAD-7 等）

metrics = checker.check_consistency(persona_responses)

print(f"Consistency score: {metrics.consistency_score}")Ensure your virtual sample reflects real population diversity:

```

- **干预模式**：测试健康信息、广告文案等对不同人群的影响

---

- **Age**: Cover different age groups (18-80 years)

## 📚 API Documentation

- **Gender**: Male, female, non-binary#### 1. Clone Repository- **A/B 测试**：同时测试多个版本，比较效果差异

### PersonaManager

- **Occupation**: Different industries and position levels

```python

from src import PersonaManager- **Education**: High school to graduate degrees- **纵向研究**：模拟多波次调查，追踪变化趋势



manager = PersonaManager()- **Geography**: Urban, rural, different regions



# Add persona- **Cultural Background**: Different ethnicities, religions, cultural traditions```bash- **敏感性分析**：系统性测试参数变化对结果的影响

manager.add_persona(persona)



# Get all personas

personas = manager.get_all_personas()### Questionnaire Design Tipsgit clone https://github.com/jason-jj-li/auto_sim_ai.git



# Filter by criteria

young_adults = manager.filter_personas(

    age_range=(18, 30),#### Characteristics of Good Questionscd auto_sim_ai#### 3️⃣ LLM 集成

    gender="Female"

)



# Save/load✅ **Clear and Specific**```- **本地部署**：LM Studio（免费，完全私密）

manager.save_to_file("personas.json")

manager.load_from_file("personas.json")

```

```- **商业API**：

### SimulationEngine

Good: "In the past two weeks, how many days have you felt down or depressed?"

```python

from src import SimulationEngineBad: "How have you been feeling lately?"#### 2. Install Dependencies  - DeepSeek（高性价比，中文优化）



engine = SimulationEngine(```

    llm_client=client,

    cache=cache,  - OpenAI（GPT-4、GPT-3.5）

    checkpoint_manager=checkpoint_mgr

)✅ **Avoid Compound Questions**



# Run survey```bash  - 其他 OpenAI 兼容服务

result = engine.run_survey(

    personas=personas,```

    questions=questions,

    temperature=0.7,Good: "How many times per week do you exercise?" + "How long is each exercise session?"# Create virtual environment (recommended)- **灵活切换**：随时更换模型或提供商

    max_tokens=300

)Bad: "How often do you exercise, for how long, and at what intensity?"



# Run intervention```python -m venv venv

result = engine.run_intervention(

    personas=personas,

    intervention_text="Health intervention text",

    questions=followup_questions✅ **Use Standardized Scales**source venv/bin/activate  # Windows: venv\Scripts\activate#### 4️⃣ 高级分析

)

```



### ResultsStorage```- **自动评分**：内置标准化量表的自动评分系统



```pythonNever(0) - Sometimes(1) - Often(2) - Always(3)

from src import ResultsStorage

```# Install dependencies- **统计分析**：描述统计、相关分析、组间比较

storage = ResultsStorage()



# Save result

storage.save_result(simulation_result)#### Built-in Templatespip install -r requirements.txt- **一致性检查**：验证响应的内部一致性和逻辑性



# Load results

results = storage.load_all_results()

The system includes validated standardized scales:```- **可视化**：交互式图表、词云、分布图

# Export to CSV

storage.export_to_csv(result, "output.csv")



# Export analysis script- **PHQ-9**: Depression screening scale- **导出功能**：CSV、JSON、Python/R 分析脚本

storage.export_analysis_script(result, "analysis.py", language="python")

```- **GAD-7**: Anxiety screening scale  



---- **PSS-10**: Perceived Stress ScaleOr use the setup script:



## ❓ FAQ- More templates continuously being added...



### Q: How many LLM API calls are needed?#### 5️⃣ 性能优化



A: Call count = Number of personas × Number of questions. For example:### Simulation Settings Optimization



- 10 personas × 9 questions = 90 calls```bash- **并行执行**：异步处理多个人物的响应

- Using cache can significantly reduce repeat calls

#### Temperature Parameter

### Q: How long does simulation take?

chmod +x setup.sh- **智能缓存**：避免重复调用 LLM，节省时间和成本

A: Depends on:

Controls response randomness and creativity:

- **Local model**: ~5-15 seconds/response

- **Online API**: ~1-3 seconds/response./setup.sh- **断点续传**：支持暂停和恢复大规模模拟

- **Parallel execution**: Can reduce time by 50-80%

- **0.0 - 0.3**: High consistency, suitable for standardized responses

### Q: How reliable are the results?

- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)```- **进度追踪**：实时显示模拟进度和预估完成时间

A: LLM simulation is an exploratory research tool, suitable for:

- **0.8 - 1.0**: More diverse, suitable for exploratory research

- ✅ Rapid prototype testing

- ✅ Hypothesis generation

- ✅ Questionnaire pre-testing

- ❌ **Cannot** replace real human research#### Max Tokens



### Q: How to improve response quality?#### 3. Configure LLM---



1. Create detailed, realistic persona backgrounds- **150-300**: Short answers (multiple choice, scale ratings)

2. Use clear, specific questions

3. Choose appropriate temperature parameters- **300-500**: Medium length (short answer questions)

4. Use more powerful models (e.g., GPT-4)

5. Enable response validation and consistency checks- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)



### Q: What are the costs?**Option A: Local LM Studio (recommended for learning/development)**## ⚡ 快速开始



- **Local LM Studio**: Completely free (requires GPU)#### Parallel Settings

- **DeepSeek API**: ~$0.0001/1k tokens, extremely low cost

- **OpenAI GPT-3.5**: ~$0.002/1k tokens

- **OpenAI GPT-4**: ~$0.03/1k tokens

- **Small Scale** (<10 personas): Concurrency 2-3

### Q: Is my data secure?

- **Medium Scale** (10-50 personas): Concurrency 5-101. Download [LM Studio](https://lmstudio.ai/)### 系统要求

- Local mode: Data never leaves your machine

- API mode: Follows each provider's privacy policy- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)

- Recommendation: Use local mode for sensitive data

2. Download a model in LM Studio:

---

---

## 🤝 Contributing

   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`- **Python**: 3.8 或更高版本

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

## 🏗️ Architecture

### Development Setup

   - Minimum: 7B parameter model- **内存**: 建议 8GB 以上

```bash

# Install development dependenciesFor detailed architecture documentation, see **[Architecture Guide](./docs/en/architecture/README.md)**

pip install -r requirements-dev.txt

3. Start local server:- **LLM 提供商**（任选其一）：

# Run tests

pytest### Project Structure



# Code formatting   - Click "Local Server" tab  - LM Studio（本地运行，免费）

black src/ tests/

isort src/ tests/```



# Type checkingauto_sim_ai/   - Select model  - DeepSeek/OpenAI API 密钥

mypy src/

```├── app.py                      # Streamlit main application



### Report Issues├── pages/                      # Multi-page application   - Click "Start Server"



Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).│   ├── 1_Setup.py             # Persona management



---│   ├── 2_Simulation.py        # Run simulations   - Confirm address is `http://localhost:1234`### 安装步骤



## 📄 License│   └── 3_Results.py           # View results



This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.├── src/                        # Core modules



---│   ├── llm_client.py          # LLM client (sync/async)



## 🙏 Acknowledgments│   ├── persona.py             # Persona management**Option B: Online API (recommended for production)**#### 1. 克隆项目



- [Streamlit](https://streamlit.io/) - Excellent Python web framework│   ├── simulation.py          # Simulation engine

- [LM Studio](https://lmstudio.ai/) - Local LLM runtime environment

- [OpenAI](https://openai.com/) - API standards│   ├── storage.py             # Results storage

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM service

│   ├── cache.py               # Response caching

---

│   ├── scoring.py             # Auto-scoring```bash```bash

## 📞 Contact

│   └── ...                    # Additional modules

- **Maintainer**: Jason Li

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)├── tests/                      # Test suite# Copy environment templategit clone https://github.com/jason-jj-li/auto_sim_ai.git

- **Email**: [Contact via GitHub Issues]

├── data/                       # Data directory

---

│   ├── personas/              # Persona datacp env.example .envcd auto_sim_ai

<div align="center">

│   ├── results/               # Simulation results

**⭐ If this project helps you, please give it a star!**

│   └── cache/                 # Cached responses```

Made with ❤️ by Jason Li

└── docs/                       # Documentation

</div>

```# Edit .env file, add API key



### Core Modules# DEEPSEEK_API_KEY=your_api_key_here#### 2. 安装依赖



**LLM Client** (`llm_client.py`)# or

- Synchronous and asynchronous modes

- Compatible with OpenAI API format# OPENAI_API_KEY=your_api_key_here```bash

- Seamless switching between providers

```# 创建虚拟环境（推荐）

**Simulation Engine** (`simulation.py`)

- Sequential and parallel executionpython -m venv venv

- Automatic error retry and progress tracking

- Result aggregation#### 4. Launch Applicationsource venv/bin/activate  # Windows: venv\Scripts\activate



**Cache System** (`cache.py`)

- Content-hash based smart caching

- Significantly reduce API costs```bash# 安装依赖

- Import/export capability

streamlit run app.pypip install -r requirements.txt

**Scoring System** (`scoring.py`)

- Automated scoring for standardized scales``````

- Configurable custom rules

- Total and subscale calculations



---The app will automatically open in your browser: `http://localhost:8501`或使用安装脚本：



## 🔬 Advanced Features```bash



> 💡 **See [API Guide](./docs/en/api/README.md) for detailed documentation**### First-Time User Guidechmod +x setup.sh



### 1. A/B Testing./setup.sh



Compare different intervention versions:1. **Connect LLM** (Home Page)```



```python   - Select LLM provider

from src import ABTestManager, Condition

   - Test connection#### 3. 配置 LLM

# Define test conditions

condition_a = Condition(   - Wait for "System Ready" message

    name="Version A",

    intervention_text="Meditating 10 minutes daily can reduce stress.",**方式 A：本地 LM Studio（推荐用于学习和开发）**

    questions=["Would you try this method?"]

)2. **Create Virtual Personas** (Setup Page)



condition_b = Condition(   - Click "Create Demo Personas" for quick start1. 下载 [LM Studio](https://lmstudio.ai/)

    name="Version B", 

    intervention_text="Research shows daily 10-minute meditation reduces stress by 30%.",   - Or manually create custom personas2. 在 LM Studio 中下载模型：

    questions=["Would you try this method?"]

)   - Or upload CSV for bulk import   - 推荐：`mistral-7b-instruct`、`llama-2-7b-chat`



# Run A/B test   - 最低：7B 参数模型

ab_manager = ABTestManager()

results = ab_manager.run_test([condition_a, condition_b], personas)3. **Run Simulation** (Simulation Page)3. 启动本地服务器：

```

   - Choose simulation type (Survey/Intervention)   - 点击 "Local Server" 标签

### 2. Longitudinal Studies

   - Select personas to participate   - 选择模型

Multi-wave tracking with conversation memory:

   - Choose questionnaire template or enter custom questions   - 点击 "Start Server"

```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig   - Click "Run Simulation"   - 确认地址为 `http://localhost:1234`



# Configure study waves

waves = [

    WaveConfig(4. **View Results** (Results Page)**方式 B：在线 API（推荐用于生产环境）**

        wave_number=1,

        wave_name="Baseline",   - Browse response data

        questions=["What is your current stress level? (1-10)"],

        days_from_baseline=0   - View statistical analysis```bash

    ),

    WaveConfig(   - Export results for further analysis# 复制环境变量模板

        wave_number=2,

        wave_name="1 Month Follow-up",cp env.example .env

        questions=["What is your stress level now? (1-10)"],

        days_from_baseline=30,---

        intervention_text="Practice 10 minutes of meditation daily"

    )# 编辑 .env 文件，添加 API 密钥

]

## 📖 User Guide# DEEPSEEK_API_KEY=your_api_key_here

# Run study

config = LongitudinalStudyConfig(# 或

    study_id="stress_study",

    study_name="Stress Intervention Study",### Persona Design Best Practices# OPENAI_API_KEY=your_api_key_here

    waves=waves

)```



engine = LongitudinalStudyEngine(llm_client)#### Creating High-Quality Personas

results = engine.run_study(personas, config)

```#### 4. 启动应用



See **[Longitudinal Study Guide](./docs/en/longitudinal/README.md)** for details.```python



### 3. Batch Persona Generation# Good Example: Specific, detailed, realistic```bash



Generate samples based on real demographic distributions:{streamlit run app.py



```python    "name": "Sarah Chen",```

from src import PersonaGenerator, DistributionConfig

    "age": 32,

# Configure distribution

config = DistributionConfig(    "gender": "Female",应用将在浏览器中自动打开：`http://localhost:8501`

    age_distribution={"18-30": 0.3, "31-50": 0.4, "51-70": 0.3},

    gender_distribution={"Male": 0.48, "Female": 0.52}    "occupation": "Software Engineer at startup",

)

    "education": "Bachelor's in Computer Science",### 首次使用指南

# Generate 100 personas

generator = PersonaGenerator()    "location": "San Francisco, CA",

personas = generator.generate_batch(

    count=100,    "background": "Works at a fast-growing tech company, often works overtime. Recently experiencing work stress and sleep quality decline. Likes to relieve stress through exercise but often too busy.",1. **连接 LLM**（首页）

    distribution_config=config,

    llm_client=client    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],   - 选择 LLM 提供商

)

```    "values": ["Career development", "Work-life balance", "Family health"]   - 测试连接



### 4. Response Validation}   - 等待"系统就绪"提示



Check response quality and consistency:



```python# Bad Example: Vague, generic2. **创建虚拟人物**（Setup 页面）

from src import ResponseValidator, ConsistencyChecker

{   - 点击 "Create Demo Personas" 快速创建

validator = ResponseValidator()

checker = ConsistencyChecker()    "name": "John Doe",   - 或手动创建自定义人物



# Validate format    "age": 30,   - 或上传 CSV 批量导入

is_valid = validator.validate_response(response, question_type)

    "gender": "Male",

# Check consistency

metrics = checker.check_consistency(persona_responses)    "occupation": "Engineer",3. **运行模拟**（Simulation 页面）

print(f"Consistency score: {metrics.consistency_score}")

```    "background": "Regular person",   - 选择模拟类型（调查/干预）



---    "personality_traits": ["Normal"],   - 选择要参与的人物



## 📚 API Documentation    "values": ["Happiness"]   - 选择问卷模板或输入自定义问题



### PersonaManager}   - 点击 "Run Simulation"



```python```

from src import PersonaManager

4. **查看结果**（Results 页面）

manager = PersonaManager()

#### Persona Diversity   - 浏览响应数据

# Add persona

manager.add_persona(persona)   - 查看统计分析



# Get all personasEnsure virtual samples reflect real population diversity:   - 导出结果用于进一步分析

personas = manager.get_all_personas()



# Filter by criteria

young_adults = manager.filter_personas(- **Age**: Cover different age groups (18-80 years)---

    age_range=(18, 30),

    gender="Female"- **Gender**: Male, female, non-binary

)

- **Occupation**: Different industries and position levels## 📖 使用指南

# Save/load

manager.save_to_file("personas.json")- **Education**: High school to graduate degrees

manager.load_from_file("personas.json")

```- **Geography**: Urban, rural, different regions### 虚拟人物设计最佳实践



### SimulationEngine- **Cultural Background**: Different ethnicities, religions, cultural traditions



```python#### 创建高质量人物

from src import SimulationEngine

### Questionnaire Design Tips

engine = SimulationEngine(

    llm_client=client,```python

    cache=cache,

    checkpoint_manager=checkpoint_mgr#### Good Question Characteristics# 好的例子：具体、详细、真实

)

{

# Run survey

result = engine.run_survey(✅ **Clear and Specific**    "name": "李明",

    personas=personas,

    questions=questions,    "age": 32,

    temperature=0.7,

    max_tokens=300```    "gender": "男",

)

Good: In the past two weeks, how many days have you felt down or depressed?    "occupation": "初创公司软件工程师",

# Run intervention

result = engine.run_intervention(Bad: How have you been feeling lately?    "education": "本科计算机科学",

    personas=personas,

    intervention_text="Health intervention text",```    "location": "北京",

    questions=followup_questions

)    "background": "在一家快速成长的科技公司工作，经常加班。最近感到工作压力大，睡眠质量下降。喜欢通过运动缓解压力，但工作繁忙常常没时间。",

```

✅ **Avoid Compound Questions**    "personality_traits": ["完美主义", "责任心强", "有些焦虑"],

### ResultsStorage

    "values": ["职业发展", "工作生活平衡", "家庭健康"]

```python

from src import ResultsStorage```}



storage = ResultsStorage()Good: How many times per week do you exercise? How long is each exercise session?



# Save resultBad: How often do you exercise, for how long, and at what intensity?# 不好的例子：模糊、一般化

storage.save_result(simulation_result)

```{

# Load results

results = storage.load_all_results()    "name": "张三",



# Export to CSV✅ **Use Standardized Scales**    "age": 30,

storage.export_to_csv(result, "output.csv")

    "gender": "男",

# Export analysis script

storage.export_analysis_script(result, "analysis.py", language="python")```    "occupation": "工程师",

```

Never(0) - Sometimes(1) - Often(2) - Always(3)    "background": "普通人",

---

```    "personality_traits": ["正常"],

## ❓ FAQ

    "values": ["幸福"]

### How many LLM API calls are needed?

#### Use Built-in Templates}

Call count = Number of personas × Number of questions

```

Example:

- 10 personas × 9 questions = 90 callsThe system includes validated standardized scales:

- Caching significantly reduces repeat calls

#### 人物多样性

### How long does simulation take?

- **PHQ-9**: Depression screening scale

Depends on:

- **Local model**: ~5-15 seconds/response- **GAD-7**: Anxiety screening scale确保虚拟样本反映真实人口的多样性：

- **Online API**: ~1-3 seconds/response

- **Parallel execution**: Can reduce time by 50-80%- **PSS-10**: Perceived Stress Scale



### How reliable are the results?- More templates continuously being added...- **年龄**：覆盖不同年龄段（18-80岁）



LLM simulation is an exploratory research tool, suitable for:- **性别**：男、女、非二元性别



✅ Rapid prototyping  ### Simulation Settings Optimization- **职业**：不同行业和职位层级

✅ Hypothesis generation  

✅ Questionnaire pre-testing  - **教育**：从高中到研究生

❌ **Cannot** replace real human research

#### Temperature Parameter- **地域**：城市、农村、不同地区

### How to improve response quality?

- **文化背景**：不同种族、宗教、文化传统

1. Create detailed, realistic persona backgrounds

2. Use clear, specific questionsControls response randomness and creativity:

3. Choose appropriate temperature parameters

4. Use more powerful models (e.g., GPT-4)### 问卷设计技巧

5. Enable response validation and consistency checks

- **0.0 - 0.3**: High consistency, suitable for standardized responses

### What about costs?

- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)#### 好的问题特征

- **Local LM Studio**: Completely free (requires GPU)

- **DeepSeek API**: ~$0.0001/1k tokens (extremely low cost)- **0.8 - 1.0**: More diverse, suitable for exploratory research and creative testing

- **OpenAI GPT-3.5**: ~$0.002/1k tokens

- **OpenAI GPT-4**: ~$0.03/1k tokens✅ **清晰具体**



### Is my data secure?#### Max Tokens```



- **Local mode**: Data never leaves your machine好：在过去两周内，您有多少天感到情绪低落或沮丧？

- **API mode**: Follows each provider's privacy policy

- **Recommendation**: Use local mode for sensitive data- **150-300**: Short answers (multiple choice, scale ratings)差：您最近心情怎么样？



---- **300-500**: Medium length (short answer questions)```



## 🤝 Contributing- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)



Contributions welcome! See [Contributing Guide](./docs/en/contributing/README.md) for details.✅ **避免复合问题**



### Development Setup#### Parallel Settings```



```bash好：您每周锻炼多少次？您每次锻炼多长时间？

# Install development dependencies

pip install -r requirements-dev.txt- **Small Scale** (<10 personas): Concurrency 2-3差：您多久锻炼一次，每次多长时间，什么强度？



# Run tests- **Medium Scale** (10-50 personas): Concurrency 5-10```

pytest

- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)

# Code formatting

black src/ tests/✅ **使用标准化量表**

isort src/ tests/

---```

# Type checking

mypy src/从不(0) - 偶尔(1) - 经常(2) - 总是(3)

```

## 🏗️ Architecture```

### Report Issues



Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).

For detailed architecture documentation, see **[Architecture Guide](./docs/en/architecture/README.md)**#### 使用内置模板

---



## 📄 License

### Project Structure系统内置多个验证过的标准化量表：

This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.



---

```- **PHQ-9**：抑郁症筛查量表

## 🙏 Acknowledgments

auto_sim_ai/- **GAD-7**：焦虑症筛查量表

- [Streamlit](https://streamlit.io/) - Python web framework

- [LM Studio](https://lmstudio.ai/) - Local LLM runtime├── app.py                      # Streamlit main application- **PSS-10**：压力感知量表

- [OpenAI](https://openai.com/) - API standards

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM service├── pages/                      # Multi-page application- 更多模板持续添加中...



---│   ├── 1_Setup.py             # Persona management page



## 📞 Contact│   ├── 2_Simulation.py        # Simulation execution page### 模拟设置优化



- **Maintainer**: Jason Li│   └── 3_Results.py           # Results analysis page

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)

- **Email**: Contact via GitHub Issues├── src/                        # Core modules#### 温度参数（Temperature）



---│   ├── llm_client.py          # LLM client (sync/async)



<div align="center">│   ├── persona.py             # Persona management控制响应的随机性和创造性：



**⭐ If this project helps you, please give it a star!**│   ├── simulation.py          # Simulation engine (single/parallel)



Made with ❤️ by Jason Li│   ├── storage.py             # Results storage- **0.0 - 0.3**：高度一致，适合需要标准化响应的场景



</div>│   ├── cache.py               # Response cache- **0.5 - 0.7**：平衡模式，推荐用于大多数调查（默认）


│   ├── checkpoint.py          # Checkpoint management- **0.8 - 1.0**：更多样化，适合探索性研究和创意测试

│   ├── scoring.py             # Auto-scoring

│   ├── ab_testing.py          # A/B testing#### 最大令牌数（Max Tokens）

│   ├── intervention_study.py  # Intervention studies (legacy)

│   ├── persona_generator.py   # Persona generator- **150-300**：简短答案（选择题、量表评分）

│   ├── survey_templates.py    # Survey template library- **300-500**：中等长度（简答题）

│   ├── survey_config.py       # Survey configuration- **500-1000**：详细回答（开放式问题、深度访谈）

│   ├── tools.py               # Tool registration system

│   ├── ui_components.py       # UI components#### 并行设置

│   ├── styles.py              # Design system

│   └── validators.py          # Input validation- **小规模**（<10人）：并发数 2-3

├── tests/                      # Test suite- **中等规模**（10-50人）：并发数 5-10

├── data/                       # Data directory- **大规模**（50+人）：并发数 10-15（注意API速率限制）

│   ├── personas/              # Persona data

│   ├── results/               # Simulation results---

│   ├── cache/                 # Cache data

│   ├── checkpoints/           # Checkpoints## 🏗️ 架构设计

│   └── survey_configs/        # Survey configurations

├── docs/                       # Documentation详细的架构文档请查看 **[Architecture Guide](./docs/architecture/README.md)**

├── requirements.txt            # Dependencies

└── pytest.ini                 # Test configuration### 项目结构

```

```

### Core Module Overviewauto_sim_ai/

├── app.py                      # Streamlit 主应用

#### LLM Client (`llm_client.py`)├── pages/                      # 多页面应用

│   ├── 1_Setup.py             # 人物管理页面

Supports both synchronous and asynchronous modes:│   ├── 2_Simulation.py        # 模拟运行页面

│   └── 3_Results.py           # 结果分析页面

- **LMStudioClient**: Sync client, suitable for simple scenarios├── src/                        # 核心模块

- **AsyncLLMClient**: Async client, supports high concurrency│   ├── llm_client.py          # LLM 客户端（同步/异步）

│   ├── persona.py             # 人物管理

Compatible with OpenAI API format, seamless switching between providers.│   ├── simulation.py          # 模拟引擎（单线程/并行）

│   ├── storage.py             # 结果存储

#### Simulation Engine (`simulation.py`)│   ├── cache.py               # 响应缓存

│   ├── checkpoint.py          # 断点管理

- **SimulationEngine**: Base engine, sequential execution│   ├── scoring.py             # 自动评分

- **ParallelSimulationEngine**: Parallel engine, supports async batch processing│   ├── ab_testing.py          # A/B测试

│   ├── intervention_study.py  # 干预研究（旧版）

Automatically handles error retry, progress tracking, result aggregation.│   ├── longitudinal_study.py  # 纵向研究（新版，推荐）

│   ├── persona_generator.py   # 人物生成器

#### Cache System (`cache.py`)│   ├── survey_templates.py    # 问卷模板库

│   ├── survey_config.py       # 问卷配置

Content-hash based smart caching:│   ├── tools.py               # 工具注册系统

│   ├── ui_components.py       # UI 组件

- Same persona + same question = directly return cached result│   ├── styles.py              # 设计系统

- Support cache export and import│   └── validators.py          # 输入验证

- Significantly reduce LLM API call costs├── tests/                      # 测试套件

├── data/                       # 数据目录

#### Scoring System (`scoring.py`)│   ├── personas/              # 人物数据

│   ├── results/               # 模拟结果

Automated scoring features:│   ├── cache/                 # 缓存数据

│   ├── checkpoints/           # 检查点

- Support for multiple standardized scales│   └── survey_configs/        # 问卷配置

- Configurable custom scoring rules├── docs/                       # 文档

- Auto-calculate total and subscale scores├── requirements.txt            # 依赖列表

└── pytest.ini                 # 测试配置

---```



## 🔬 Advanced Features### 核心模块说明



> 💡 **Tip**: For detailed API documentation and advanced features, see [API Guide](./docs/en/api/README.md)#### LLM 客户端 (`llm_client.py`)



### 1. A/B Testing支持同步和异步两种模式：



Compare intervention effects across versions:- **LMStudioClient**：同步客户端，适合简单场景

- **AsyncLLMClient**：异步客户端，支持高并发

```python

from src import ABTestManager, Condition兼容 OpenAI API 格式，可无缝切换不同提供商。



# Define test conditions#### 模拟引擎 (`simulation.py`)

condition_a = Condition(

    name="Version A",- **SimulationEngine**：基础引擎，顺序执行

    intervention_text="Meditating 10 minutes daily can reduce stress.",- **ParallelSimulationEngine**：并行引擎，支持异步批处理

    questions=["Would you try this method?"]

)自动处理错误重试、进度追踪、结果聚合。



condition_b = Condition(#### 缓存系统 (`cache.py`)

    name="Version B", 

    intervention_text="Research shows daily 10-minute meditation reduces stress levels by 30%.",基于内容哈希的智能缓存：

    questions=["Would you try this method?"]- 相同人物 + 相同问题 = 直接返回缓存结果

)- 支持缓存导出和导入

- 显著降低 LLM API 调用成本

# Run A/B test

ab_manager = ABTestManager()#### 评分系统 (`scoring.py`)

results = ab_manager.run_test([condition_a, condition_b], personas)

```自动化评分功能：

- 支持多种标准化量表

### 2. Longitudinal Studies (Multi-Wave Tracking)- 可配置自定义评分规则

- 自动计算总分和子量表分数

Implement realistic longitudinal tracking with conversation memory:

---

```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig## 🔬 高级功能



# Configure study waves> 💡 **提示**: 详细的API文档和高级功能请查看 [API Guide](./docs/api/README.md)

waves = [

    WaveConfig(### 1. A/B 测试

        wave_number=1,

        wave_name="Baseline",比较不同版本的干预效果：

        questions=["What is your current stress level? (1-10)"],

        days_from_baseline=0```python

    ),from src import ABTestManager, Condition

    WaveConfig(

        wave_number=2,# 定义测试条件

        wave_name="1 Month Follow-up",condition_a = Condition(

        questions=["What is your stress level now? (1-10)"],    name="版本A",

        days_from_baseline=30,    intervention_text="每天冥想10分钟可以降低压力。",

        intervention_text="Practice 10 minutes of meditation daily"    questions=["您会尝试这个方法吗？"]

    ))

]

condition_b = Condition(

# Run longitudinal study    name="版本B", 

config = LongitudinalStudyConfig(    intervention_text="研究表明，每天冥想10分钟可以降低30%的压力水平。",

    study_id="stress_study",    questions=["您会尝试这个方法吗？"]

    study_name="Stress Intervention Study",)

    waves=waves

)# 运行A/B测试

ab_manager = ABTestManager()

engine = LongitudinalStudyEngine(llm_client)results = ab_manager.run_test([condition_a, condition_b], personas)

results = engine.run_study(personas, config)```

```

### 2. 纵向研究（多波次追踪）

For detailed longitudinal study guide, see **[Longitudinal Study Guide](./docs/en/longitudinal/README.md)**

使用对话记忆实现真实的纵向追踪：

### 3. Batch Persona Generation

```python

Generate virtual samples based on real demographic distributions:from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig



```python# 配置研究波次

from src import PersonaGenerator, DistributionConfigwaves = [

    WaveConfig(

# Configure distribution        wave_number=1,

config = DistributionConfig(        wave_name="基线",

    age_distribution={        questions=["您目前的压力水平如何？(1-10)"],

        "18-30": 0.3,        days_from_baseline=0

        "31-50": 0.4,    ),

        "51-70": 0.3    WaveConfig(

    },        wave_number=2,

    gender_distribution={        wave_name="1个月后",

        "Male": 0.48,        questions=["您现在的压力水平如何？(1-10)"],

        "Female": 0.52        days_from_baseline=30,

    }        intervention_text="每天练习10分钟冥想"

)    )

]

# Generate 100 personas

generator = PersonaGenerator()# 运行纵向研究

personas = generator.generate_batch(config = LongitudinalStudyConfig(

    count=100,    study_id="stress_study",

    distribution_config=config,    study_name="压力干预研究",

    llm_client=client    waves=waves

))

```

engine = LongitudinalStudyEngine(llm_client)

### 4. Response Validationresults = engine.run_study(personas, config)

```

Automatically check response quality and consistency:

详细的纵向研究指南请查看 **[Longitudinal Study Guide](./docs/longitudinal/README.md)**

```python

from src import ResponseValidator, ConsistencyChecker### 3. 批量人物生成



validator = ResponseValidator()基于真实人口统计分布生成虚拟样本：

checker = ConsistencyChecker()

```python

# Validate response formatfrom src import PersonaGenerator, DistributionConfig

is_valid = validator.validate_response(response, question_type)

# 配置分布

# Check consistencyconfig = DistributionConfig(

metrics = checker.check_consistency(persona_responses)    age_distribution={

print(f"Consistency score: {metrics.consistency_score}")        "18-30": 0.3,

```        "31-50": 0.4,

        "51-70": 0.3

---    },

    gender_distribution={

## 📚 API Documentation        "男": 0.48,

        "女": 0.52

### PersonaManager    }

)

```python

from src import PersonaManager# 生成100个人物

generator = PersonaGenerator()

manager = PersonaManager()personas = generator.generate_batch(

    count=100,

# Add persona    distribution_config=config,

manager.add_persona(persona)    llm_client=client

)

# Get all personas```

personas = manager.get_all_personas()

### 5. 响应验证

# Filter by criteria

young_adults = manager.filter_personas(自动检查响应质量和一致性：

    age_range=(18, 30),

    gender="Female"```python

)from src import ResponseValidator, ConsistencyChecker



# Save/loadvalidator = ResponseValidator()

manager.save_to_file("personas.json")checker = ConsistencyChecker()

manager.load_from_file("personas.json")

```# 验证响应格式

is_valid = validator.validate_response(response, question_type)

### SimulationEngine

# 检查一致性

```pythonmetrics = checker.check_consistency(persona_responses)

from src import SimulationEngineprint(f"一致性得分: {metrics.consistency_score}")

```

engine = SimulationEngine(

    llm_client=client,---

    cache=cache,

    checkpoint_manager=checkpoint_mgr## 📚 API 文档

)

### PersonaManager

# Run survey

result = engine.run_survey(```python

    personas=personas,from src import PersonaManager

    questions=questions,

    temperature=0.7,manager = PersonaManager()

    max_tokens=300

)# 添加人物

manager.add_persona(persona)

# Run intervention

result = engine.run_intervention(# 获取所有人物

    personas=personas,personas = manager.get_all_personas()

    intervention_text="Health intervention text",

    questions=followup_questions# 按条件筛选

)young_adults = manager.filter_personas(

```    age_range=(18, 30),

    gender="女"

### ResultsStorage)



```python# 保存/加载

from src import ResultsStoragemanager.save_to_file("personas.json")

manager.load_from_file("personas.json")

storage = ResultsStorage()```



# Save result### SimulationEngine

storage.save_result(simulation_result)

```python

# Load resultsfrom src import SimulationEngine

results = storage.load_all_results()

engine = SimulationEngine(

# Export to CSV    llm_client=client,

storage.export_to_csv(result, "output.csv")    cache=cache,

    checkpoint_manager=checkpoint_mgr

# Export analysis script)

storage.export_analysis_script(result, "analysis.py", language="python")

```# 运行调查

result = engine.run_survey(

---    personas=personas,

    questions=questions,

## ❓ FAQ    temperature=0.7,

    max_tokens=300

### Q: How many LLM API calls are needed?)



A: Call count = Number of personas × Number of questions. For example:# 运行干预

result = engine.run_intervention(

- 10 personas × 9 questions = 90 calls    personas=personas,

- Caching can significantly reduce repeat calls    intervention_text="健康干预文本",

    questions=followup_questions

### Q: How long does simulation take?)

```

A: Depends on:

### ResultsStorage

- **Local model**: ~5-15 seconds/response

- **Online API**: ~1-3 seconds/response```python

- **Parallel execution**: Can reduce time by 50-80%from src import ResultsStorage



### Q: How reliable are the results?storage = ResultsStorage()



A: LLM simulation is an exploratory research tool, suitable for:# 保存结果

storage.save_result(simulation_result)

- ✅ Rapid prototyping

- ✅ Hypothesis generation# 加载结果

- ✅ Questionnaire pre-testingresults = storage.load_all_results()

- ❌ **Cannot** replace real human research

# 导出为CSV

### Q: How to improve response quality?storage.export_to_csv(result, "output.csv")



1. Create detailed, realistic persona backgrounds# 导出分析脚本

2. Use clear, specific questionsstorage.export_analysis_script(result, "analysis.py", language="python")

3. Choose appropriate temperature parameters```

4. Use more powerful models (e.g., GPT-4)

5. Enable response validation and consistency checks---



### Q: What about costs?## ❓ 常见问题



- **Local LM Studio**: Completely free (requires GPU)### Q: 需要多少 LLM API 调用？

- **DeepSeek API**: ~$0.0001/1k tokens, extremely low cost

- **OpenAI GPT-3.5**: ~$0.002/1k tokensA: 调用次数 = 人物数量 × 问题数量。例如：

- **OpenAI GPT-4**: ~$0.03/1k tokens- 10个人物 × 9个问题 = 90次调用

- 使用缓存可大幅减少重复调用

### Q: Is my data secure?

### Q: 模拟需要多长时间？

- Local mode: Data never leaves your machine

- API mode: Follows each provider's privacy policyA: 取决于：

- Recommendation: Use local mode for sensitive data- **本地模型**：约 5-15 秒/响应

- **在线API**：约 1-3 秒/响应

---- **并行执行**：可缩短 50-80% 时间



## 🤝 Contributing### Q: 结果的可靠性如何？



Contributions welcome! See [CONTRIBUTING.md](./docs/en/contributing/README.md) for details.A: LLM模拟是探索性研究工具，适合：

- ✅ 快速原型测试

### Development Setup- ✅ 假设生成

- ✅ 问卷预测试

```bash- ❌ **不能**替代真实人类研究

# Install development dependencies

pip install -r requirements-dev.txt### Q: 如何提高响应质量？



# Run tests1. 创建详细、真实的人物背景

pytest2. 使用清晰、具体的问题

3. 选择合适的温度参数

# Code formatting4. 使用更强大的模型（如 GPT-4）

black src/ tests/5. 启用响应验证和一致性检查

isort src/ tests/

### Q: 成本如何？

# Type checking

mypy src/- **本地LM Studio**：完全免费（需要GPU）

```- **DeepSeek API**：~0.001元/千token，极低成本

- **OpenAI GPT-3.5**：~0.015元/千token

### Report Issues- **OpenAI GPT-4**：~0.3元/千token



Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).### Q: 数据安全吗？



---- 本地模式：数据完全不出本地

- API模式：遵循各提供商的隐私政策

## 📄 License- 建议：敏感数据使用本地模式



This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.---



---## 🤝 贡献



## 🙏 Acknowledgments欢迎贡献！请查看 [CONTRIBUTING.md](CONTRIBUTING.md) 了解详情。



- [Streamlit](https://streamlit.io/) - Excellent Python web framework### 开发设置

- [LM Studio](https://lmstudio.ai/) - Local LLM runtime environment

- [OpenAI](https://openai.com/) - API standards```bash

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM service# 安装开发依赖

pip install -r requirements-dev.txt

---

# 运行测试

## 📞 Contactpytest



- **Maintainer**: Jason Li# 代码格式化

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)black src/ tests/

- **Email**: [Contact via GitHub Issues]isort src/ tests/



---# 类型检查

mypy src/

<div align="center">```



**⭐ If this project helps you, please give it a star!**### 报告问题



Made with ❤️ by Jason Li发现 Bug 或有功能建议？请[创建 Issue](https://github.com/jason-jj-li/auto_sim_ai/issues)。



</div>---


## 📄 许可证

本项目采用 MIT 许可证。详见 [LICENSE](LICENSE) 文件。

---

## 🙏 致谢

- [Streamlit](https://streamlit.io/) - 优秀的Python Web框架
- [LM Studio](https://lmstudio.ai/) - 本地LLM运行环境
- [OpenAI](https://openai.com/) - API标准
- [DeepSeek](https://www.deepseek.com/) - 高性价比LLM服务

---

## 📞 联系方式

- **维护者**: Jason Li
- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)
- **Email**: [通过GitHub Issues联系]

---

<div align="center">

**⭐ 如果这个项目对您有帮助，请给一个星标！**

Made with ❤️ by Jason Li

</div>
