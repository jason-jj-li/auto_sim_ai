# Auto Sim AI - LLM Survey Simulation System

<div align="center">

**English Version | [ä¸­æ–‡ç‰ˆ](README_zh.md)**

---

ğŸ”¬ **AI-Powered Survey and Intervention Simulation System**

Simulate real survey research and intervention effects using LLM-driven virtual personas.

ğŸš€ **[Launch Web Application](https://jason-jj-li-auto-sim-ai-app-gkcvcf.streamlit.app)** ğŸš€

ï¿½ **[View Complete English Documentation](./docs/en/README.md)**

[Quick Start](./docs/en/quickstart/README.md) â€¢
[Features](#-features) â€¢
[API Reference](./docs/en/api/README.md) â€¢
[Contributing](./docs/en/contributing/README.md)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Quick Start](#quick-start)
- [User Guide](#user-guide)
- [Architecture](#architecture)
- [Advanced Features](#advanced-features)
- [API Reference](#api-reference)
- [ğŸ“š Complete Documentation](#complete-documentation)
  - [Quick Start Guide](./docs/en/quickstart/README.md)
  - [API Guide](./docs/en/api/README.md)
  - [Architecture Guide](./docs/en/architecture/README.md)
  - [Longitudinal Study Guide](./docs/en/longitudinal/README.md)
  - [Contributing Guide](./docs/en/contributing/README.md)
- [FAQ](#faq)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)
- [Contact](#contact)

---

<a id="overview"></a>
## ğŸ¯ Overview

**LLM Simulation Survey System** is an innovative research toolkit that leverages Large Language Models (LLMs) to generate virtual personas that emulate human responses to surveys and interventions.

### Use Cases

- ğŸ¥ **Health Intervention Research** â€“ Test the impact of health messaging on different populations
- ğŸ“Š **Market Research** â€“ Rapidly evaluate user feedback for products or services
- ğŸ“ **Educational Research** â€“ Assess teaching effectiveness across learner profiles
- ğŸ’¡ **Policy Analysis** â€“ Anticipate policy impacts on diverse communities
- ğŸ§ª **A/B Testing** â€“ Compare multiple approaches before real-world rollout
- ğŸ“ˆ **Prototype Validation** â€“ Iterate research designs before launching costly studies

### Core Advantages

âœ… **Fast Iteration** â€“ Complete hundreds of simulated responses in minutes  
âœ… **Cost-Effective** â€“ Eliminate participant recruitment overhead  
âœ… **Reproducible** â€“ Precisely control variables for repeatable experiments  
âœ… **Diverse** â€“ Create personas covering varied backgrounds, ages, and cultures  
âœ… **Deep Insights** â€“ Gather rich qualitative and quantitative outputs  
âœ… **Flexible Deployment** â€“ Run locally or through cloud APIs

---

<a id="features"></a>
## ğŸš€ Features

### Core Capabilities

#### 1ï¸âƒ£ Virtual Persona Management
- **Rich persona attributes**: age, gender, occupation, education, personality traits, values, and more
- **Batch generation** based on demographic distributions to mirror real populations
- **CSV import/export** for seamless integration with spreadsheets and databases
- **Demo templates** that provide ready-to-use persona examples

#### 2ï¸âƒ£ Multiple Simulation Modes
- **Survey mode** for standardized questionnaires (PHQ-9, GAD-7, PSS-10, etc.)
- **Intervention mode** to evaluate message impact across audience segments
- **A/B testing** to compare multiple content versions simultaneously
- **Longitudinal studies** for multi-wave tracking with memory
- **Sensitivity analysis** to measure how parameter changes influence outcomes

#### 3ï¸âƒ£ LLM Integration
- **Local deployment** via LM Studio (free, private, hardware dependent)
- **Commercial APIs**:
  - DeepSeek (cost-effective, Chinese optimized)
  - OpenAI (GPT-4, GPT-3.5)
  - Other OpenAI-compatible providers
- **Flexible switching** to change models and providers without code changes

#### 4ï¸âƒ£ Advanced Analysis
- **Automatic scoring** for validated scales (PHQ-9, GAD-7, PSS-10, etc.)
- **Statistical summaries** covering descriptive stats, correlations, and group comparisons
- **Consistency checks** to validate response logic and internal coherence
- **Interactive visualizations** including charts, distributions, and word clouds
- **Data exports** to CSV, JSON, or ready-to-run Python/R analysis scripts

#### 5ï¸âƒ£ Performance Optimization
- **Asynchronous execution** to process many personas in parallel
- **Smart caching** that avoids duplicate LLM calls and reduces cost
- **Checkpointing** for pause-and-resume workflows
- **Progress tracking** with real-time completion estimates

---

<a id="quick-start"></a>
## âš¡ Quick Start

### System Requirements

- **Python**: 3.8 or higher
- **Memory**: 8 GB RAM recommended
- **LLM Provider** (choose one):
  - LM Studio for local, offline inference
  - DeepSeek or OpenAI API key for hosted models

### 1. Clone the Project

```bash
git clone https://github.com/jason-jj-li/auto_sim_ai.git
cd auto_sim_ai
```

### 2. Install Dependencies

```bash
# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install runtime requirements
pip install -r requirements.txt
```

Or use the automated setup script:

```bash
chmod +x setup.sh
./setup.sh
```

### 3. Configure LLM Access

**Option A: LM Studio (local, recommended for development)**

1. Download [LM Studio](https://lmstudio.ai/).
2. Install a compatible model (`mistral-7b-instruct`, `llama-2-7b-chat`, or any 7B+ model).
3. Open the "Local Server" tab, select the model, click **Start Server**, and confirm the endpoint `http://localhost:1234`.

**Option B: Hosted APIs (recommended for production)**

```bash
cp env.example .env
# Edit .env and add your credentials
# DEEPSEEK_API_KEY=your_api_key
# OPENAI_API_KEY=your_api_key
```

### 4. Launch the Streamlit App

```bash
streamlit run app.py
```

The UI will open at `http://localhost:8501`.

### First Run Checklist

1. **Connect the LLM** â€“ Pick a provider, test the connection, and wait for the "System Ready" status.
2. **Create Personas** â€“ Use "Create Demo Personas", manually add entries, or import from CSV.
3. **Run a Simulation** â€“ Choose the mode (survey/intervention/A/B), select personas, set questions, and start the run.
4. **Review Results** â€“ Inspect individual responses, explore analytics, and export data for further analysis.

---

<a id="user-guide"></a>
## ğŸ“– User Guide

### Persona Design Best Practices

#### Create High-Quality Personas

```python
# Strong example: specific, detailed, and believable
{
    "name": "Li Ming",
    "age": 32,
    "gender": "Male",
    "occupation": "Software engineer at a startup",
    "education": "BSc Computer Science",
    "location": "Beijing",
    "background": "Works long hours in a fast-growing tech company. Recently stressed with poor sleep. Enjoys exercise but rarely has time.",
    "personality_traits": ["Perfectionist", "Responsible", "Slightly anxious"],
    "values": ["Career growth", "Work-life balance", "Family health"]
}

# Weak example: vague and generic
{
    "name": "John Doe",
    "age": 30,
    "gender": "Male",
    "occupation": "Engineer",
    "background": "Average person",
    "personality_traits": ["Normal"],
    "values": ["Happiness"]
}
```

#### Ensure Persona Diversity

- **Age**: represent multiple age brackets (18â€“80)
- **Gender**: include male, female, and non-binary identities
- **Occupation**: cover industries and seniority levels
- **Education**: range from high school to graduate degrees
- **Location**: mix of urban, rural, and regional contexts
- **Cultural background**: incorporate different ethnicities, religions, and traditions

### Questionnaire Design Tips

#### Characteristics of Strong Questions

âœ… **Clear and specific**
```
Good: In the past two weeks, how many days did you feel down or depressed?
Poor: How have you been feeling lately?
```

âœ… **Avoid double-barreled questions**
```
Good: How many times do you exercise each week? How long is each session?
Poor: How often do you exercise, how long, and at what intensity?
```

âœ… **Use standardized scales when possible**
```
Never (0) â€“ Rarely (1) â€“ Often (2) â€“ Always (3)
```

#### Leverage Built-In Templates

The app ships with validated questionnaires such as:

- **PHQ-9** for depression screening
- **GAD-7** for anxiety screening
- **PSS-10** for perceived stress
- Additional templates are continuously added

### Simulation Settings

#### Temperature

Controls output variability:

- **0.0 â€“ 0.3**: highly consistent, suited to standardized responses
- **0.5 â€“ 0.7**: balanced; recommended default for most surveys
- **0.8 â€“ 1.0**: diverse and creative; use for exploratory studies

#### Max Tokens

- **150â€“300**: short answers (Likert items, scale ratings)
- **300â€“500**: medium-length responses (short answer questions)
- **500â€“1000**: long-form narratives or in-depth interviews

#### Parallelism

- **Small runs (<10 personas)**: 2â€“3 concurrent workers
- **Medium runs (10â€“50 personas)**: 5â€“10 concurrent workers
- **Large runs (>50 personas)**: 10â€“15 workers (monitor API rate limits)

---

<a id="architecture"></a>
## ğŸ—ï¸ Architecture

Refer to the detailed **[Architecture Guide](./docs/en/architecture/README.md)** for diagrams and deep dives.

### Project Layout

```
auto_sim_ai/
â”œâ”€â”€ app.py                      # Streamlit entry point
â”œâ”€â”€ pages/                      # Multi-page UI
â”‚   â”œâ”€â”€ 1_Setup.py              # Persona management
â”‚   â”œâ”€â”€ 2_Simulation.py         # Simulation workflows
â”‚   â””â”€â”€ 3_Results.py            # Results visualization
â”œâ”€â”€ src/                        # Core engine modules
â”‚   â”œâ”€â”€ llm_client.py           # LLM clients (sync & async)
â”‚   â”œâ”€â”€ persona.py              # Persona model utilities
â”‚   â”œâ”€â”€ simulation.py           # Simulation orchestration
â”‚   â”œâ”€â”€ storage.py              # Persistent storage helpers
â”‚   â”œâ”€â”€ cache.py                # Response caching layer
â”‚   â”œâ”€â”€ checkpoint.py           # Pause/resume checkpoints
â”‚   â”œâ”€â”€ scoring.py              # Standardized scoring utilities
â”‚   â”œâ”€â”€ ab_testing.py           # A/B testing helpers
â”‚   â”œâ”€â”€ intervention_study.py   # Legacy intervention engine
â”‚   â”œâ”€â”€ longitudinal_study.py   # Recommended longitudinal engine
â”‚   â”œâ”€â”€ persona_generator.py    # Batch persona generation
â”‚   â”œâ”€â”€ survey_templates.py     # Questionnaire templates
â”‚   â”œâ”€â”€ survey_config.py        # Survey configuration schemas
â”‚   â”œâ”€â”€ tools.py                # Tool registry
â”‚   â”œâ”€â”€ ui_components.py        # Shared Streamlit widgets
â”‚   â”œâ”€â”€ styles.py               # Design system definitions
â”‚   â””â”€â”€ validators.py           # Input validation rules
â”œâ”€â”€ data/                       # Working data artifacts
â”‚   â”œâ”€â”€ personas/               # Persona definitions
â”‚   â”œâ”€â”€ results/                # Simulation outputs
â”‚   â”œâ”€â”€ cache/                  # Cached LLM responses
â”‚   â”œâ”€â”€ checkpoints/            # Checkpoint snapshots
â”‚   â””â”€â”€ survey_configs/         # Survey configuration bundles
â”œâ”€â”€ docs/                       # Multilingual documentation
â”œâ”€â”€ tests/                      # Automated test suite
â”œâ”€â”€ requirements.txt            # Runtime dependencies
â””â”€â”€ pytest.ini                  # Pytest configuration
```

### Core Modules

- **`llm_client.py`** â€“ Unified sync/async clients compatible with LM Studio and OpenAI-style APIs.
- **`simulation.py`** â€“ Sequential and parallel execution engines with retries, aggregation, and progress callbacks.
- **`cache.py`** â€“ Hash-based response caching to reuse identical prompts and cut latency and cost.
- **`scoring.py`** â€“ Automates calculation of standardized scales with support for custom scoring rules.

---

<a id="advanced-features"></a>
## ğŸ”¬ Advanced Features

> ğŸ’¡ **Tip**: Additional usage examples are available in the [API Guide](./docs/en/api/README.md).

### 1. A/B Testing

```python
from src import ABTestManager, Condition

condition_a = Condition(
    name="Version A",
    intervention_text="Meditating 10 minutes daily reduces stress.",
    questions=["Would you try this method?"]
)

condition_b = Condition(
    name="Version B",
    intervention_text="Studies show daily 10-minute meditation can cut stress by 30%.",
    questions=["Would you try this method?"]
)

manager = ABTestManager()
results = manager.run_test([condition_a, condition_b], personas)
```

### 2. Longitudinal Studies

```python
from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig

waves = [
    WaveConfig(
        wave_number=1,
        wave_name="Baseline",
        questions=["Rate your current stress (1-10)."],
        days_from_baseline=0
    ),
    WaveConfig(
        wave_number=2,
        wave_name="1 Month",
        questions=["Rate your current stress (1-10)."],
        days_from_baseline=30,
        intervention_text="Practice 10 minutes of meditation daily."
    )
]

config = LongitudinalStudyConfig(
    study_id="stress",
    study_name="Stress Intervention",
    waves=waves
)

engine = LongitudinalStudyEngine(llm_client)
results = engine.run_study(personas, config)
```

### 3. Batch Persona Generation

```python
from src import PersonaGenerator, DistributionConfig

config = DistributionConfig(
    age_distribution={"18-30": 0.3, "31-50": 0.4, "51-70": 0.3},
    gender_distribution={"Male": 0.48, "Female": 0.52}
)

generator = PersonaGenerator()
personas = generator.generate_batch(
    count=100,
    distribution_config=config,
    llm_client=client
)
```

### 4. Response Validation

```python
from src import ResponseValidator, ConsistencyChecker

validator = ResponseValidator()
checker = ConsistencyChecker()

is_valid = validator.validate_response(response, question_type)
metrics = checker.check_consistency(persona_responses)
print(metrics.consistency_score)
```

---

<a id="api-reference"></a>
## ğŸ§° API Reference

### PersonaManager

```python
from src import PersonaManager

manager = PersonaManager()

# Add a persona
manager.add_persona(persona)

# Retrieve all personas
personas = manager.get_all_personas()
# Filter by attributes
young_adults = manager.filter_personas(
    age_range=(18, 30),
manager.load_from_file("personas.json")

### SimulationEngine

```python
from src import SimulationEngine

gine = SimulationEngine(
    llm_client=client,
    cache=cache,
# Run a survey
survey_result = engine.run_survey(
    personas=personas,
    questions=questions,
    temperature=0.7,
    max_tokens=300
)

# Run an intervention
intervention_result = engine.run_intervention(
    personas=personas,
    intervention_text="Health intervention content",
    questions=followup_questions
)
```
from src import ResultsStorage

storage = ResultsStorage()

results = storage.load_all_results()

# Export to CSV
storage.export_to_csv(simulation_result, "output.csv")

# Generate analysis scripts
storage.export_analysis_script(simulation_result, "analysis.py", language="python")
```

---

<a id="complete-documentation"></a>
## ğŸ“š Complete Documentation

- [Quick Start Guide](./docs/en/quickstart/README.md)
- [API Guide](./docs/en/api/README.md)
- [Architecture Guide](./docs/en/architecture/README.md)
- [Longitudinal Study Guide](./docs/en/longitudinal/README.md)
- [Contributing Guide](./docs/en/contributing/README.md)


<a id="faq"></a>
## â“ FAQ

### How many LLM API calls will I need?


### How long do simulations take?

### How reliable are the results?
LLM simulations are exploratory. They excel at prototyping, hypothesis generation, and pretesting questionnaires but do **not** replace human studies.

### How can I improve response quality?
3. Tune temperature for the desired variability.
5. Enable validation and consistency checks.

### What does it cost?

- **LM Studio**: free (requires capable local hardware)
- **DeepSeek API**: â‰ˆ $0.0001 per 1K tokens
- **OpenAI GPT-3.5**: â‰ˆ $0.002 per 1K tokens
- **OpenAI GPT-4**: â‰ˆ $0.03 per 1K tokens

### Is data secure?

- **Local mode** keeps data on-premises.
- **API mode** follows each provider's privacy policy.
- Prefer local mode for sensitive datasets.

---

<a id="contributing"></a>
## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for the complete workflow.

### Development Setup

```bash
pip install -r requirements-dev.txt
pytest
black src/ tests/
isort src/ tests/
mypy src/
```

### Report Issues

Encountered a bug or have an idea? [Open an issue](https://github.com/jason-jj-li/auto_sim_ai/issues) on GitHub.

---

<a id="license"></a>
## ğŸ“„ License

This project is released under the MIT License. See [LICENSE](LICENSE) for details.

---

<a id="acknowledgments"></a>
## ğŸ™ Acknowledgments

- [Streamlit](https://streamlit.io/) â€“ Python web framework
- [LM Studio](https://lmstudio.ai/) â€“ Local LLM runtime
- [OpenAI](https://openai.com/) â€“ API compatibility standard
- [DeepSeek](https://www.deepseek.com/) â€“ Cost-effective LLM provider

---

<a id="contact"></a>
## ğŸ“ Contact

- **Maintainer**: Jason Li
- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)
- **Email**: Please reach out via GitHub issues

---

<div align="center">

**â­ If this project helps you, please give it a star!**

Made with â¤ï¸ by Jason Li

</div>
