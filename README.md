# Auto Sim AI - LLM Survey Simulation System

**English Version | [ä¸­æ–‡ç‰ˆ](README_zh.md)**

ğŸ”¬ **AI-Powered Survey and Intervention Simulation System**

Simulate real survey research and intervention effects using LLM-driven virtual personas.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ğŸ“– **[Complete English Documentation](docs/en/README.md)**

- [Quick Start](#quick-start)
- [Features](#features)
- [API Reference](docs/en/api/README.md)
- [Contributing](docs/en/contributing/README.md)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Quick Start](#quick-start)
- [User Guide](#user-guide)
- [ğŸ“š Complete Documentation](#-complete-documentation)
- [FAQ](#faq)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)
- [Contact](#contact)

---

## ğŸ¯ Overview

**LLM Simulation Survey System** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions.

### Use Cases

- ğŸ¥ **Health Intervention Research** â€“ Test the impact of health messaging on different populations
- ğŸ“Š **Market Research** â€“ Rapidly evaluate user feedback for products or services
- ğŸ“ **Educational Research** â€“ Assess teaching method effectiveness across learner types
- ğŸ’¡ **Policy Analysis** â€“ Predict potential policy impacts on diverse populations
- ğŸ§ª **A/B Testing** â€“ Compare effectiveness across different approaches
- ğŸ“ˆ **Prototype Validation** â€“ Iterate designs before running real-world research

### Core Advantages

- âœ… **Fast Iteration** â€“ Complete simulations with hundreds of personas in minutes
- âœ… **Cost-Effective** â€“ No need to recruit real participants
- âœ… **Reproducible** â€“ Precisely control variables for repeatable experiments
- âœ… **Diverse** â€“ Create personas spanning varied backgrounds, ages, and cultures
- âœ… **Deep Insights** â€“ Gather rich qualitative and quantitative outputs
- âœ… **Flexible Deployment** â€“ Run locally or via cloud APIs

---

## ğŸš€ Features

### 1ï¸âƒ£ Virtual Persona Management

- Rich persona attributes: age, gender, occupation, education, personality traits, values, more
- Batch creation based on demographic distributions
- CSV import/export for Excel or database pipelines
- Ready-to-use demo persona templates

### 2ï¸âƒ£ Multiple Simulation Modes

- Survey mode for standardized questionnaires (PHQ-9, GAD-7, etc.)
- Intervention mode to evaluate message impact on different personas
- A/B testing to compare multiple content versions
- Longitudinal studies for multi-wave tracking
- Sensitivity analysis to measure parameter influence

### 3ï¸âƒ£ LLM Integration

- Local deployment through LM Studio (free and private)
- Commercial APIs including DeepSeek, OpenAI (GPT-4, GPT-3.5), and compatible providers
- Seamless provider switching via unified client interfaces

### 4ï¸âƒ£ Advanced Analysis

- Automatic scoring for standardized scales
- Descriptive stats, correlation analysis, and group comparisons
- Consistency checks for validating answer logic
- Interactive visualizations (charts, word clouds, distributions)
- Exports to CSV, JSON, and ready-to-run Python/R scripts

### 5ï¸âƒ£ Performance Optimization

- Asynchronous processing for high-throughput simulations
- Smart caching to avoid repeated LLM calls
- Checkpointing for pause-and-resume workflows
- Real-time progress tracking with completion estimates

---

## âš¡ Quick Start

### System Requirements

- Python 3.8 or higher
- 8 GB RAM recommended
- One LLM provider: LM Studio or an API key for DeepSeek/OpenAI

### Installation Steps

```bash
git clone https://github.com/jason-jj-li/auto_sim_ai.git
cd auto_sim_ai
```

#### Option A: Automated Setup (Recommended)

```bash
chmod +x setup.sh
./setup.sh
```

#### Option B: Manual Setup

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Configure LLM Access

#### Local LM Studio

1. Download [LM Studio](https://lmstudio.ai/).
2. Install a model (`mistral-7b-instruct`, `llama-2-7b-chat`, or any 7B+ model).
3. Start the local server ("Local Server" tab) and confirm `http://localhost:1234`.

#### Online API Provider

```bash
cp env.example .env
# Edit .env and add credentials
# DEEPSEEK_API_KEY=your_key
# or
# OPENAI_API_KEY=your_key
```

### Launch Streamlit App

```bash
streamlit run app.py
```

The UI loads at `http://localhost:8501`.

### First Run Checklist

1. **Connect LLM** â€“ Choose provider, test connection, wait for "System Ready".
2. **Create Personas** â€“ Use "Create Demo Personas", manual entry, or CSV upload.
3. **Run Simulation** â€“ Pick mode (survey/intervention/A/B), select personas, configure questions, run.
4. **Review Results** â€“ Inspect responses, view analytics, export for further study.

---

## ğŸ“– User Guide

### Persona Design Best Practices

```python
# Strong persona: specific, credible, detailed
{
    "name": "Li Ming",
    "age": 32,
    "gender": "Male",
    "occupation": "Software engineer at a startup",
    "education": "BSc Computer Science",
    "location": "Beijing",
    "background": "Works long hours in a fast-growing tech company. Recently stressed with poor sleep. Enjoys exercise but rarely has time.",
    "personality_traits": ["Perfectionist", "Responsible", "Slightly anxious"],
    "values": ["Career growth", "Work-life balance", "Family health"]
}
```

- Cover demographic diversity (age, gender identity, occupation, education, region, culture).
- Provide concrete backgrounds and motivations to guide consistent LLM responses.

### Questionnaire Design Tips

- Ask clear, single-focus questions.
- Avoid multi-part questions; split them instead.
- Use validated scales when possible (PHQ-9, GAD-7, PSS-10).
- Provide answer formats (Likert scales, numeric ranges) in prompts.

### Simulation Settings

- **Temperature**: 0.0â€“0.3 for consistent responses, 0.5â€“0.7 balanced (default), 0.8â€“1.0 exploratory.
- **Max Tokens**: 150â€“300 for short answers, 300â€“500 medium, 500â€“1000 detailed narratives.
- **Parallelism**: 2â€“3 personas (small runs), 5â€“10 (medium), 10â€“15 (large â€“ watch rate limits).

---

## ğŸ—ï¸ Architecture Design

Project structure highlights:

```
auto_sim_ai/
â”œâ”€â”€ app.py                  # Streamlit entrypoint
â”œâ”€â”€ pages/                  # UI subpages
â”‚   â”œâ”€â”€ 1_Setup.py          # Persona management
â”‚   â”œâ”€â”€ 2_Simulation.py     # Simulation workflows
â”‚   â””â”€â”€ 3_Results.py        # Results visualization
â”œâ”€â”€ src/                    # Core engine modules
â”‚   â”œâ”€â”€ llm_client.py       # Sync/async LLM clients
â”‚   â”œâ”€â”€ persona.py          # Persona model & utilities
â”‚   â”œâ”€â”€ simulation.py       # Simulation orchestration
â”‚   â”œâ”€â”€ storage.py          # Persistent results management
â”‚   â”œâ”€â”€ cache.py            # Response caching layer
â”‚   â”œâ”€â”€ checkpoint.py       # Pause/resume checkpoints
â”‚   â”œâ”€â”€ scoring.py          # Survey scoring utilities
â”‚   â”œâ”€â”€ ab_testing.py       # A/B testing helpers
â”‚   â”œâ”€â”€ longitudinal_study.py
â”‚   â”œâ”€â”€ persona_generator.py
â”‚   â”œâ”€â”€ survey_templates.py
â”‚   â”œâ”€â”€ survey_config.py
â”‚   â”œâ”€â”€ tools.py            # Tool registry
â”‚   â”œâ”€â”€ ui_components.py    # Shared Streamlit widgets
â”‚   â”œâ”€â”€ styles.py           # Design system
â”‚   â””â”€â”€ validators.py       # Input validation
â”œâ”€â”€ tests/                  # Automated tests
â”œâ”€â”€ data/                   # Personas, results, caches
â”œâ”€â”€ docs/                   # Full documentation
â”œâ”€â”€ requirements.txt        # Runtime dependencies
â””â”€â”€ pytest.ini              # Test configuration
```

### Core Modules

- **llm_client.py** â€“ Unified interfaces for LM Studio and API providers (sync + async).
- **simulation.py** â€“ Sequential and parallel engines with retries and aggregation.
- **cache.py** â€“ Hash-based cache to reuse identical prompts and save cost.
- **scoring.py** â€“ Automatic scoring for standardized scales with custom rules.

---

## ğŸ”¬ Advanced Features

### A/B Testing

```python
from src import ABTestManager, Condition

condition_a = Condition(
    name="Version A",
    intervention_text="Meditating 10 minutes daily reduces stress.",
    questions=["Would you try this method?"]
)

condition_b = Condition(
    name="Version B",
    intervention_text="Studies show daily 10-minute meditation can cut stress by 30%.",
    questions=["Would you try this method?"]
)

manager = ABTestManager()
results = manager.run_test([condition_a, condition_b], personas)
```

### Longitudinal Studies

```python
from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig

waves = [
    WaveConfig(
        wave_number=1,
        wave_name="Baseline",
        questions=["Rate your current stress (1-10)."],
        days_from_baseline=0
    ),
    WaveConfig(
        wave_number=2,
        wave_name="1 Month",
        questions=["Rate your current stress (1-10)."],
        days_from_baseline=30,
        intervention_text="Practice 10 minutes of meditation daily."
    )
]

config = LongitudinalStudyConfig(
    study_id="stress",
    study_name="Stress Intervention",
    waves=waves
)

engine = LongitudinalStudyEngine(llm_client)
results = engine.run_study(personas, config)
```

### Batch Persona Generation

```python
from src import PersonaGenerator, DistributionConfig

config = DistributionConfig(
    age_distribution={"18-30": 0.3, "31-50": 0.4, "51-70": 0.3},
    gender_distribution={"Male": 0.48, "Female": 0.52}
)

generator = PersonaGenerator()
personas = generator.generate_batch(
    count=100,
    distribution_config=config,
    llm_client=client
)
```

### Response Validation

```python
from src import ResponseValidator, ConsistencyChecker

validator = ResponseValidator()
checker = ConsistencyChecker()

is_valid = validator.validate_response(response, question_type)
metrics = checker.check_consistency(persona_responses)
print(metrics.consistency_score)
```

---

## ğŸ“š Complete Documentation

- [Quick Start Guide](docs/quickstart/README.md)
- [API Documentation](docs/api/README.md)
- [Architecture Guide](docs/architecture/README.md)
- [Longitudinal Studies Guide](docs/longitudinal/README.md)
- [Contributing Guide](docs/contributing/README.md)

---

## â“ FAQ

### How many LLM API calls will I need?

Call count = number of personas Ã— number of questions. Example: 10 personas Ã— 9 questions = 90 calls. Caching greatly reduces duplicates.

### How long do simulations take?

- Local model: roughly 5â€“15 seconds per response
- API model: roughly 1â€“3 seconds per response
- Parallel execution can reduce runtime by 50â€“80%

### How reliable are the results?

LLM simulations are exploratoryâ€”ideal for prototyping, hypothesis generation, and pretesting questionnaires. They do **not** replace human studies.

### How can I improve response quality?

1. Provide rich persona backgrounds.
2. Ask precise, scoped questions.
3. Tune temperature for the desired variability.
4. Use higher-quality models when possible.
5. Enable validation and consistency checks.

### What does it cost?

- LM Studio: free (requires capable hardware)
- DeepSeek API: â‰ˆ $0.0001 per 1k tokens
- OpenAI GPT-3.5: â‰ˆ $0.002 per 1k tokens
- OpenAI GPT-4: â‰ˆ $0.03 per 1k tokens

### Is data secure?

- Local mode keeps data on-premises.
- API mode follows provider privacy policies.
- Prefer local mode for sensitive datasets.

---

## ğŸ¤ Contributing

We welcome contributions! Read [CONTRIBUTING.md](CONTRIBUTING.md) for workflow details.

```bash
pip install -r requirements-dev.txt
pytest
black src/ tests/
isort src/ tests/
mypy src/
```

Report bugs or ideas via [GitHub issues](https://github.com/jason-jj-li/auto_sim_ai/issues).

---

## ğŸ“„ License

Released under the MIT License. Refer to [LICENSE](LICENSE).

---

## ğŸ™ Acknowledgments

- [Streamlit](https://streamlit.io/) â€“ Python web framework
- [LM Studio](https://lmstudio.ai/) â€“ Local LLM runtime
- [OpenAI](https://openai.com/) â€“ API compatibility standard
- [DeepSeek](https://www.deepseek.com/) â€“ Cost-effective LLM provider

---

## ğŸ“ Contact

- Maintainer: Jason Li
- GitHub: [@jason-jj-li](https://github.com/jason-jj-li)
- Email: please reach out via GitHub issues

---

**â­ If this project helps you, consider starring the repository.**
