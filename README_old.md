# Auto Sim AI - LLM Survey Simulation System



<div align="center">



**English Version | [ä¸­æ–‡ç‰ˆ](./README_zh.md)**<div align="center">



---<div align="center">



ğŸ”¬ **AI-Powered Survey and Intervention Simulation System****English Version | [ä¸­æ–‡ç‰ˆ](./README_zh.md)**



Simulate real survey research and intervention effects using LLM-driven virtual personas



[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)---

[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)**English Version | [ä¸­æ–‡ç‰ˆ](./README_zh.md)**<div align="center">



ğŸ“– **[View Complete English Documentation](./docs/en/README.md)**ğŸ”¬ **AI-Powered Survey and Intervention Simulation System**



[Quick Start](./docs/en/quickstart/README.md) â€¢

[Features](#features) â€¢

[API Reference](./docs/en/api/README.md) â€¢Simulate real survey research and intervention effects using LLM-driven virtual personas

[Contributing](./docs/en/contributing/README.md)

---

</div>

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

---

[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)

## ğŸ“‹ Table of Contents

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

- [Overview](#overview)

- [Features](#features)ğŸ”¬ **AI-Powered Survey and Intervention Simulation System****English Version | [ä¸­æ–‡ç‰ˆ](./README_zh.md)**<div align="center"><div align="right">

- [Quick Start](#quick-start)

- [User Guide](#user-guide)ğŸ“– **[View Complete English Documentation](./docs/en/README.md)**

- [ğŸ“š Complete Documentation](./docs/README.md)

  - [Quick Start Guide](./docs/quickstart/README.md)

  - [API Documentation](./docs/api/README.md)

  - [Architecture Design](./docs/architecture/README.md)[Quick Start](#quick-start) â€¢

  - [Longitudinal Studies Guide](./docs/longitudinal/README.md)

  - [Contributing Guide](./docs/contributing/README.md)[Features](#features) â€¢Simulate real survey research and intervention effects using LLM-driven virtual personas

- [FAQ](#faq)

- [License](#license)[API Reference](./docs/en/api/README.md) â€¢



---[Contributing](./docs/en/contributing/README.md)



## ğŸ¯ Overview



**LLM Simulation Survey System** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions.</div>ğŸ“– **[View Complete English Documentation](./docs/en/README.md)**---



### Use Cases



- ğŸ¥ **Health Intervention Research** - Test the impact of health messaging on different populations---

- ğŸ“Š **Market Research** - Rapidly evaluate user feedback for products or services

- ğŸ“ **Educational Research** - Assess teaching method effectiveness across different learner types

- ğŸ’¡ **Policy Analysis** - Predict potential policy impacts on diverse populations

- ğŸ§ª **A/B Testing** - Compare effectiveness across different approaches## ğŸ“‹ Table of Contents[Quick Start](./docs/en/quickstart/README.md) â€¢

- ğŸ“ˆ **Prototype Validation** - Rapidly iterate designs before real-world research



### Core Advantages

- [Overview](#overview)[Features](#features) â€¢

âœ… **Fast Iteration** - Complete survey simulations with hundreds of participants in minutes  

âœ… **Cost-Effective** - No need to recruit real participants  - [Features](#features)

âœ… **Reproducible** - Precise variable control ensures experimental repeatability  

âœ… **Diverse** - Easily create virtual personas with varied backgrounds, ages, and cultures  - [Quick Start](#quick-start)[API Reference](./docs/en/api/README.md) â€¢ğŸ”¬ **AI-Powered Survey and Intervention Simulation System****English Version | [ä¸­æ–‡ç‰ˆ](./README_zh.md)**[![English](https://img.shields.io/badge/docs-English-blue?style=flat-square)](./docs/en/README.md)

âœ… **Deep Insights** - Obtain detailed qualitative and quantitative data  

âœ… **Flexible Deployment** - Support for local deployment and cloud API- [User Guide](#user-guide)



---- [ğŸ“š Complete Documentation](./docs/README.md)[Contributing](./docs/en/contributing/README.md)



## ğŸš€ Features  - [Quick Start Guide](./docs/quickstart/README.md)



### Core Capabilities  - [API Documentation](./docs/api/README.md)



#### 1ï¸âƒ£ Virtual Persona Management  - [Architecture Design](./docs/architecture/README.md)



- **Rich Persona Attributes**: Age, gender, occupation, education, personality traits, values, etc.  - [Longitudinal Studies Guide](./docs/longitudinal/README.md)</div>

- **Batch Creation**: Auto-generate virtual samples matching real population distributions using demographic statistics

- **CSV Import/Export**: Bulk import personas from Excel or databases  - [Contributing Guide](./docs/contributing/README.md)

- **Demo Templates**: Built-in typical persona templates, ready to use

- [FAQ](#faq)Simulate real survey research and intervention effects using LLM-driven virtual personas[![ä¸­æ–‡æ–‡æ¡£](https://img.shields.io/badge/æ–‡æ¡£-ä¸­æ–‡-red?style=flat-square)](./docs/zh/README.md)

#### 2ï¸âƒ£ Multiple Simulation Modes

- [License](#license)

- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)

- **Intervention Mode**: Test health messaging, advertising copy, etc. on different populations---

- **A/B Testing**: Test multiple versions simultaneously and compare effectiveness

- **Longitudinal Studies**: Simulate multi-wave surveys to track changes over time---

- **Sensitivity Analysis**: Systematically test how parameter changes affect results



#### 3ï¸âƒ£ LLM Integration

## ğŸ¯ Overview

- **Local Deployment**: LM Studio (free, completely private)

- **Commercial APIs**:## ğŸ“‹ Table of Contents

  - DeepSeek (cost-effective, Chinese-optimized)

  - OpenAI (GPT-4, GPT-3.5)**LLM Simulation Survey System** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions.

  - Other OpenAI-compatible services

- **Flexible Switching**: Change models or providers anytime[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)---



#### 4ï¸âƒ£ Advanced Analysis### Use Cases



- **Auto-Scoring**: Built-in automatic scoring system for standardized scales- [Overview](#overview)

- **Statistical Analysis**: Descriptive statistics, correlation analysis, group comparisons

- **Consistency Checks**: Validate internal consistency and logic of responses- ğŸ¥ **Health Intervention Research** - Test the impact of health messaging on different populations

- **Visualization**: Interactive charts, word clouds, distribution plots

- **Export Functions**: CSV, JSON, Python/R analysis scripts- ğŸ“Š **Market Research** - Rapidly evaluate user feedback for products or services- [Features](#features)[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)



#### 5ï¸âƒ£ Performance Optimization- ğŸ“ **Educational Research** - Assess teaching method effectiveness across different learner types



- **Parallel Execution**: Async processing for multiple persona responses- ğŸ’¡ **Policy Analysis** - Predict potential policy impacts on diverse populations- [Quick Start](#quick-start)

- **Smart Caching**: Avoid redundant LLM calls, save time and cost

- **Checkpoint Resume**: Support pausing and resuming large-scale simulations- ğŸ§ª **A/B Testing** - Compare effectiveness across different approaches

- **Progress Tracking**: Real-time progress display and estimated completion time

- ğŸ“ˆ **Prototype Validation** - Rapidly iterate designs before real-world research- [User Guide](#user-guide)[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)</div>

---



## âš¡ Quick Start

### Core Advantages- [ğŸ“š Complete Documentation](./docs/README.md)

### System Requirements



- **Python**: 3.8 or higher

- **Memory**: 8GB+ recommendedâœ… **Fast Iteration** - Complete survey simulations with hundreds of participants in minutes    - [Quick Start Guide](./docs/quickstart/README.md)

- **LLM Provider** (choose one):

  - LM Studio (local, free)âœ… **Cost-Effective** - No need to recruit real participants  

  - DeepSeek/OpenAI API key

âœ… **Reproducible** - Precise variable control ensures experimental repeatability    - [API Documentation](./docs/api/README.md)

### Installation Steps

âœ… **Diverse** - Easily create virtual personas with varied backgrounds, ages, and cultures  

#### Step 1: Clone Repository

âœ… **Deep Insights** - Obtain detailed qualitative and quantitative data    - [Architecture Design](./docs/architecture/README.md)ğŸ“– **[Complete English Documentation](./docs/en/README.md)**ğŸ”¬ **AI-Powered Survey and Intervention Simulation System**

```bash

git clone https://github.com/jason-jj-li/auto_sim_ai.gitâœ… **Flexible Deployment** - Support for local deployment and cloud API

cd auto_sim_ai

```  - [Longitudinal Studies Guide](./docs/longitudinal/README.md)



#### Step 2: Install Dependencies---



```bash  - [Contributing Guide](./docs/contributing/README.md)

# Create virtual environment (recommended)

python -m venv venv## ğŸš€ Features

source venv/bin/activate  # Windows: venv\Scripts\activate

- [FAQ](#faq)

# Install dependencies

pip install -r requirements.txt### Core Capabilities

```

- [License](#license)[Quick Start](#-quick-start) â€¢<div align="center">

Or use the setup script:

#### 1ï¸âƒ£ Virtual Persona Management

```bash

chmod +x setup.sh

./setup.sh

```- **Rich Persona Attributes**: Age, gender, occupation, education, personality traits, values, etc.



#### Step 3: Configure LLM- **Batch Creation**: Auto-generate virtual samples matching real population distributions using demographic statistics---[Features](#-features) â€¢



**Option A: Local LM Studio (Recommended for Learning and Development)**- **CSV Import/Export**: Bulk import personas from Excel or databases



1. Download [LM Studio](https://lmstudio.ai/)- **Demo Templates**: Built-in typical persona templates, ready to use

2. Download a model in LM Studio:

   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`

   - Minimum: 7B parameter model

3. Start local server:#### 2ï¸âƒ£ Multiple Simulation Modes## ğŸ¯ Overview[API Reference](./docs/en/api/README.md) â€¢Simulate real survey research and intervention effects using LLM-driven virtual personas

   - Click "Local Server" tab

   - Select model

   - Click "Start Server"

   - Confirm address is `http://localhost:1234`- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)



**Option B: Online API (Recommended for Production)**- **Intervention Mode**: Test health messaging, advertising copy, etc. on different populations



```bash- **A/B Testing**: Test multiple versions simultaneously and compare effectiveness**LLM Simulation Survey System** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions.[Contributing](./docs/en/contributing/README.md)

# Copy environment template

cp env.example .env- **Longitudinal Studies**: Simulate multi-wave surveys to track changes over time



# Edit .env file and add API key- **Sensitivity Analysis**: Systematically test how parameter changes affect results

# DEEPSEEK_API_KEY=your_api_key_here

# or

# OPENAI_API_KEY=your_api_key_here

```#### 3ï¸âƒ£ LLM Integration### Use Cases**Language / è¯­è¨€:** [English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)



#### Step 4: Launch Application



```bash- **Local Deployment**: LM Studio (free, completely private)

streamlit run app.py

```- **Commercial APIs**:



The app will automatically open in your browser: `http://localhost:8501`  - DeepSeek (cost-effective, Chinese-optimized)- ğŸ¥ **Health Intervention Research** - Test the impact of health messaging on different populations</div>



### First-Time User Guide  - OpenAI (GPT-4, GPT-3.5)



1. **Connect LLM** (Home Page)  - Other OpenAI-compatible services- ğŸ“Š **Market Research** - Rapidly evaluate user feedback for products or services

   - Select LLM provider

   - Test connection- **Flexible Switching**: Change models or providers anytime

   - Wait for "System Ready" message

- ğŸ“ **Educational Research** - Assess teaching method effectiveness across different learner types[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

2. **Create Virtual Personas** (Setup Page)

   - Click "Create Demo Personas" for quick start#### 4ï¸âƒ£ Advanced Analysis

   - Or manually create custom personas

   - Or upload CSV for bulk import- ğŸ’¡ **Policy Analysis** - Predict potential policy impacts on diverse populations



3. **Run Simulation** (Simulation Page)- **Auto-Scoring**: Built-in automatic scoring system for standardized scales

   - Choose simulation type (Survey/Intervention)

   - Select personas to participate- **Statistical Analysis**: Descriptive statistics, correlation analysis, group comparisons- ğŸ§ª **A/B Testing** - Compare effectiveness across different approaches---

   - Choose questionnaire template or enter custom questions

   - Click "Run Simulation"- **Consistency Checks**: Validate internal consistency and logic of responses



4. **View Results** (Results Page)- **Visualization**: Interactive charts, word clouds, distribution plots- ğŸ“ˆ **Prototype Validation** - Rapidly iterate designs before real-world research

   - Browse response data

   - View statistical analysis- **Export Functions**: CSV, JSON, Python/R analysis scripts

   - Export results for further analysis

[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)---

---

#### 5ï¸âƒ£ Performance Optimization

## ğŸ“– User Guide

### Core Advantages

### Persona Design Best Practices

- **Parallel Execution**: Async processing for multiple persona responses

#### Creating High-Quality Personas

- **Smart Caching**: Avoid redundant LLM calls, save time and cost## ğŸ“‹ Table of Contents

```python

# Good Example: Specific, detailed, realistic- **Checkpoint Resume**: Support pausing and resuming large-scale simulations

{

    "name": "Li Ming",- **Progress Tracking**: Real-time progress display and estimated completion timeâœ… **Fast Iteration** - Complete survey simulations with hundreds of participants in minutes  

    "age": 32,

    "gender": "Male",

    "occupation": "Software Engineer at Startup",

    "education": "Bachelor's in Computer Science",---âœ… **Cost-Effective** - No need to recruit real participants  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

    "location": "Beijing",

    "background": "Works at a fast-growing tech company, often works overtime. Recently feeling work stress and declining sleep quality. Enjoys relieving stress through exercise but often too busy to do so.",

    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],

    "values": ["Career development", "Work-life balance", "Family health"]## âš¡ Quick Startâœ… **Reproducible** - Precise variable control ensures experimental repeatability  

}



# Bad Example: Vague, generic

{### System Requirementsâœ… **Diverse** - Easily create virtual personas with varied backgrounds, ages, and cultures  - [Overview](#-overview)

    "name": "John Doe",

    "age": 30,

    "gender": "Male",

    "occupation": "Engineer",- **Python**: 3.8 or higherâœ… **Deep Insights** - Obtain detailed qualitative and quantitative data  

    "background": "Regular person",

    "personality_traits": ["Normal"],- **Memory**: 8GB+ recommended

    "values": ["Happiness"]

}- **LLM Provider** (choose one):âœ… **Flexible Deployment** - Support for local deployment and cloud API- [Features](#-features)## English

```

  - LM Studio (local, free)

#### Persona Diversity

  - DeepSeek/OpenAI API key

Ensure virtual samples reflect real population diversity:



- **Age**: Cover different age groups (18-80 years)

- **Gender**: Male, female, non-binary### Installation Steps---- [Quick Start](#-quick-start)

- **Occupation**: Different industries and job levels

- **Education**: High school to graduate degrees

- **Geography**: Urban, rural, different regions

- **Cultural Background**: Different ethnicities, religions, cultural traditions#### Step 1: Clone Repository



### Questionnaire Design Tips



#### Good Question Characteristics```bash## ğŸš€ Features- [User Guide](#-user-guide)ğŸ“– **[View Full English Documentation](./docs/en/README.md)**



âœ… **Clear and Specific**git clone https://github.com/jason-jj-li/auto_sim_ai.git



```cd auto_sim_ai

Good: In the past two weeks, how many days have you felt down or depressed?

Bad: How have you been feeling lately?```

```

### Core Capabilities- [Architecture](#-architecture)

âœ… **Avoid Compound Questions**

#### Step 2: Install Dependencies

```

Good: How many times per week do you exercise? How long is each exercise session?

Bad: How often do you exercise, for how long, and at what intensity?

``````bash



âœ… **Use Standardized Scales**# Create virtual environment (recommended)#### 1ï¸âƒ£ Virtual Persona Management- [Advanced Features](#-advanced-features)ğŸ”¬ **AI-Powered Survey and Intervention Simulation System**



```python -m venv venv

Never(0) - Sometimes(1) - Often(2) - Always(3)

```source venv/bin/activate  # Windows: venv\Scripts\activate



#### Use Built-in Templates



The system includes multiple validated standardized scales:# Install dependencies- **Rich Persona Attributes**: Age, gender, occupation, education, personality traits, values, etc.- [API Documentation](#-api-documentation)



- **PHQ-9**: Depression screening scalepip install -r requirements.txt

- **GAD-7**: Anxiety screening scale

- **PSS-10**: Perceived Stress Scale```- **Batch Creation**: Auto-generate virtual samples matching real population distributions using demographic statistics

- More templates continuously being added...



### Simulation Settings Optimization

Or use the setup script:- **CSV Import/Export**: Bulk import personas from Excel or databases- [FAQ](#-faq)[Quick Start](#quick-start) â€¢

#### Temperature Parameter



Controls response randomness and creativity:

```bash- **Demo Templates**: Built-in typical persona templates, ready to use

- **0.0 - 0.3**: High consistency, suitable for standardized responses

- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)chmod +x setup.sh

- **0.8 - 1.0**: More diverse, suitable for exploratory research and creative testing

./setup.sh- [Contributing](#-contributing)

#### Max Tokens

```

- **150-300**: Short answers (multiple choice, scale ratings)

- **300-500**: Medium length (short answer questions)#### 2ï¸âƒ£ Multiple Simulation Modes

- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)

#### Step 3: Configure LLM

#### Parallel Settings

- [License](#-license)[Features](#features) â€¢Simulate real survey research and intervention effects using LLM-driven virtual personas

- **Small Scale** (<10 personas): Concurrency 2-3

- **Medium Scale** (10-50 personas): Concurrency 5-10**Option A: Local LM Studio (Recommended for Learning and Development)**

- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)

- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)

---

1. Download [LM Studio](https://lmstudio.ai/)

## ğŸ—ï¸ Architecture Design

2. Download a model in LM Studio:- **Intervention Mode**: Test health messaging, advertising copy, etc. on different populations

For detailed architecture documentation, see **[Architecture Guide](./docs/architecture/README.md)**

   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`

### Project Structure

   - Minimum: 7B parameter model- **A/B Testing**: Test multiple versions simultaneously and compare effectiveness

```

auto_sim_ai/3. Start local server:

â”œâ”€â”€ app.py                      # Streamlit main application

â”œâ”€â”€ pages/                      # Multi-page application   - Click "Local Server" tab- **Longitudinal Studies**: Simulate multi-wave surveys to track changes over time---[API Reference](./docs/en/api/README.md) â€¢

â”‚   â”œâ”€â”€ 1_Setup.py             # Persona management page

â”‚   â”œâ”€â”€ 2_Simulation.py        # Simulation execution page   - Select model

â”‚   â””â”€â”€ 3_Results.py           # Results analysis page

â”œâ”€â”€ src/                        # Core modules   - Click "Start Server"- **Sensitivity Analysis**: Systematically test how parameter changes affect results

â”‚   â”œâ”€â”€ llm_client.py          # LLM client (sync/async)

â”‚   â”œâ”€â”€ persona.py             # Persona management   - Confirm address is `http://localhost:1234`

â”‚   â”œâ”€â”€ simulation.py          # Simulation engine (single-thread/parallel)

â”‚   â”œâ”€â”€ storage.py             # Results storage

â”‚   â”œâ”€â”€ cache.py               # Response cache

â”‚   â”œâ”€â”€ checkpoint.py          # Checkpoint management**Option B: Online API (Recommended for Production)**

â”‚   â”œâ”€â”€ scoring.py             # Auto-scoring

â”‚   â”œâ”€â”€ ab_testing.py          # A/B testing#### 3ï¸âƒ£ LLM Integration

â”‚   â”œâ”€â”€ intervention_study.py  # Intervention studies (legacy)

â”‚   â”œâ”€â”€ longitudinal_study.py  # Longitudinal studies (new, recommended)```bash

â”‚   â”œâ”€â”€ persona_generator.py   # Persona generator

â”‚   â”œâ”€â”€ survey_templates.py    # Survey template library# Copy environment template## ğŸ¯ Overview[Contributing](./docs/en/contributing/README.md)[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

â”‚   â”œâ”€â”€ survey_config.py       # Survey configuration

â”‚   â”œâ”€â”€ tools.py               # Tool registration systemcp env.example .env

â”‚   â”œâ”€â”€ ui_components.py       # UI components

â”‚   â”œâ”€â”€ styles.py              # Design system- **Local Deployment**: LM Studio (free, completely private)

â”‚   â””â”€â”€ validators.py          # Input validation

â”œâ”€â”€ tests/                      # Test suite# Edit .env file and add API key

â”œâ”€â”€ data/                       # Data directory

â”‚   â”œâ”€â”€ personas/              # Persona data# DEEPSEEK_API_KEY=your_api_key_here- **Commercial APIs**:

â”‚   â”œâ”€â”€ results/               # Simulation results

â”‚   â”œâ”€â”€ cache/                 # Cache data# or

â”‚   â”œâ”€â”€ checkpoints/           # Checkpoints

â”‚   â””â”€â”€ survey_configs/        # Survey configurations# OPENAI_API_KEY=your_api_key_here  - DeepSeek (cost-effective, Chinese-optimized)

â”œâ”€â”€ docs/                       # Documentation

â”œâ”€â”€ requirements.txt            # Dependencies```

â””â”€â”€ pytest.ini                 # Test configuration

```  - OpenAI (GPT-4, GPT-3.5)**Auto Sim AI** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions.[![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-FF4B4B.svg)](https://streamlit.io)



### Core Module Descriptions#### Step 4: Launch Application



#### LLM Client (`llm_client.py`)  - Other OpenAI-compatible services



Supports both synchronous and asynchronous modes:```bash



- **LMStudioClient**: Sync client, suitable for simple scenariosstreamlit run app.py- **Flexible Switching**: Change models or providers anytime

- **AsyncLLMClient**: Async client, supports high concurrency

```

Compatible with OpenAI API format, seamless switching between providers.



#### Simulation Engine (`simulation.py`)

The app will automatically open in your browser: `http://localhost:8501`

- **SimulationEngine**: Base engine, sequential execution

- **ParallelSimulationEngine**: Parallel engine, supports async batch processing#### 4ï¸âƒ£ Advanced Analysis### Use Cases</div>[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)



Automatically handles error retry, progress tracking, and result aggregation.### First-Time User Guide



#### Cache System (`cache.py`)



Content-hash based smart caching:1. **Connect LLM** (Home Page)



- Same persona + same question = directly return cached result   - Select LLM provider- **Auto-Scoring**: Built-in automatic scoring system for standardized scales

- Support cache export and import

- Significantly reduce LLM API call costs   - Test connection



#### Scoring System (`scoring.py`)   - Wait for "System Ready" message- **Statistical Analysis**: Descriptive statistics, correlation analysis, group comparisons



Automated scoring features:



- Support multiple standardized scales2. **Create Virtual Personas** (Setup Page)- **Consistency Checks**: Validate internal consistency and logic of responses- ğŸ¥ **Health Intervention Research** - Test health messaging impact on different populations

- Configurable custom scoring rules

- Auto-calculate total and subscale scores   - Click "Create Demo Personas" for quick start



---   - Or manually create custom personas- **Visualization**: Interactive charts, word clouds, distribution plots



## ğŸ”¬ Advanced Features   - Or upload CSV for bulk import



> ğŸ’¡ **Tip**: For detailed API documentation and advanced features, see [API Guide](./docs/api/README.md)- **Export Functions**: CSV, JSON, Python/R analysis scripts- ğŸ“Š **Market Research** - Rapidly evaluate product/service user feedback



### 1. A/B Testing3. **Run Simulation** (Simulation Page)



Compare intervention effectiveness across different versions:   - Choose simulation type (Survey/Intervention)



```python   - Select personas to participate

from src import ABTestManager, Condition

   - Choose questionnaire template or enter custom questions#### 5ï¸âƒ£ Performance Optimization- ğŸ“ **Educational Research** - Assess teaching method effectiveness across learner types---ğŸ“– **[View Full English Documentation](./docs/en/README.md)**

# Define test conditions

condition_a = Condition(   - Click "Run Simulation"

    name="Version A",

    intervention_text="Meditating 10 minutes daily can reduce stress.",

    questions=["Would you try this method?"]

)4. **View Results** (Results Page)



condition_b = Condition(   - Browse response data- **Parallel Execution**: Async processing for multiple persona responses- ğŸ’¡ **Policy Analysis** - Predict policy impact on diverse populations

    name="Version B", 

    intervention_text="Research shows that meditating 10 minutes daily can reduce stress levels by 30%.",   - View statistical analysis

    questions=["Would you try this method?"]

)   - Export results for further analysis- **Smart Caching**: Avoid redundant LLM calls, save time and cost



# Run A/B test

ab_manager = ABTestManager()

results = ab_manager.run_test([condition_a, condition_b], personas)---- **Checkpoint Resume**: Support pausing and resuming large-scale simulations- ğŸ§ª **A/B Testing** - Compare effectiveness of different approaches

```



### 2. Longitudinal Studies (Multi-Wave Tracking)

## ğŸ“– User Guide- **Progress Tracking**: Real-time progress display and estimated completion time

Implement realistic longitudinal tracking using conversation memory:



```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig### Persona Design Best Practices- ğŸ“ˆ **Prototype Validation** - Rapidly iterate designs before real-world research



# Configure study waves

waves = [

    WaveConfig(#### Creating High-Quality Personas---

        wave_number=1,

        wave_name="Baseline",

        questions=["What is your current stress level? (1-10)"],

        days_from_baseline=0```python## ğŸ“‹ Table of Contents[Quick Start](./docs/en/quickstart/README.md) â€¢

    ),

    WaveConfig(# Good Example: Specific, detailed, realistic

        wave_number=2,

        wave_name="1 Month Follow-up",{## âš¡ Quick Start

        questions=["What is your stress level now? (1-10)"],

        days_from_baseline=30,    "name": "Li Ming",

        intervention_text="Practice 10 minutes of meditation daily"

    )    "age": 32,### Key Advantages

]

    "gender": "Male",

# Run longitudinal study

config = LongitudinalStudyConfig(    "occupation": "Software Engineer at Startup",### System Requirements

    study_id="stress_study",

    study_name="Stress Intervention Study",    "education": "Bachelor's in Computer Science",

    waves=waves

)    "location": "Beijing",[Features](#features-en) â€¢



engine = LongitudinalStudyEngine(llm_client)    "background": "Works at a fast-growing tech company, often works overtime. Recently feeling work stress and declining sleep quality. Enjoys relieving stress through exercise but often too busy to do so.",

results = engine.run_study(personas, config)

```    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],- **Python**: 3.8 or higher



For detailed longitudinal study guide, see **[Longitudinal Study Guide](./docs/longitudinal/README.md)**    "values": ["Career development", "Work-life balance", "Family health"]



### 3. Batch Persona Generation}- **Memory**: 8GB+ recommendedâœ… **Fast Iteration** - Complete hundreds of survey simulations in minutes  



Generate virtual samples based on real demographic distributions:



```python# Bad Example: Vague, generic- **LLM Provider** (choose one):

from src import PersonaGenerator, DistributionConfig

{

# Configure distribution

config = DistributionConfig(    "name": "John Doe",  - LM Studio (local, free)âœ… **Cost-Effective** - No need to recruit real participants  - [Overview](#overview)[API Reference](./docs/en/api/README.md) â€¢

    age_distribution={

        "18-30": 0.3,    "age": 30,

        "31-50": 0.4,

        "51-70": 0.3    "gender": "Male",  - DeepSeek/OpenAI API key

    },

    gender_distribution={    "occupation": "Engineer",

        "Male": 0.48,

        "Female": 0.52    "background": "Regular person",âœ… **Reproducible** - Precise variable control ensures repeatability  

    }

)    "personality_traits": ["Normal"],



# Generate 100 personas    "values": ["Happiness"]### Installation Steps

generator = PersonaGenerator()

personas = generator.generate_batch(}

    count=100,

    distribution_config=config,```âœ… **Diverse** - Easily create personas with varied backgrounds, ages, cultures  - [Features](#features)[Contributing](./docs/en/contributing/README.md)

    llm_client=client

)

```

#### Persona Diversity#### 1. Clone Repository

### 4. Response Validation



Automatically check response quality and consistency:

Ensure virtual samples reflect real population diversity:âœ… **Deep Insights** - Obtain detailed qualitative and quantitative data  

```python

from src import ResponseValidator, ConsistencyChecker



validator = ResponseValidator()- **Age**: Cover different age groups (18-80 years)```bash

checker = ConsistencyChecker()

- **Gender**: Male, female, non-binary

# Validate response format

is_valid = validator.validate_response(response, question_type)- **Occupation**: Different industries and job levelsgit clone https://github.com/jason-jj-li/auto_sim_ai.gitâœ… **Flexible Deployment** - Support for local and cloud API deployments- [Quick Start](#quick-start)



# Check consistency- **Education**: High school to graduate degrees

metrics = checker.check_consistency(persona_responses)

print(f"Consistency score: {metrics.consistency_score}")- **Geography**: Urban, rural, different regionscd auto_sim_ai

```

- **Cultural Background**: Different ethnicities, religions, cultural traditions

---

```

## ğŸ“š API Documentation

### Questionnaire Design Tips

### PersonaManager



```python

from src import PersonaManager#### Good Question Characteristics



manager = PersonaManager()#### 2. Install Dependencies---- [User Guide](#user-guide)### Features (EN)



# Add personaâœ… **Clear and Specific**

manager.add_persona(persona)



# Get all personas

personas = manager.get_all_personas()```



# Filter by criteriaGood: In the past two weeks, how many days have you felt down or depressed?```bash

young_adults = manager.filter_personas(

    age_range=(18, 30),Bad: How have you been feeling lately?

    gender="Female"

)```# Create virtual environment (recommended)



# Save/load

manager.save_to_file("personas.json")

manager.load_from_file("personas.json")âœ… **Avoid Compound Questions**python -m venv venv## ğŸš€ Features- [ğŸ“š Complete Documentation](./docs/en/README.md)

```



### SimulationEngine

```source venv/bin/activate  # Windows: venv\Scripts\activate

```python

from src import SimulationEngineGood: How many times per week do you exercise? How long is each exercise session?



engine = SimulationEngine(Bad: How often do you exercise, for how long, and at what intensity?

    llm_client=client,

    cache=cache,```

    checkpoint_manager=checkpoint_mgr

)# Install dependencies



# Run surveyâœ… **Use Standardized Scales**

result = engine.run_survey(

    personas=personas,pip install -r requirements.txt### Core Capabilities  - [Quick Start Guide](./docs/en/quickstart/README.md)- **Three Research Modes**: Survey Testing, Message Testing, A/B Testing

    questions=questions,

    temperature=0.7,```

    max_tokens=300

)Never(0) - Sometimes(1) - Often(2) - Always(3)```



# Run intervention```

result = engine.run_intervention(

    personas=personas,

    intervention_text="Health intervention text",

    questions=followup_questions#### Use Built-in Templates

)

```Or use the setup script:



### ResultsStorageThe system includes multiple validated standardized scales:



```python#### 1. Virtual Persona Management  - [API Documentation](./docs/en/api/README.md)- **Longitudinal Studies**: Multi-wave research with persona memory

from src import ResultsStorage

- **PHQ-9**: Depression screening scale

storage = ResultsStorage()

- **GAD-7**: Anxiety screening scale```bash

# Save result

storage.save_result(simulation_result)- **PSS-10**: Perceived Stress Scale



# Load results- More templates continuously being added...chmod +x setup.sh

results = storage.load_all_results()



# Export to CSV

storage.export_to_csv(result, "output.csv")### Simulation Settings Optimization./setup.sh



# Export analysis script

storage.export_analysis_script(result, "analysis.py", language="python")

```#### Temperature Parameter```- **Rich Attributes**: Age, gender, occupation, education, personality traits, values, etc.  - [Architecture Design](./docs/en/architecture/README.md)- **Async Processing**: High-performance parallel simulations



---



## â“ FAQControls response randomness and creativity:



### How many LLM API calls are needed?



Call count = Number of personas Ã— Number of questions. For example:- **0.0 - 0.3**: High consistency, suitable for standardized responses#### 3. Configure LLM- **Batch Creation**: Auto-generate samples matching real population distributions



- 10 personas Ã— 9 questions = 90 calls- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)

- Using cache can significantly reduce repeat calls

- **0.8 - 1.0**: More diverse, suitable for exploratory research and creative testing

### How long does simulation take?



Depends on:

#### Max Tokens**Option A: Local LM Studio (Recommended for Learning and Development)**- **CSV Import/Export**: Bulk import personas from Excel or databases  - [Longitudinal Studies](./docs/en/longitudinal/README.md)- **Flexible LLM Support**: Local (LM Studio) or API (DeepSeek, OpenAI)

- **Local model**: ~5-15 seconds/response

- **Online API**: ~1-3 seconds/response

- **Parallel execution**: Can reduce time by 50-80%

- **150-300**: Short answers (multiple choice, scale ratings)

### How reliable are the results?

- **300-500**: Medium length (short answer questions)

LLM simulation is an exploratory research tool, suitable for:

- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)1. Download [LM Studio](https://lmstudio.ai/)- **Demo Templates**: Built-in templates for common persona types, ready to use

- âœ… Rapid prototype testing

- âœ… Hypothesis generation

- âœ… Questionnaire pre-testing

- âŒ **Cannot** replace real human research#### Parallel Settings2. Download a model in LM Studio:



### How to improve response quality?



1. Create detailed, realistic persona backgrounds- **Small Scale** (<10 personas): Concurrency 2-3   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`  - [Contributing Guide](./docs/en/contributing/README.md)- **Web Interface**: User-friendly Streamlit UI

2. Use clear, specific questions

3. Choose appropriate temperature parameters- **Medium Scale** (10-50 personas): Concurrency 5-10

4. Use more powerful models (e.g., GPT-4)

5. Enable response validation and consistency checks- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)   - Minimum: 7B parameter model



### What are the costs?



- **Local LM Studio**: Completely free (requires GPU)---3. Start local server:#### 2. Multiple Simulation Modes

- **DeepSeek API**: ~$0.0001/1k tokens, extremely low cost

- **OpenAI GPT-3.5**: ~$0.002/1k tokens

- **OpenAI GPT-4**: ~$0.03/1k tokens

## ğŸ—ï¸ Architecture Design   - Click "Local Server" tab

### Is my data secure?



- Local mode: Data never leaves your machine

- API mode: Follows each provider's privacy policyFor detailed architecture documentation, see **[Architecture Guide](./docs/architecture/README.md)**   - Select model- [FAQ](#faq)- **Complete Data Export**: CSV, JSON formats for statistical analysis

- Recommendation: Use local mode for sensitive data



---

### Project Structure   - Click "Start Server"

## ğŸ¤ Contributing



Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

```   - Confirm address is `http://localhost:1234`- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)

### Development Setup

auto_sim_ai/

```bash

# Install development dependenciesâ”œâ”€â”€ app.py                      # Streamlit main application

pip install -r requirements-dev.txt

â”œâ”€â”€ pages/                      # Multi-page application

# Run tests

pytestâ”‚   â”œâ”€â”€ 1_Setup.py             # Persona management page**Option B: Online API (Recommended for Production)**- **Intervention Mode**: Test health messages, ad copy, etc. on different populations- [License](#license)



# Code formattingâ”‚   â”œâ”€â”€ 2_Simulation.py        # Simulation execution page

black src/ tests/

isort src/ tests/â”‚   â””â”€â”€ 3_Results.py           # Results analysis page



# Type checkingâ”œâ”€â”€ src/                        # Core modules

mypy src/

```â”‚   â”œâ”€â”€ llm_client.py          # LLM client (sync/async)```bash- **A/B Testing**: Test multiple versions simultaneously and compare effects



### Report Issuesâ”‚   â”œâ”€â”€ persona.py             # Persona management



Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).â”‚   â”œâ”€â”€ simulation.py          # Simulation engine (single-thread/parallel)# Copy environment template



---â”‚   â”œâ”€â”€ storage.py             # Results storage



## ğŸ“„ Licenseâ”‚   â”œâ”€â”€ cache.py               # Response cachecp env.example .env- **Longitudinal Studies**: Multi-wave research to track changes over time### Quick Start (EN)



This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.â”‚   â”œâ”€â”€ checkpoint.py          # Checkpoint management



---â”‚   â”œâ”€â”€ scoring.py             # Auto-scoring



## ğŸ™ Acknowledgmentsâ”‚   â”œâ”€â”€ ab_testing.py          # A/B testing



- [Streamlit](https://streamlit.io/) - Excellent Python web frameworkâ”‚   â”œâ”€â”€ intervention_study.py  # Intervention studies (legacy)# Edit .env file and add API key- **Sensitivity Analysis**: Systematically test parameter impact on results

- [LM Studio](https://lmstudio.ai/) - Local LLM runtime environment

- [OpenAI](https://openai.com/) - API standardsâ”‚   â”œâ”€â”€ longitudinal_study.py  # Longitudinal studies (new, recommended)

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM service

â”‚   â”œâ”€â”€ persona_generator.py   # Persona generator# DEEPSEEK_API_KEY=your_api_key_here

---

â”‚   â”œâ”€â”€ survey_templates.py    # Survey template library

## ğŸ“ Contact

â”‚   â”œâ”€â”€ survey_config.py       # Survey configuration# or---

- **Maintainer**: Jason Li

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)â”‚   â”œâ”€â”€ tools.py               # Tool registration system

- **Email**: [Contact via GitHub Issues]

â”‚   â”œâ”€â”€ ui_components.py       # UI components# OPENAI_API_KEY=your_api_key_here

---

â”‚   â”œâ”€â”€ styles.py              # Design system

<div align="center">

â”‚   â””â”€â”€ validators.py          # Input validation```#### 3. LLM Integration

**â­ If this project helps you, please give it a star!**

â”œâ”€â”€ tests/                      # Test suite

Made with â¤ï¸ by Jason Li

â”œâ”€â”€ data/                       # Data directory

</div>

â”‚   â”œâ”€â”€ personas/              # Persona data

â”‚   â”œâ”€â”€ results/               # Simulation results#### 4. Launch Application```bash

â”‚   â”œâ”€â”€ cache/                 # Cache data

â”‚   â”œâ”€â”€ checkpoints/           # Checkpoints

â”‚   â””â”€â”€ survey_configs/        # Survey configurations

â”œâ”€â”€ docs/                       # Documentation```bash- **Local Deployment**: LM Studio (free, completely private)

â”œâ”€â”€ requirements.txt            # Dependencies

â””â”€â”€ pytest.ini                 # Test configurationstreamlit run app.py

```

```- **Commercial APIs**:## ğŸ¯ Overviewgit clone https://github.com/jason-jj-li/auto_sim_ai.git

### Core Module Descriptions



#### LLM Client (`llm_client.py`)

The app will automatically open in your browser: `http://localhost:8501`  - DeepSeek (cost-effective, Chinese-optimized)

Supports both synchronous and asynchronous modes:



- **LMStudioClient**: Sync client, suitable for simple scenarios

- **AsyncLLMClient**: Async client, supports high concurrency### First-Time User Guide  - OpenAI (GPT-4, GPT-3.5)cd auto_sim_ai



Compatible with OpenAI API format, seamless switching between providers.



#### Simulation Engine (`simulation.py`)1. **Connect LLM** (Home Page)  - Other OpenAI-compatible services



- **SimulationEngine**: Base engine, sequential execution   - Select LLM provider

- **ParallelSimulationEngine**: Parallel engine, supports async batch processing

   - Test connection- **Flexible Switching**: Change models or providers anytime**Auto Sim AI** is an innovative research tool that leverages Large Language Models (LLMs) to generate virtual personas that simulate real human responses to surveys and interventions../setup.sh

Automatically handles error retry, progress tracking, and result aggregation.

   - Wait for "System Ready" message

#### Cache System (`cache.py`)



Content-hash based smart caching:

2. **Create Virtual Personas** (Setup Page)

- Same persona + same question = directly return cached result

- Support cache export and import   - Click "Create Demo Personas" for quick start#### 4. Advanced Analysisstreamlit run app.py

- Significantly reduce LLM API call costs

   - Or manually create custom personas

#### Scoring System (`scoring.py`)

   - Or upload CSV for bulk import

Automated scoring features:



- Support multiple standardized scales

- Configurable custom scoring rules3. **Run Simulation** (Simulation Page)- **Auto-Scoring**: Built-in scoring for standardized scales### Use Cases```

- Auto-calculate total and subscale scores

   - Choose simulation type (Survey/Intervention)

---

   - Select personas to participate- **Statistical Analysis**: Descriptive stats, correlation, group comparisons

## ğŸ”¬ Advanced Features

   - Choose questionnaire template or enter custom questions

> ğŸ’¡ **Tip**: For detailed API documentation and advanced features, see [API Guide](./docs/api/README.md)

   - Click "Run Simulation"- **Consistency Checks**: Validate response internal consistency and logic

### 1. A/B Testing



Compare intervention effectiveness across different versions:

4. **View Results** (Results Page)- **Visualization**: Interactive charts, word clouds, distributions

```python

from src import ABTestManager, Condition   - Browse response data



# Define test conditions   - View statistical analysis- **Export Features**: CSV, JSON, Python/R analysis scripts- ğŸ¥ **Health Intervention Research** - Test health messaging impact on different populationsğŸ“˜ **[Complete English Documentation â†’](./docs/en/README.md)**

condition_a = Condition(

    name="Version A",   - Export results for further analysis

    intervention_text="Meditating 10 minutes daily can reduce stress.",

    questions=["Would you try this method?"]

)

---

condition_b = Condition(

    name="Version B", #### 5. Performance Optimization- ğŸ“Š **Market Research** - Rapidly evaluate product/service user feedback

    intervention_text="Research shows that meditating 10 minutes daily can reduce stress levels by 30%.",

    questions=["Would you try this method?"]## ğŸ“– User Guide

)



# Run A/B test

ab_manager = ABTestManager()### Persona Design Best Practices

results = ab_manager.run_test([condition_a, condition_b], personas)

```- **Parallel Execution**: Async processing for multiple persona responses- ğŸ“ **Educational Research** - Assess teaching method effectiveness across learner types---



### 2. Longitudinal Studies (Multi-Wave Tracking)#### Creating High-Quality Personas



Implement realistic longitudinal tracking using conversation memory:- **Smart Caching**: Avoid redundant LLM calls, save time and cost



```python```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig

# Good Example: Specific, detailed, realistic- **Resume Capability**: Pause and resume large-scale simulations- ğŸ’¡ **Policy Analysis** - Predict policy impact on diverse populations

# Configure study waves

waves = [{

    WaveConfig(

        wave_number=1,    "name": "Li Ming",- **Progress Tracking**: Real-time progress and estimated completion time

        wave_name="Baseline",

        questions=["What is your current stress level? (1-10)"],    "age": 32,

        days_from_baseline=0

    ),    "gender": "Male",- ğŸ§ª **A/B Testing** - Compare effectiveness of different approaches## ä¸­æ–‡

    WaveConfig(

        wave_number=2,    "occupation": "Software Engineer at Startup",

        wave_name="1 Month Follow-up",

        questions=["What is your stress level now? (1-10)"],    "education": "Bachelor's in Computer Science",---

        days_from_baseline=30,

        intervention_text="Practice 10 minutes of meditation daily"    "location": "Beijing",

    )

]    "background": "Works at a fast-growing tech company, often works overtime. Recently feeling work stress and declining sleep quality. Enjoys relieving stress through exercise but often too busy to do so.",- ğŸ“ˆ **Prototype Validation** - Rapidly iterate designs before real-world research



# Run longitudinal study    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],

config = LongitudinalStudyConfig(

    study_id="stress_study",    "values": ["Career development", "Work-life balance", "Family health"]## âš¡ Quick Start

    study_name="Stress Intervention Study",

    waves=waves}

)

ğŸ”¬ **åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è°ƒæŸ¥ä¸å¹²é¢„æ¨¡æ‹Ÿç³»ç»Ÿ**

engine = LongitudinalStudyEngine(llm_client)

results = engine.run_study(personas, config)# Bad Example: Vague, generic

```

{### System Requirements

For detailed longitudinal study guide, see **[Longitudinal Study Guide](./docs/longitudinal/README.md)**

    "name": "John Doe",

### 3. Batch Persona Generation

    "age": 30,### Key Advantages

Generate virtual samples based on real demographic distributions:

    "gender": "Male",

```python

from src import PersonaGenerator, DistributionConfig    "occupation": "Engineer",- **Python**: 3.8 or higher



# Configure distribution    "background": "Regular person",

config = DistributionConfig(

    age_distribution={    "personality_traits": ["Normal"],- **Memory**: 8GB+ recommendedä½¿ç”¨ AI é©±åŠ¨çš„è™šæ‹Ÿäººç‰©æ¨¡æ‹ŸçœŸå®çš„è°ƒæŸ¥ç ”ç©¶å’Œå¹²é¢„æ•ˆæœ

        "18-30": 0.3,

        "31-50": 0.4,    "values": ["Happiness"]

        "51-70": 0.3

    },}- **LLM Provider** (choose one):

    gender_distribution={

        "Male": 0.48,```

        "Female": 0.52

    }  - LM Studio (local, free)âœ… **Fast Iteration** - Complete hundreds of survey simulations in minutes  

)

#### Persona Diversity

# Generate 100 personas

generator = PersonaGenerator()  - DeepSeek/OpenAI API key

personas = generator.generate_batch(

    count=100,Ensure virtual samples reflect real population diversity:

    distribution_config=config,

    llm_client=clientâœ… **Cost-Effective** - No need to recruit real participants  ğŸ“— **[æŸ¥çœ‹å®Œæ•´ä¸­æ–‡æ–‡æ¡£](./docs/zh/README.md)**

)

```- **Age**: Cover different age groups (18-80 years)



### 4. Response Validation- **Gender**: Male, female, non-binary### Installation Steps



Automatically check response quality and consistency:- **Occupation**: Different industries and job levels



```python- **Education**: High school to graduate degreesâœ… **Reproducible** - Precise variable control ensures repeatability  

from src import ResponseValidator, ConsistencyChecker

- **Geography**: Urban, rural, different regions

validator = ResponseValidator()

checker = ConsistencyChecker()- **Cultural Background**: Different ethnicities, religions, cultural traditions#### Step 1: Clone Repository



# Validate response format

is_valid = validator.validate_response(response, question_type)

### Questionnaire Design Tipsâœ… **Diverse** - Easily create personas with varied backgrounds, ages, cultures  [å¿«é€Ÿå¼€å§‹](./docs/zh/quickstart/README.md) â€¢

# Check consistency

metrics = checker.check_consistency(persona_responses)

print(f"Consistency score: {metrics.consistency_score}")

```#### Good Question Characteristics```bash



---



## ğŸ“š API Documentationâœ… **Clear and Specific**git clone https://github.com/jason-jj-li/auto_sim_ai.gitâœ… **Deep Insights** - Obtain detailed qualitative and quantitative data  [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§) â€¢



### PersonaManager



```python```cd auto_sim_ai

from src import PersonaManager

Good: In the past two weeks, how many days have you felt down or depressed?

manager = PersonaManager()

Bad: How have you been feeling lately?```âœ… **Flexible Deployment** - Support for local and cloud API deployments[API å‚è€ƒ](./docs/zh/api/README.md) â€¢

# Add persona

manager.add_persona(persona)```



# Get all personas

personas = manager.get_all_personas()

âœ… **Avoid Compound Questions**

# Filter by criteria

young_adults = manager.filter_personas(#### Step 2: Install Dependencies[è´¡çŒ®æŒ‡å—](./docs/zh/contributing/README.md)

    age_range=(18, 30),

    gender="Female"```

)

Good: How many times per week do you exercise? How long is each exercise session?

# Save/load

manager.save_to_file("personas.json")Bad: How often do you exercise, for how long, and at what intensity?

manager.load_from_file("personas.json")

``````**Option A: Automated Setup (Recommended)**---



### SimulationEngine



```pythonâœ… **Use Standardized Scales**

from src import SimulationEngine



engine = SimulationEngine(

    llm_client=client,``````bash</div>

    cache=cache,

    checkpoint_manager=checkpoint_mgrNever(0) - Sometimes(1) - Often(2) - Always(3)

)

```chmod +x setup.sh

# Run survey

result = engine.run_survey(

    personas=personas,

    questions=questions,#### Use Built-in Templates./setup.sh## ğŸš€ Features

    temperature=0.7,

    max_tokens=300

)

The system includes multiple validated standardized scales:```

# Run intervention

result = engine.run_intervention(

    personas=personas,

    intervention_text="Health intervention text",- **PHQ-9**: Depression screening scale---

    questions=followup_questions

)- **GAD-7**: Anxiety screening scale

```

- **PSS-10**: Perceived Stress Scale**Option B: Manual Installation**

### ResultsStorage

- More templates continuously being added...

```python

from src import ResultsStorage### Core Capabilities



storage = ResultsStorage()### Simulation Settings Optimization



# Save result```bash

storage.save_result(simulation_result)

#### Temperature Parameter

# Load results

results = storage.load_all_results()# Create virtual environment## ğŸ“‹ ç›®å½•



# Export to CSVControls response randomness and creativity:

storage.export_to_csv(result, "output.csv")

python -m venv venv

# Export analysis script

storage.export_analysis_script(result, "analysis.py", language="python")- **0.0 - 0.3**: High consistency, suitable for standardized responses

```

- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)source venv/bin/activate  # Windows: venv\Scripts\activate#### 1ï¸âƒ£ Virtual Persona Management

---

- **0.8 - 1.0**: More diverse, suitable for exploratory research and creative testing

## â“ FAQ



### How many LLM API calls are needed?

#### Max Tokens

Call count = Number of personas Ã— Number of questions. For example:

# Install dependencies- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)

- 10 personas Ã— 9 questions = 90 calls

- Using cache can significantly reduce repeat calls- **150-300**: Short answers (multiple choice, scale ratings)



### How long does simulation take?- **300-500**: Medium length (short answer questions)pip install -r requirements.txt



Depends on:- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)



- **Local model**: ~5-15 seconds/response```- **Rich Attributes**: Age, gender, occupation, education, personality traits, values, etc.- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)

- **Online API**: ~1-3 seconds/response

- **Parallel execution**: Can reduce time by 50-80%#### Parallel Settings



### How reliable are the results?



LLM simulation is an exploratory research tool, suitable for:- **Small Scale** (<10 personas): Concurrency 2-3



- âœ… Rapid prototype testing- **Medium Scale** (10-50 personas): Concurrency 5-10#### Step 3: Configure LLM- **Batch Creation**: Auto-generate samples matching real population distributions- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)

- âœ… Hypothesis generation

- âœ… Questionnaire pre-testing- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)

- âŒ **Cannot** replace real human research



### How to improve response quality?

---

1. Create detailed, realistic persona backgrounds

2. Use clear, specific questions**Option A: Local LM Studio (Recommended for Learning)**- **CSV Import/Export**: Bulk import personas from Excel or databases- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)

3. Choose appropriate temperature parameters

4. Use more powerful models (e.g., GPT-4)## ğŸ—ï¸ Architecture Design

5. Enable response validation and consistency checks



### What are the costs?

For detailed architecture documentation, see **[Architecture Guide](./docs/architecture/README.md)**

- **Local LM Studio**: Completely free (requires GPU)

- **DeepSeek API**: ~$0.0001/1k tokens, extremely low cost1. Download [LM Studio](https://lmstudio.ai/)- **Demo Templates**: Built-in templates for common persona types, ready to use- [ğŸ“š å®Œæ•´æ–‡æ¡£](./docs/README.md)

- **OpenAI GPT-3.5**: ~$0.002/1k tokens

- **OpenAI GPT-4**: ~$0.03/1k tokens### Project Structure



### Is my data secure?2. Download a model in LM Studio:



- Local mode: Data never leaves your machine```

- API mode: Follows each provider's privacy policy

- Recommendation: Use local mode for sensitive dataauto_sim_ai/   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`  - [å¿«é€Ÿå¼€å§‹æŒ‡å—](./docs/quickstart/README.md)



---â”œâ”€â”€ app.py                      # Streamlit main application



## ğŸ¤ Contributingâ”œâ”€â”€ pages/                      # Multi-page application   - Minimum: 7B parameter model



Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.â”‚   â”œâ”€â”€ 1_Setup.py             # Persona management page



### Development Setupâ”‚   â”œâ”€â”€ 2_Simulation.py        # Simulation execution page3. Start local server:#### 2ï¸âƒ£ Multiple Simulation Modes  - [APIæ–‡æ¡£](./docs/api/README.md)



```bashâ”‚   â””â”€â”€ 3_Results.py           # Results analysis page

# Install development dependencies

pip install -r requirements-dev.txtâ”œâ”€â”€ src/                        # Core modules   - Click "Local Server" tab



# Run testsâ”‚   â”œâ”€â”€ llm_client.py          # LLM client (sync/async)

pytest

â”‚   â”œâ”€â”€ persona.py             # Persona management   - Select model  - [æ¶æ„è®¾è®¡](./docs/architecture/README.md)

# Code formatting

black src/ tests/â”‚   â”œâ”€â”€ simulation.py          # Simulation engine (single-thread/parallel)

isort src/ tests/

â”‚   â”œâ”€â”€ storage.py             # Results storage   - Click "Start Server"

# Type checking

mypy src/â”‚   â”œâ”€â”€ cache.py               # Response cache

```

â”‚   â”œâ”€â”€ checkpoint.py          # Checkpoint management   - Confirm address is `http://localhost:1234`- **Survey Mode**: Run standardized questionnaires (PHQ-9, GAD-7, etc.)  - [çºµå‘ç ”ç©¶æŒ‡å—](./docs/longitudinal/README.md)

### Report Issues

â”‚   â”œâ”€â”€ scoring.py             # Auto-scoring

Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).

â”‚   â”œâ”€â”€ ab_testing.py          # A/B testing

---

â”‚   â”œâ”€â”€ intervention_study.py  # Intervention studies (legacy)

## ğŸ“„ License

â”‚   â”œâ”€â”€ longitudinal_study.py  # Longitudinal studies (new, recommended)**Option B: Online API (Recommended for Production)**- **Intervention Mode**: Test health messages, ad copy, etc. on different populations  - [è´¡çŒ®æŒ‡å—](./docs/contributing/README.md)

This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.

â”‚   â”œâ”€â”€ persona_generator.py   # Persona generator

---

â”‚   â”œâ”€â”€ survey_templates.py    # Survey template library

## ğŸ™ Acknowledgments

â”‚   â”œâ”€â”€ survey_config.py       # Survey configuration

- [Streamlit](https://streamlit.io/) - Excellent Python web framework

- [LM Studio](https://lmstudio.ai/) - Local LLM runtime environmentâ”‚   â”œâ”€â”€ tools.py               # Tool registration system```bash- **A/B Testing**: Test multiple versions simultaneously and compare effects- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

- [OpenAI](https://openai.com/) - API standards

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM serviceâ”‚   â”œâ”€â”€ ui_components.py       # UI components



---â”‚   â”œâ”€â”€ styles.py              # Design system# Copy environment template



## ğŸ“ Contactâ”‚   â””â”€â”€ validators.py          # Input validation



- **Maintainer**: Jason Liâ”œâ”€â”€ tests/                      # Test suitecp env.example .env- **Longitudinal Studies**: Multi-wave research to track changes over time- [è®¸å¯è¯](#è®¸å¯è¯)

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)

- **Email**: [Contact via GitHub Issues]â”œâ”€â”€ data/                       # Data directory



---â”‚   â”œâ”€â”€ personas/              # Persona data



<div align="center">â”‚   â”œâ”€â”€ results/               # Simulation results



**â­ If this project helps you, please give it a star!**â”‚   â”œâ”€â”€ cache/                 # Cache data# Edit .env file and add your API key- **Sensitivity Analysis**: Systematically test parameter impact on results



Made with â¤ï¸ by Jason Liâ”‚   â”œâ”€â”€ checkpoints/           # Checkpoints



</div>â”‚   â””â”€â”€ survey_configs/        # Survey configurations# DEEPSEEK_API_KEY=your_api_key_here

â”œâ”€â”€ docs/                       # Documentation

â”œâ”€â”€ requirements.txt            # Dependencies# or---

â””â”€â”€ pytest.ini                 # Test configuration

```# OPENAI_API_KEY=your_api_key_here



### Core Module Descriptions```#### 3ï¸âƒ£ LLM Integration



#### LLM Client (`llm_client.py`)



Supports both synchronous and asynchronous modes:#### Step 4: Launch Application## ğŸ¯ é¡¹ç›®ç®€ä»‹



- **LMStudioClient**: Sync client, suitable for simple scenarios

- **AsyncLLMClient**: Async client, supports high concurrency

```bash- **Local Deployment**: LM Studio (free, completely private)

Compatible with OpenAI API format, seamless switching between providers.

streamlit run app.py

#### Simulation Engine (`simulation.py`)

```- **Commercial APIs**:**LLM Simulation Survey System** æ˜¯ä¸€ä¸ªåˆ›æ–°çš„ç ”ç©¶å·¥å…·ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè™šæ‹Ÿäººç‰©ï¼ˆPersonasï¼‰ï¼Œæ¨¡æ‹ŸçœŸå®äººç¾¤å¯¹è°ƒæŸ¥é—®å·å’Œå¹²é¢„æªæ–½çš„å“åº”ã€‚

- **SimulationEngine**: Base engine, sequential execution

- **ParallelSimulationEngine**: Parallel engine, supports async batch processing



Automatically handles error retry, progress tracking, and result aggregation.The app will automatically open in your browser at: `http://localhost:8501`  - DeepSeek (cost-effective, Chinese-optimized)



#### Cache System (`cache.py`)



Content-hash based smart caching:### First-Time User Guide  - OpenAI (GPT-4, GPT-3.5)### é€‚ç”¨åœºæ™¯



- Same persona + same question = directly return cached result

- Support cache export and import

- Significantly reduce LLM API call costs**1. Connect LLM (Home Page)**  - Other OpenAI-compatible services



#### Scoring System (`scoring.py`)   - Select your LLM provider



Automated scoring features:   - Test the connection- **Flexible Switching**: Change models or providers anytime- ğŸ¥ **å¥åº·å¹²é¢„ç ”ç©¶** - æµ‹è¯•å¥åº·ä¿¡æ¯å¯¹ä¸åŒäººç¾¤çš„å½±å“



- Support multiple standardized scales   - Wait for "System Ready" message

- Configurable custom scoring rules

- Auto-calculate total and subscale scores- ğŸ“Š **å¸‚åœºè°ƒç ”** - å¿«é€Ÿè¯„ä¼°äº§å“æˆ–æœåŠ¡çš„ç”¨æˆ·åé¦ˆ



---**2. Create Virtual Personas (Setup Page)**



## ğŸ”¬ Advanced Features   - Click "Create Demo Personas" for quick start#### 4ï¸âƒ£ Advanced Analysis- ğŸ“ **æ•™è‚²ç ”ç©¶** - è¯„ä¼°æ•™å­¦æ–¹æ³•å¯¹ä¸åŒå­¦ä¹ è€…çš„æ•ˆæœ



> ğŸ’¡ **Tip**: For detailed API documentation and advanced features, see [API Guide](./docs/api/README.md)   - Or manually create custom personas



### 1. A/B Testing   - Or upload CSV for bulk import- ğŸ’¡ **æ”¿ç­–åˆ†æ** - é¢„æµ‹æ”¿ç­–å¯¹å¤šå…ƒç¾¤ä½“çš„æ½œåœ¨å½±å“



Compare intervention effectiveness across different versions:



```python**3. Run Simulation (Simulation Page)**- **Auto-Scoring**: Built-in scoring for standardized scales- ğŸ§ª **A/B æµ‹è¯•** - æ¯”è¾ƒä¸åŒæ–¹æ¡ˆçš„æ•ˆæœå·®å¼‚

from src import ABTestManager, Condition

   - Choose simulation type (Survey/Intervention/A/B Test)

# Define test conditions

condition_a = Condition(   - Select personas to participate- **Statistical Analysis**: Descriptive stats, correlation, group comparisons- ğŸ“ˆ **åŸå‹éªŒè¯** - åœ¨çœŸå®è°ƒç ”å‰å¿«é€Ÿè¿­ä»£è®¾è®¡

    name="Version A",

    intervention_text="Meditating 10 minutes daily can reduce stress.",   - Choose questionnaire template or enter custom questions

    questions=["Would you try this method?"]

)   - Click "Run Simulation"- **Consistency Checks**: Validate response internal consistency and logic



condition_b = Condition(

    name="Version B", 

    intervention_text="Research shows that meditating 10 minutes daily can reduce stress levels by 30%.",**4. View Results (Results Page)**- **Visualization**: Interactive charts, word clouds, distributions### æ ¸å¿ƒä¼˜åŠ¿

    questions=["Would you try this method?"]

)   - Browse response data



# Run A/B test   - View statistical analysis- **Export Features**: CSV, JSON, Python/R analysis scripts

ab_manager = ABTestManager()

results = ab_manager.run_test([condition_a, condition_b], personas)   - Export results for further analysis

```

âœ… **å¿«é€Ÿè¿­ä»£** - å‡ åˆ†é’Ÿå†…å®Œæˆæ•°ç™¾äººçš„è°ƒæŸ¥æ¨¡æ‹Ÿ  

### 2. Longitudinal Studies (Multi-Wave Tracking)

---

Implement realistic longitudinal tracking using conversation memory:

#### 5ï¸âƒ£ Performance Optimizationâœ… **æˆæœ¬ä½å»‰** - æ— éœ€æ‹›å‹ŸçœŸå®å‚ä¸è€…  

```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig## ğŸ“– User Guide



# Configure study wavesâœ… **å¯é‡å¤æ€§** - ç²¾ç¡®æ§åˆ¶å˜é‡ï¼Œç¡®ä¿å®éªŒå¯é‡å¤  

waves = [

    WaveConfig(### Persona Design Best Practices

        wave_number=1,

        wave_name="Baseline",- **Parallel Execution**: Async processing for multiple persona responsesâœ… **å¤šæ ·åŒ–** - è½»æ¾åˆ›å»ºä¸åŒèƒŒæ™¯ã€å¹´é¾„ã€æ–‡åŒ–çš„è™šæ‹Ÿäººç‰©  

        questions=["What is your current stress level? (1-10)"],

        days_from_baseline=0#### Creating High-Quality Personas

    ),

    WaveConfig(- **Smart Caching**: Avoid redundant LLM calls, save time and costâœ… **æ·±åº¦æ´å¯Ÿ** - è·å¾—è¯¦ç»†çš„è´¨æ€§å’Œé‡åŒ–æ•°æ®  

        wave_number=2,

        wave_name="1 Month Follow-up",**Good Example: Specific, detailed, realistic**

        questions=["What is your stress level now? (1-10)"],

        days_from_baseline=30,- **Resume Capability**: Pause and resume large-scale simulationsâœ… **çµæ´»éƒ¨ç½²** - æ”¯æŒæœ¬åœ°è¿è¡Œå’Œäº‘ç«¯API

        intervention_text="Practice 10 minutes of meditation daily"

    )```python

]

{- **Progress Tracking**: Real-time progress and estimated completion time

# Run longitudinal study

config = LongitudinalStudyConfig(    "name": "Sarah Chen",

    study_id="stress_study",

    study_name="Stress Intervention Study",    "age": 32,---

    waves=waves

)    "gender": "Female",



engine = LongitudinalStudyEngine(llm_client)    "occupation": "Software Engineer at startup",---

results = engine.run_study(personas, config)

```    "education": "Bachelor's in Computer Science",



For detailed longitudinal study guide, see **[Longitudinal Study Guide](./docs/longitudinal/README.md)**    "location": "San Francisco, CA",## ğŸš€ åŠŸèƒ½ç‰¹æ€§



### 3. Batch Persona Generation    "background": "Works at a fast-growing tech company, often works overtime. Recently experiencing work stress and sleep quality decline. Likes to relieve stress through exercise but often too busy.",



Generate virtual samples based on real demographic distributions:    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],## âš¡ Quick Start



```python    "values": ["Career development", "Work-life balance", "Family health"]

from src import PersonaGenerator, DistributionConfig

}### æ ¸å¿ƒåŠŸèƒ½

# Configure distribution

config = DistributionConfig(```

    age_distribution={

        "18-30": 0.3,### System Requirements

        "31-50": 0.4,

        "51-70": 0.3**Bad Example: Vague, generic**

    },

    gender_distribution={#### 1ï¸âƒ£ è™šæ‹Ÿäººç‰©ç®¡ç†

        "Male": 0.48,

        "Female": 0.52```python

    }

){- **Python**: 3.8 or higher- **ä¸°å¯Œçš„äººç‰©å±æ€§**ï¼šå¹´é¾„ã€æ€§åˆ«ã€èŒä¸šã€æ•™è‚²èƒŒæ™¯ã€æ€§æ ¼ç‰¹å¾ã€ä»·å€¼è§‚ç­‰



# Generate 100 personas    "name": "John Doe",

generator = PersonaGenerator()

personas = generator.generate_batch(    "age": 30,- **Memory**: 8GB+ recommended- **æ‰¹é‡åˆ›å»º**ï¼šä½¿ç”¨äººå£ç»Ÿè®¡åˆ†å¸ƒè‡ªåŠ¨ç”Ÿæˆç¬¦åˆçœŸå®äººå£çš„è™šæ‹Ÿæ ·æœ¬

    count=100,

    distribution_config=config,    "gender": "Male",

    llm_client=client

)    "occupation": "Engineer",- **LLM Provider** (choose one):- **CSV å¯¼å…¥/å¯¼å‡º**ï¼šæ”¯æŒä»Excelæˆ–æ•°æ®åº“æ‰¹é‡å¯¼å…¥äººç‰©

```

    "background": "Regular person",

### 4. Response Validation

    "personality_traits": ["Normal"],  - LM Studio (local, free)- **æ¼”ç¤ºæ¨¡æ¿**ï¼šå†…ç½®å¤šç§å…¸å‹äººç‰©æ¨¡æ¿ï¼Œå³å¼€å³ç”¨

Automatically check response quality and consistency:

    "values": ["Happiness"]

```python

from src import ResponseValidator, ConsistencyChecker}  - DeepSeek/OpenAI API key



validator = ResponseValidator()```

checker = ConsistencyChecker()

#### 2ï¸âƒ£ å¤šç§æ¨¡æ‹Ÿæ¨¡å¼

# Validate response format

is_valid = validator.validate_response(response, question_type)#### Persona Diversity Guidelines



# Check consistency### Installation- **è°ƒæŸ¥æ¨¡å¼**ï¼šè¿è¡Œæ ‡å‡†åŒ–é—®å·ï¼ˆPHQ-9ã€GAD-7 ç­‰ï¼‰

metrics = checker.check_consistency(persona_responses)

print(f"Consistency score: {metrics.consistency_score}")Ensure your virtual sample reflects real population diversity:

```

- **å¹²é¢„æ¨¡å¼**ï¼šæµ‹è¯•å¥åº·ä¿¡æ¯ã€å¹¿å‘Šæ–‡æ¡ˆç­‰å¯¹ä¸åŒäººç¾¤çš„å½±å“

---

- **Age**: Cover different age groups (18-80 years)

## ğŸ“š API Documentation

- **Gender**: Male, female, non-binary#### 1. Clone Repository- **A/B æµ‹è¯•**ï¼šåŒæ—¶æµ‹è¯•å¤šä¸ªç‰ˆæœ¬ï¼Œæ¯”è¾ƒæ•ˆæœå·®å¼‚

### PersonaManager

- **Occupation**: Different industries and position levels

```python

from src import PersonaManager- **Education**: High school to graduate degrees- **çºµå‘ç ”ç©¶**ï¼šæ¨¡æ‹Ÿå¤šæ³¢æ¬¡è°ƒæŸ¥ï¼Œè¿½è¸ªå˜åŒ–è¶‹åŠ¿



manager = PersonaManager()- **Geography**: Urban, rural, different regions



# Add persona- **Cultural Background**: Different ethnicities, religions, cultural traditions```bash- **æ•æ„Ÿæ€§åˆ†æ**ï¼šç³»ç»Ÿæ€§æµ‹è¯•å‚æ•°å˜åŒ–å¯¹ç»“æœçš„å½±å“

manager.add_persona(persona)



# Get all personas

personas = manager.get_all_personas()### Questionnaire Design Tipsgit clone https://github.com/jason-jj-li/auto_sim_ai.git



# Filter by criteria

young_adults = manager.filter_personas(

    age_range=(18, 30),#### Characteristics of Good Questionscd auto_sim_ai#### 3ï¸âƒ£ LLM é›†æˆ

    gender="Female"

)



# Save/loadâœ… **Clear and Specific**```- **æœ¬åœ°éƒ¨ç½²**ï¼šLM Studioï¼ˆå…è´¹ï¼Œå®Œå…¨ç§å¯†ï¼‰

manager.save_to_file("personas.json")

manager.load_from_file("personas.json")

```

```- **å•†ä¸šAPI**ï¼š

### SimulationEngine

Good: "In the past two weeks, how many days have you felt down or depressed?"

```python

from src import SimulationEngineBad: "How have you been feeling lately?"#### 2. Install Dependencies  - DeepSeekï¼ˆé«˜æ€§ä»·æ¯”ï¼Œä¸­æ–‡ä¼˜åŒ–ï¼‰



engine = SimulationEngine(```

    llm_client=client,

    cache=cache,  - OpenAIï¼ˆGPT-4ã€GPT-3.5ï¼‰

    checkpoint_manager=checkpoint_mgr

)âœ… **Avoid Compound Questions**



# Run survey```bash  - å…¶ä»– OpenAI å…¼å®¹æœåŠ¡

result = engine.run_survey(

    personas=personas,```

    questions=questions,

    temperature=0.7,Good: "How many times per week do you exercise?" + "How long is each exercise session?"# Create virtual environment (recommended)- **çµæ´»åˆ‡æ¢**ï¼šéšæ—¶æ›´æ¢æ¨¡å‹æˆ–æä¾›å•†

    max_tokens=300

)Bad: "How often do you exercise, for how long, and at what intensity?"



# Run intervention```python -m venv venv

result = engine.run_intervention(

    personas=personas,

    intervention_text="Health intervention text",

    questions=followup_questionsâœ… **Use Standardized Scales**source venv/bin/activate  # Windows: venv\Scripts\activate#### 4ï¸âƒ£ é«˜çº§åˆ†æ

)

```



### ResultsStorage```- **è‡ªåŠ¨è¯„åˆ†**ï¼šå†…ç½®æ ‡å‡†åŒ–é‡è¡¨çš„è‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿ



```pythonNever(0) - Sometimes(1) - Often(2) - Always(3)

from src import ResultsStorage

```# Install dependencies- **ç»Ÿè®¡åˆ†æ**ï¼šæè¿°ç»Ÿè®¡ã€ç›¸å…³åˆ†æã€ç»„é—´æ¯”è¾ƒ

storage = ResultsStorage()



# Save result

storage.save_result(simulation_result)#### Built-in Templatespip install -r requirements.txt- **ä¸€è‡´æ€§æ£€æŸ¥**ï¼šéªŒè¯å“åº”çš„å†…éƒ¨ä¸€è‡´æ€§å’Œé€»è¾‘æ€§



# Load results

results = storage.load_all_results()

The system includes validated standardized scales:```- **å¯è§†åŒ–**ï¼šäº¤äº’å¼å›¾è¡¨ã€è¯äº‘ã€åˆ†å¸ƒå›¾

# Export to CSV

storage.export_to_csv(result, "output.csv")



# Export analysis script- **PHQ-9**: Depression screening scale- **å¯¼å‡ºåŠŸèƒ½**ï¼šCSVã€JSONã€Python/R åˆ†æè„šæœ¬

storage.export_analysis_script(result, "analysis.py", language="python")

```- **GAD-7**: Anxiety screening scale  



---- **PSS-10**: Perceived Stress ScaleOr use the setup script:



## â“ FAQ- More templates continuously being added...



### Q: How many LLM API calls are needed?#### 5ï¸âƒ£ æ€§èƒ½ä¼˜åŒ–



A: Call count = Number of personas Ã— Number of questions. For example:### Simulation Settings Optimization



- 10 personas Ã— 9 questions = 90 calls```bash- **å¹¶è¡Œæ‰§è¡Œ**ï¼šå¼‚æ­¥å¤„ç†å¤šä¸ªäººç‰©çš„å“åº”

- Using cache can significantly reduce repeat calls

#### Temperature Parameter

### Q: How long does simulation take?

chmod +x setup.sh- **æ™ºèƒ½ç¼“å­˜**ï¼šé¿å…é‡å¤è°ƒç”¨ LLMï¼ŒèŠ‚çœæ—¶é—´å’Œæˆæœ¬

A: Depends on:

Controls response randomness and creativity:

- **Local model**: ~5-15 seconds/response

- **Online API**: ~1-3 seconds/response./setup.sh- **æ–­ç‚¹ç»­ä¼ **ï¼šæ”¯æŒæš‚åœå’Œæ¢å¤å¤§è§„æ¨¡æ¨¡æ‹Ÿ

- **Parallel execution**: Can reduce time by 50-80%

- **0.0 - 0.3**: High consistency, suitable for standardized responses

### Q: How reliable are the results?

- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)```- **è¿›åº¦è¿½è¸ª**ï¼šå®æ—¶æ˜¾ç¤ºæ¨¡æ‹Ÿè¿›åº¦å’Œé¢„ä¼°å®Œæˆæ—¶é—´

A: LLM simulation is an exploratory research tool, suitable for:

- **0.8 - 1.0**: More diverse, suitable for exploratory research

- âœ… Rapid prototype testing

- âœ… Hypothesis generation

- âœ… Questionnaire pre-testing

- âŒ **Cannot** replace real human research#### Max Tokens



### Q: How to improve response quality?#### 3. Configure LLM---



1. Create detailed, realistic persona backgrounds- **150-300**: Short answers (multiple choice, scale ratings)

2. Use clear, specific questions

3. Choose appropriate temperature parameters- **300-500**: Medium length (short answer questions)

4. Use more powerful models (e.g., GPT-4)

5. Enable response validation and consistency checks- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)



### Q: What are the costs?**Option A: Local LM Studio (recommended for learning/development)**## âš¡ å¿«é€Ÿå¼€å§‹



- **Local LM Studio**: Completely free (requires GPU)#### Parallel Settings

- **DeepSeek API**: ~$0.0001/1k tokens, extremely low cost

- **OpenAI GPT-3.5**: ~$0.002/1k tokens

- **OpenAI GPT-4**: ~$0.03/1k tokens

- **Small Scale** (<10 personas): Concurrency 2-3

### Q: Is my data secure?

- **Medium Scale** (10-50 personas): Concurrency 5-101. Download [LM Studio](https://lmstudio.ai/)### ç³»ç»Ÿè¦æ±‚

- Local mode: Data never leaves your machine

- API mode: Follows each provider's privacy policy- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)

- Recommendation: Use local mode for sensitive data

2. Download a model in LM Studio:

---

---

## ğŸ¤ Contributing

   - Recommended: `mistral-7b-instruct`, `llama-2-7b-chat`- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

## ğŸ—ï¸ Architecture

### Development Setup

   - Minimum: 7B parameter model- **å†…å­˜**: å»ºè®® 8GB ä»¥ä¸Š

```bash

# Install development dependenciesFor detailed architecture documentation, see **[Architecture Guide](./docs/en/architecture/README.md)**

pip install -r requirements-dev.txt

3. Start local server:- **LLM æä¾›å•†**ï¼ˆä»»é€‰å…¶ä¸€ï¼‰ï¼š

# Run tests

pytest### Project Structure



# Code formatting   - Click "Local Server" tab  - LM Studioï¼ˆæœ¬åœ°è¿è¡Œï¼Œå…è´¹ï¼‰

black src/ tests/

isort src/ tests/```



# Type checkingauto_sim_ai/   - Select model  - DeepSeek/OpenAI API å¯†é’¥

mypy src/

```â”œâ”€â”€ app.py                      # Streamlit main application



### Report Issuesâ”œâ”€â”€ pages/                      # Multi-page application   - Click "Start Server"



Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).â”‚   â”œâ”€â”€ 1_Setup.py             # Persona management



---â”‚   â”œâ”€â”€ 2_Simulation.py        # Run simulations   - Confirm address is `http://localhost:1234`### å®‰è£…æ­¥éª¤



## ğŸ“„ Licenseâ”‚   â””â”€â”€ 3_Results.py           # View results



This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.â”œâ”€â”€ src/                        # Core modules



---â”‚   â”œâ”€â”€ llm_client.py          # LLM client (sync/async)



## ğŸ™ Acknowledgmentsâ”‚   â”œâ”€â”€ persona.py             # Persona management**Option B: Online API (recommended for production)**#### 1. å…‹éš†é¡¹ç›®



- [Streamlit](https://streamlit.io/) - Excellent Python web frameworkâ”‚   â”œâ”€â”€ simulation.py          # Simulation engine

- [LM Studio](https://lmstudio.ai/) - Local LLM runtime environment

- [OpenAI](https://openai.com/) - API standardsâ”‚   â”œâ”€â”€ storage.py             # Results storage

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM service

â”‚   â”œâ”€â”€ cache.py               # Response caching

---

â”‚   â”œâ”€â”€ scoring.py             # Auto-scoring```bash```bash

## ğŸ“ Contact

â”‚   â””â”€â”€ ...                    # Additional modules

- **Maintainer**: Jason Li

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)â”œâ”€â”€ tests/                      # Test suite# Copy environment templategit clone https://github.com/jason-jj-li/auto_sim_ai.git

- **Email**: [Contact via GitHub Issues]

â”œâ”€â”€ data/                       # Data directory

---

â”‚   â”œâ”€â”€ personas/              # Persona datacp env.example .envcd auto_sim_ai

<div align="center">

â”‚   â”œâ”€â”€ results/               # Simulation results

**â­ If this project helps you, please give it a star!**

â”‚   â””â”€â”€ cache/                 # Cached responses```

Made with â¤ï¸ by Jason Li

â””â”€â”€ docs/                       # Documentation

</div>

```# Edit .env file, add API key



### Core Modules# DEEPSEEK_API_KEY=your_api_key_here#### 2. å®‰è£…ä¾èµ–



**LLM Client** (`llm_client.py`)# or

- Synchronous and asynchronous modes

- Compatible with OpenAI API format# OPENAI_API_KEY=your_api_key_here```bash

- Seamless switching between providers

```# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰

**Simulation Engine** (`simulation.py`)

- Sequential and parallel executionpython -m venv venv

- Automatic error retry and progress tracking

- Result aggregation#### 4. Launch Applicationsource venv/bin/activate  # Windows: venv\Scripts\activate



**Cache System** (`cache.py`)

- Content-hash based smart caching

- Significantly reduce API costs```bash# å®‰è£…ä¾èµ–

- Import/export capability

streamlit run app.pypip install -r requirements.txt

**Scoring System** (`scoring.py`)

- Automated scoring for standardized scales``````

- Configurable custom rules

- Total and subscale calculations



---The app will automatically open in your browser: `http://localhost:8501`æˆ–ä½¿ç”¨å®‰è£…è„šæœ¬ï¼š



## ğŸ”¬ Advanced Features```bash



> ğŸ’¡ **See [API Guide](./docs/en/api/README.md) for detailed documentation**### First-Time User Guidechmod +x setup.sh



### 1. A/B Testing./setup.sh



Compare different intervention versions:1. **Connect LLM** (Home Page)```



```python   - Select LLM provider

from src import ABTestManager, Condition

   - Test connection#### 3. é…ç½® LLM

# Define test conditions

condition_a = Condition(   - Wait for "System Ready" message

    name="Version A",

    intervention_text="Meditating 10 minutes daily can reduce stress.",**æ–¹å¼ Aï¼šæœ¬åœ° LM Studioï¼ˆæ¨èç”¨äºå­¦ä¹ å’Œå¼€å‘ï¼‰**

    questions=["Would you try this method?"]

)2. **Create Virtual Personas** (Setup Page)



condition_b = Condition(   - Click "Create Demo Personas" for quick start1. ä¸‹è½½ [LM Studio](https://lmstudio.ai/)

    name="Version B", 

    intervention_text="Research shows daily 10-minute meditation reduces stress by 30%.",   - Or manually create custom personas2. åœ¨ LM Studio ä¸­ä¸‹è½½æ¨¡å‹ï¼š

    questions=["Would you try this method?"]

)   - Or upload CSV for bulk import   - æ¨èï¼š`mistral-7b-instruct`ã€`llama-2-7b-chat`



# Run A/B test   - æœ€ä½ï¼š7B å‚æ•°æ¨¡å‹

ab_manager = ABTestManager()

results = ab_manager.run_test([condition_a, condition_b], personas)3. **Run Simulation** (Simulation Page)3. å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ï¼š

```

   - Choose simulation type (Survey/Intervention)   - ç‚¹å‡» "Local Server" æ ‡ç­¾

### 2. Longitudinal Studies

   - Select personas to participate   - é€‰æ‹©æ¨¡å‹

Multi-wave tracking with conversation memory:

   - Choose questionnaire template or enter custom questions   - ç‚¹å‡» "Start Server"

```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig   - Click "Run Simulation"   - ç¡®è®¤åœ°å€ä¸º `http://localhost:1234`



# Configure study waves

waves = [

    WaveConfig(4. **View Results** (Results Page)**æ–¹å¼ Bï¼šåœ¨çº¿ APIï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰**

        wave_number=1,

        wave_name="Baseline",   - Browse response data

        questions=["What is your current stress level? (1-10)"],

        days_from_baseline=0   - View statistical analysis```bash

    ),

    WaveConfig(   - Export results for further analysis# å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿

        wave_number=2,

        wave_name="1 Month Follow-up",cp env.example .env

        questions=["What is your stress level now? (1-10)"],

        days_from_baseline=30,---

        intervention_text="Practice 10 minutes of meditation daily"

    )# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œæ·»åŠ  API å¯†é’¥

]

## ğŸ“– User Guide# DEEPSEEK_API_KEY=your_api_key_here

# Run study

config = LongitudinalStudyConfig(# æˆ–

    study_id="stress_study",

    study_name="Stress Intervention Study",### Persona Design Best Practices# OPENAI_API_KEY=your_api_key_here

    waves=waves

)```



engine = LongitudinalStudyEngine(llm_client)#### Creating High-Quality Personas

results = engine.run_study(personas, config)

```#### 4. å¯åŠ¨åº”ç”¨



See **[Longitudinal Study Guide](./docs/en/longitudinal/README.md)** for details.```python



### 3. Batch Persona Generation# Good Example: Specific, detailed, realistic```bash



Generate samples based on real demographic distributions:{streamlit run app.py



```python    "name": "Sarah Chen",```

from src import PersonaGenerator, DistributionConfig

    "age": 32,

# Configure distribution

config = DistributionConfig(    "gender": "Female",åº”ç”¨å°†åœ¨æµè§ˆå™¨ä¸­è‡ªåŠ¨æ‰“å¼€ï¼š`http://localhost:8501`

    age_distribution={"18-30": 0.3, "31-50": 0.4, "51-70": 0.3},

    gender_distribution={"Male": 0.48, "Female": 0.52}    "occupation": "Software Engineer at startup",

)

    "education": "Bachelor's in Computer Science",### é¦–æ¬¡ä½¿ç”¨æŒ‡å—

# Generate 100 personas

generator = PersonaGenerator()    "location": "San Francisco, CA",

personas = generator.generate_batch(

    count=100,    "background": "Works at a fast-growing tech company, often works overtime. Recently experiencing work stress and sleep quality decline. Likes to relieve stress through exercise but often too busy.",1. **è¿æ¥ LLM**ï¼ˆé¦–é¡µï¼‰

    distribution_config=config,

    llm_client=client    "personality_traits": ["Perfectionist", "Strong sense of responsibility", "Somewhat anxious"],   - é€‰æ‹© LLM æä¾›å•†

)

```    "values": ["Career development", "Work-life balance", "Family health"]   - æµ‹è¯•è¿æ¥



### 4. Response Validation}   - ç­‰å¾…"ç³»ç»Ÿå°±ç»ª"æç¤º



Check response quality and consistency:



```python# Bad Example: Vague, generic2. **åˆ›å»ºè™šæ‹Ÿäººç‰©**ï¼ˆSetup é¡µé¢ï¼‰

from src import ResponseValidator, ConsistencyChecker

{   - ç‚¹å‡» "Create Demo Personas" å¿«é€Ÿåˆ›å»º

validator = ResponseValidator()

checker = ConsistencyChecker()    "name": "John Doe",   - æˆ–æ‰‹åŠ¨åˆ›å»ºè‡ªå®šä¹‰äººç‰©



# Validate format    "age": 30,   - æˆ–ä¸Šä¼  CSV æ‰¹é‡å¯¼å…¥

is_valid = validator.validate_response(response, question_type)

    "gender": "Male",

# Check consistency

metrics = checker.check_consistency(persona_responses)    "occupation": "Engineer",3. **è¿è¡Œæ¨¡æ‹Ÿ**ï¼ˆSimulation é¡µé¢ï¼‰

print(f"Consistency score: {metrics.consistency_score}")

```    "background": "Regular person",   - é€‰æ‹©æ¨¡æ‹Ÿç±»å‹ï¼ˆè°ƒæŸ¥/å¹²é¢„ï¼‰



---    "personality_traits": ["Normal"],   - é€‰æ‹©è¦å‚ä¸çš„äººç‰©



## ğŸ“š API Documentation    "values": ["Happiness"]   - é€‰æ‹©é—®å·æ¨¡æ¿æˆ–è¾“å…¥è‡ªå®šä¹‰é—®é¢˜



### PersonaManager}   - ç‚¹å‡» "Run Simulation"



```python```

from src import PersonaManager

4. **æŸ¥çœ‹ç»“æœ**ï¼ˆResults é¡µé¢ï¼‰

manager = PersonaManager()

#### Persona Diversity   - æµè§ˆå“åº”æ•°æ®

# Add persona

manager.add_persona(persona)   - æŸ¥çœ‹ç»Ÿè®¡åˆ†æ



# Get all personasEnsure virtual samples reflect real population diversity:   - å¯¼å‡ºç»“æœç”¨äºè¿›ä¸€æ­¥åˆ†æ

personas = manager.get_all_personas()



# Filter by criteria

young_adults = manager.filter_personas(- **Age**: Cover different age groups (18-80 years)---

    age_range=(18, 30),

    gender="Female"- **Gender**: Male, female, non-binary

)

- **Occupation**: Different industries and position levels## ğŸ“– ä½¿ç”¨æŒ‡å—

# Save/load

manager.save_to_file("personas.json")- **Education**: High school to graduate degrees

manager.load_from_file("personas.json")

```- **Geography**: Urban, rural, different regions### è™šæ‹Ÿäººç‰©è®¾è®¡æœ€ä½³å®è·µ



### SimulationEngine- **Cultural Background**: Different ethnicities, religions, cultural traditions



```python#### åˆ›å»ºé«˜è´¨é‡äººç‰©

from src import SimulationEngine

### Questionnaire Design Tips

engine = SimulationEngine(

    llm_client=client,```python

    cache=cache,

    checkpoint_manager=checkpoint_mgr#### Good Question Characteristics# å¥½çš„ä¾‹å­ï¼šå…·ä½“ã€è¯¦ç»†ã€çœŸå®

)

{

# Run survey

result = engine.run_survey(âœ… **Clear and Specific**    "name": "ææ˜",

    personas=personas,

    questions=questions,    "age": 32,

    temperature=0.7,

    max_tokens=300```    "gender": "ç”·",

)

Good: In the past two weeks, how many days have you felt down or depressed?    "occupation": "åˆåˆ›å…¬å¸è½¯ä»¶å·¥ç¨‹å¸ˆ",

# Run intervention

result = engine.run_intervention(Bad: How have you been feeling lately?    "education": "æœ¬ç§‘è®¡ç®—æœºç§‘å­¦",

    personas=personas,

    intervention_text="Health intervention text",```    "location": "åŒ—äº¬",

    questions=followup_questions

)    "background": "åœ¨ä¸€å®¶å¿«é€Ÿæˆé•¿çš„ç§‘æŠ€å…¬å¸å·¥ä½œï¼Œç»å¸¸åŠ ç­ã€‚æœ€è¿‘æ„Ÿåˆ°å·¥ä½œå‹åŠ›å¤§ï¼Œç¡çœ è´¨é‡ä¸‹é™ã€‚å–œæ¬¢é€šè¿‡è¿åŠ¨ç¼“è§£å‹åŠ›ï¼Œä½†å·¥ä½œç¹å¿™å¸¸å¸¸æ²¡æ—¶é—´ã€‚",

```

âœ… **Avoid Compound Questions**    "personality_traits": ["å®Œç¾ä¸»ä¹‰", "è´£ä»»å¿ƒå¼º", "æœ‰äº›ç„¦è™‘"],

### ResultsStorage

    "values": ["èŒä¸šå‘å±•", "å·¥ä½œç”Ÿæ´»å¹³è¡¡", "å®¶åº­å¥åº·"]

```python

from src import ResultsStorage```}



storage = ResultsStorage()Good: How many times per week do you exercise? How long is each exercise session?



# Save resultBad: How often do you exercise, for how long, and at what intensity?# ä¸å¥½çš„ä¾‹å­ï¼šæ¨¡ç³Šã€ä¸€èˆ¬åŒ–

storage.save_result(simulation_result)

```{

# Load results

results = storage.load_all_results()    "name": "å¼ ä¸‰",



# Export to CSVâœ… **Use Standardized Scales**    "age": 30,

storage.export_to_csv(result, "output.csv")

    "gender": "ç”·",

# Export analysis script

storage.export_analysis_script(result, "analysis.py", language="python")```    "occupation": "å·¥ç¨‹å¸ˆ",

```

Never(0) - Sometimes(1) - Often(2) - Always(3)    "background": "æ™®é€šäºº",

---

```    "personality_traits": ["æ­£å¸¸"],

## â“ FAQ

    "values": ["å¹¸ç¦"]

### How many LLM API calls are needed?

#### Use Built-in Templates}

Call count = Number of personas Ã— Number of questions

```

Example:

- 10 personas Ã— 9 questions = 90 callsThe system includes validated standardized scales:

- Caching significantly reduces repeat calls

#### äººç‰©å¤šæ ·æ€§

### How long does simulation take?

- **PHQ-9**: Depression screening scale

Depends on:

- **Local model**: ~5-15 seconds/response- **GAD-7**: Anxiety screening scaleç¡®ä¿è™šæ‹Ÿæ ·æœ¬åæ˜ çœŸå®äººå£çš„å¤šæ ·æ€§ï¼š

- **Online API**: ~1-3 seconds/response

- **Parallel execution**: Can reduce time by 50-80%- **PSS-10**: Perceived Stress Scale



### How reliable are the results?- More templates continuously being added...- **å¹´é¾„**ï¼šè¦†ç›–ä¸åŒå¹´é¾„æ®µï¼ˆ18-80å²ï¼‰



LLM simulation is an exploratory research tool, suitable for:- **æ€§åˆ«**ï¼šç”·ã€å¥³ã€éäºŒå…ƒæ€§åˆ«



âœ… Rapid prototyping  ### Simulation Settings Optimization- **èŒä¸š**ï¼šä¸åŒè¡Œä¸šå’ŒèŒä½å±‚çº§

âœ… Hypothesis generation  

âœ… Questionnaire pre-testing  - **æ•™è‚²**ï¼šä»é«˜ä¸­åˆ°ç ”ç©¶ç”Ÿ

âŒ **Cannot** replace real human research

#### Temperature Parameter- **åœ°åŸŸ**ï¼šåŸå¸‚ã€å†œæ‘ã€ä¸åŒåœ°åŒº

### How to improve response quality?

- **æ–‡åŒ–èƒŒæ™¯**ï¼šä¸åŒç§æ—ã€å®—æ•™ã€æ–‡åŒ–ä¼ ç»Ÿ

1. Create detailed, realistic persona backgrounds

2. Use clear, specific questionsControls response randomness and creativity:

3. Choose appropriate temperature parameters

4. Use more powerful models (e.g., GPT-4)### é—®å·è®¾è®¡æŠ€å·§

5. Enable response validation and consistency checks

- **0.0 - 0.3**: High consistency, suitable for standardized responses

### What about costs?

- **0.5 - 0.7**: Balanced mode, recommended for most surveys (default)#### å¥½çš„é—®é¢˜ç‰¹å¾

- **Local LM Studio**: Completely free (requires GPU)

- **DeepSeek API**: ~$0.0001/1k tokens (extremely low cost)- **0.8 - 1.0**: More diverse, suitable for exploratory research and creative testing

- **OpenAI GPT-3.5**: ~$0.002/1k tokens

- **OpenAI GPT-4**: ~$0.03/1k tokensâœ… **æ¸…æ™°å…·ä½“**



### Is my data secure?#### Max Tokens```



- **Local mode**: Data never leaves your machineå¥½ï¼šåœ¨è¿‡å»ä¸¤å‘¨å†…ï¼Œæ‚¨æœ‰å¤šå°‘å¤©æ„Ÿåˆ°æƒ…ç»ªä½è½æˆ–æ²®ä¸§ï¼Ÿ

- **API mode**: Follows each provider's privacy policy

- **Recommendation**: Use local mode for sensitive data- **150-300**: Short answers (multiple choice, scale ratings)å·®ï¼šæ‚¨æœ€è¿‘å¿ƒæƒ…æ€ä¹ˆæ ·ï¼Ÿ



---- **300-500**: Medium length (short answer questions)```



## ğŸ¤ Contributing- **500-1000**: Detailed responses (open-ended questions, in-depth interviews)



Contributions welcome! See [Contributing Guide](./docs/en/contributing/README.md) for details.âœ… **é¿å…å¤åˆé—®é¢˜**



### Development Setup#### Parallel Settings```



```bashå¥½ï¼šæ‚¨æ¯å‘¨é”»ç‚¼å¤šå°‘æ¬¡ï¼Ÿæ‚¨æ¯æ¬¡é”»ç‚¼å¤šé•¿æ—¶é—´ï¼Ÿ

# Install development dependencies

pip install -r requirements-dev.txt- **Small Scale** (<10 personas): Concurrency 2-3å·®ï¼šæ‚¨å¤šä¹…é”»ç‚¼ä¸€æ¬¡ï¼Œæ¯æ¬¡å¤šé•¿æ—¶é—´ï¼Œä»€ä¹ˆå¼ºåº¦ï¼Ÿ



# Run tests- **Medium Scale** (10-50 personas): Concurrency 5-10```

pytest

- **Large Scale** (50+ personas): Concurrency 10-15 (watch API rate limits)

# Code formatting

black src/ tests/âœ… **ä½¿ç”¨æ ‡å‡†åŒ–é‡è¡¨**

isort src/ tests/

---```

# Type checking

mypy src/ä»ä¸(0) - å¶å°”(1) - ç»å¸¸(2) - æ€»æ˜¯(3)

```

## ğŸ—ï¸ Architecture```

### Report Issues



Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).

For detailed architecture documentation, see **[Architecture Guide](./docs/en/architecture/README.md)**#### ä½¿ç”¨å†…ç½®æ¨¡æ¿

---



## ğŸ“„ License

### Project Structureç³»ç»Ÿå†…ç½®å¤šä¸ªéªŒè¯è¿‡çš„æ ‡å‡†åŒ–é‡è¡¨ï¼š

This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.



---

```- **PHQ-9**ï¼šæŠ‘éƒç—‡ç­›æŸ¥é‡è¡¨

## ğŸ™ Acknowledgments

auto_sim_ai/- **GAD-7**ï¼šç„¦è™‘ç—‡ç­›æŸ¥é‡è¡¨

- [Streamlit](https://streamlit.io/) - Python web framework

- [LM Studio](https://lmstudio.ai/) - Local LLM runtimeâ”œâ”€â”€ app.py                      # Streamlit main application- **PSS-10**ï¼šå‹åŠ›æ„ŸçŸ¥é‡è¡¨

- [OpenAI](https://openai.com/) - API standards

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM serviceâ”œâ”€â”€ pages/                      # Multi-page application- æ›´å¤šæ¨¡æ¿æŒç»­æ·»åŠ ä¸­...



---â”‚   â”œâ”€â”€ 1_Setup.py             # Persona management page



## ğŸ“ Contactâ”‚   â”œâ”€â”€ 2_Simulation.py        # Simulation execution page### æ¨¡æ‹Ÿè®¾ç½®ä¼˜åŒ–



- **Maintainer**: Jason Liâ”‚   â””â”€â”€ 3_Results.py           # Results analysis page

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)

- **Email**: Contact via GitHub Issuesâ”œâ”€â”€ src/                        # Core modules#### æ¸©åº¦å‚æ•°ï¼ˆTemperatureï¼‰



---â”‚   â”œâ”€â”€ llm_client.py          # LLM client (sync/async)



<div align="center">â”‚   â”œâ”€â”€ persona.py             # Persona managementæ§åˆ¶å“åº”çš„éšæœºæ€§å’Œåˆ›é€ æ€§ï¼š



**â­ If this project helps you, please give it a star!**â”‚   â”œâ”€â”€ simulation.py          # Simulation engine (single/parallel)



Made with â¤ï¸ by Jason Liâ”‚   â”œâ”€â”€ storage.py             # Results storage- **0.0 - 0.3**ï¼šé«˜åº¦ä¸€è‡´ï¼Œé€‚åˆéœ€è¦æ ‡å‡†åŒ–å“åº”çš„åœºæ™¯



</div>â”‚   â”œâ”€â”€ cache.py               # Response cache- **0.5 - 0.7**ï¼šå¹³è¡¡æ¨¡å¼ï¼Œæ¨èç”¨äºå¤§å¤šæ•°è°ƒæŸ¥ï¼ˆé»˜è®¤ï¼‰


â”‚   â”œâ”€â”€ checkpoint.py          # Checkpoint management- **0.8 - 1.0**ï¼šæ›´å¤šæ ·åŒ–ï¼Œé€‚åˆæ¢ç´¢æ€§ç ”ç©¶å’Œåˆ›æ„æµ‹è¯•

â”‚   â”œâ”€â”€ scoring.py             # Auto-scoring

â”‚   â”œâ”€â”€ ab_testing.py          # A/B testing#### æœ€å¤§ä»¤ç‰Œæ•°ï¼ˆMax Tokensï¼‰

â”‚   â”œâ”€â”€ intervention_study.py  # Intervention studies (legacy)

â”‚   â”œâ”€â”€ persona_generator.py   # Persona generator- **150-300**ï¼šç®€çŸ­ç­”æ¡ˆï¼ˆé€‰æ‹©é¢˜ã€é‡è¡¨è¯„åˆ†ï¼‰

â”‚   â”œâ”€â”€ survey_templates.py    # Survey template library- **300-500**ï¼šä¸­ç­‰é•¿åº¦ï¼ˆç®€ç­”é¢˜ï¼‰

â”‚   â”œâ”€â”€ survey_config.py       # Survey configuration- **500-1000**ï¼šè¯¦ç»†å›ç­”ï¼ˆå¼€æ”¾å¼é—®é¢˜ã€æ·±åº¦è®¿è°ˆï¼‰

â”‚   â”œâ”€â”€ tools.py               # Tool registration system

â”‚   â”œâ”€â”€ ui_components.py       # UI components#### å¹¶è¡Œè®¾ç½®

â”‚   â”œâ”€â”€ styles.py              # Design system

â”‚   â””â”€â”€ validators.py          # Input validation- **å°è§„æ¨¡**ï¼ˆ<10äººï¼‰ï¼šå¹¶å‘æ•° 2-3

â”œâ”€â”€ tests/                      # Test suite- **ä¸­ç­‰è§„æ¨¡**ï¼ˆ10-50äººï¼‰ï¼šå¹¶å‘æ•° 5-10

â”œâ”€â”€ data/                       # Data directory- **å¤§è§„æ¨¡**ï¼ˆ50+äººï¼‰ï¼šå¹¶å‘æ•° 10-15ï¼ˆæ³¨æ„APIé€Ÿç‡é™åˆ¶ï¼‰

â”‚   â”œâ”€â”€ personas/              # Persona data

â”‚   â”œâ”€â”€ results/               # Simulation results---

â”‚   â”œâ”€â”€ cache/                 # Cache data

â”‚   â”œâ”€â”€ checkpoints/           # Checkpoints## ğŸ—ï¸ æ¶æ„è®¾è®¡

â”‚   â””â”€â”€ survey_configs/        # Survey configurations

â”œâ”€â”€ docs/                       # Documentationè¯¦ç»†çš„æ¶æ„æ–‡æ¡£è¯·æŸ¥çœ‹ **[Architecture Guide](./docs/architecture/README.md)**

â”œâ”€â”€ requirements.txt            # Dependencies

â””â”€â”€ pytest.ini                 # Test configuration### é¡¹ç›®ç»“æ„

```

```

### Core Module Overviewauto_sim_ai/

â”œâ”€â”€ app.py                      # Streamlit ä¸»åº”ç”¨

#### LLM Client (`llm_client.py`)â”œâ”€â”€ pages/                      # å¤šé¡µé¢åº”ç”¨

â”‚   â”œâ”€â”€ 1_Setup.py             # äººç‰©ç®¡ç†é¡µé¢

Supports both synchronous and asynchronous modes:â”‚   â”œâ”€â”€ 2_Simulation.py        # æ¨¡æ‹Ÿè¿è¡Œé¡µé¢

â”‚   â””â”€â”€ 3_Results.py           # ç»“æœåˆ†æé¡µé¢

- **LMStudioClient**: Sync client, suitable for simple scenariosâ”œâ”€â”€ src/                        # æ ¸å¿ƒæ¨¡å—

- **AsyncLLMClient**: Async client, supports high concurrencyâ”‚   â”œâ”€â”€ llm_client.py          # LLM å®¢æˆ·ç«¯ï¼ˆåŒæ­¥/å¼‚æ­¥ï¼‰

â”‚   â”œâ”€â”€ persona.py             # äººç‰©ç®¡ç†

Compatible with OpenAI API format, seamless switching between providers.â”‚   â”œâ”€â”€ simulation.py          # æ¨¡æ‹Ÿå¼•æ“ï¼ˆå•çº¿ç¨‹/å¹¶è¡Œï¼‰

â”‚   â”œâ”€â”€ storage.py             # ç»“æœå­˜å‚¨

#### Simulation Engine (`simulation.py`)â”‚   â”œâ”€â”€ cache.py               # å“åº”ç¼“å­˜

â”‚   â”œâ”€â”€ checkpoint.py          # æ–­ç‚¹ç®¡ç†

- **SimulationEngine**: Base engine, sequential executionâ”‚   â”œâ”€â”€ scoring.py             # è‡ªåŠ¨è¯„åˆ†

- **ParallelSimulationEngine**: Parallel engine, supports async batch processingâ”‚   â”œâ”€â”€ ab_testing.py          # A/Bæµ‹è¯•

â”‚   â”œâ”€â”€ intervention_study.py  # å¹²é¢„ç ”ç©¶ï¼ˆæ—§ç‰ˆï¼‰

Automatically handles error retry, progress tracking, result aggregation.â”‚   â”œâ”€â”€ longitudinal_study.py  # çºµå‘ç ”ç©¶ï¼ˆæ–°ç‰ˆï¼Œæ¨èï¼‰

â”‚   â”œâ”€â”€ persona_generator.py   # äººç‰©ç”Ÿæˆå™¨

#### Cache System (`cache.py`)â”‚   â”œâ”€â”€ survey_templates.py    # é—®å·æ¨¡æ¿åº“

â”‚   â”œâ”€â”€ survey_config.py       # é—®å·é…ç½®

Content-hash based smart caching:â”‚   â”œâ”€â”€ tools.py               # å·¥å…·æ³¨å†Œç³»ç»Ÿ

â”‚   â”œâ”€â”€ ui_components.py       # UI ç»„ä»¶

- Same persona + same question = directly return cached resultâ”‚   â”œâ”€â”€ styles.py              # è®¾è®¡ç³»ç»Ÿ

- Support cache export and importâ”‚   â””â”€â”€ validators.py          # è¾“å…¥éªŒè¯

- Significantly reduce LLM API call costsâ”œâ”€â”€ tests/                      # æµ‹è¯•å¥—ä»¶

â”œâ”€â”€ data/                       # æ•°æ®ç›®å½•

#### Scoring System (`scoring.py`)â”‚   â”œâ”€â”€ personas/              # äººç‰©æ•°æ®

â”‚   â”œâ”€â”€ results/               # æ¨¡æ‹Ÿç»“æœ

Automated scoring features:â”‚   â”œâ”€â”€ cache/                 # ç¼“å­˜æ•°æ®

â”‚   â”œâ”€â”€ checkpoints/           # æ£€æŸ¥ç‚¹

- Support for multiple standardized scalesâ”‚   â””â”€â”€ survey_configs/        # é—®å·é…ç½®

- Configurable custom scoring rulesâ”œâ”€â”€ docs/                       # æ–‡æ¡£

- Auto-calculate total and subscale scoresâ”œâ”€â”€ requirements.txt            # ä¾èµ–åˆ—è¡¨

â””â”€â”€ pytest.ini                 # æµ‹è¯•é…ç½®

---```



## ğŸ”¬ Advanced Features### æ ¸å¿ƒæ¨¡å—è¯´æ˜



> ğŸ’¡ **Tip**: For detailed API documentation and advanced features, see [API Guide](./docs/en/api/README.md)#### LLM å®¢æˆ·ç«¯ (`llm_client.py`)



### 1. A/B Testingæ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§æ¨¡å¼ï¼š



Compare intervention effects across versions:- **LMStudioClient**ï¼šåŒæ­¥å®¢æˆ·ç«¯ï¼Œé€‚åˆç®€å•åœºæ™¯

- **AsyncLLMClient**ï¼šå¼‚æ­¥å®¢æˆ·ç«¯ï¼Œæ”¯æŒé«˜å¹¶å‘

```python

from src import ABTestManager, Conditionå…¼å®¹ OpenAI API æ ¼å¼ï¼Œå¯æ— ç¼åˆ‡æ¢ä¸åŒæä¾›å•†ã€‚



# Define test conditions#### æ¨¡æ‹Ÿå¼•æ“ (`simulation.py`)

condition_a = Condition(

    name="Version A",- **SimulationEngine**ï¼šåŸºç¡€å¼•æ“ï¼Œé¡ºåºæ‰§è¡Œ

    intervention_text="Meditating 10 minutes daily can reduce stress.",- **ParallelSimulationEngine**ï¼šå¹¶è¡Œå¼•æ“ï¼Œæ”¯æŒå¼‚æ­¥æ‰¹å¤„ç†

    questions=["Would you try this method?"]

)è‡ªåŠ¨å¤„ç†é”™è¯¯é‡è¯•ã€è¿›åº¦è¿½è¸ªã€ç»“æœèšåˆã€‚



condition_b = Condition(#### ç¼“å­˜ç³»ç»Ÿ (`cache.py`)

    name="Version B", 

    intervention_text="Research shows daily 10-minute meditation reduces stress levels by 30%.",åŸºäºå†…å®¹å“ˆå¸Œçš„æ™ºèƒ½ç¼“å­˜ï¼š

    questions=["Would you try this method?"]- ç›¸åŒäººç‰© + ç›¸åŒé—®é¢˜ = ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ

)- æ”¯æŒç¼“å­˜å¯¼å‡ºå’Œå¯¼å…¥

- æ˜¾è‘—é™ä½ LLM API è°ƒç”¨æˆæœ¬

# Run A/B test

ab_manager = ABTestManager()#### è¯„åˆ†ç³»ç»Ÿ (`scoring.py`)

results = ab_manager.run_test([condition_a, condition_b], personas)

```è‡ªåŠ¨åŒ–è¯„åˆ†åŠŸèƒ½ï¼š

- æ”¯æŒå¤šç§æ ‡å‡†åŒ–é‡è¡¨

### 2. Longitudinal Studies (Multi-Wave Tracking)- å¯é…ç½®è‡ªå®šä¹‰è¯„åˆ†è§„åˆ™

- è‡ªåŠ¨è®¡ç®—æ€»åˆ†å’Œå­é‡è¡¨åˆ†æ•°

Implement realistic longitudinal tracking with conversation memory:

---

```python

from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig## ğŸ”¬ é«˜çº§åŠŸèƒ½



# Configure study waves> ğŸ’¡ **æç¤º**: è¯¦ç»†çš„APIæ–‡æ¡£å’Œé«˜çº§åŠŸèƒ½è¯·æŸ¥çœ‹ [API Guide](./docs/api/README.md)

waves = [

    WaveConfig(### 1. A/B æµ‹è¯•

        wave_number=1,

        wave_name="Baseline",æ¯”è¾ƒä¸åŒç‰ˆæœ¬çš„å¹²é¢„æ•ˆæœï¼š

        questions=["What is your current stress level? (1-10)"],

        days_from_baseline=0```python

    ),from src import ABTestManager, Condition

    WaveConfig(

        wave_number=2,# å®šä¹‰æµ‹è¯•æ¡ä»¶

        wave_name="1 Month Follow-up",condition_a = Condition(

        questions=["What is your stress level now? (1-10)"],    name="ç‰ˆæœ¬A",

        days_from_baseline=30,    intervention_text="æ¯å¤©å†¥æƒ³10åˆ†é’Ÿå¯ä»¥é™ä½å‹åŠ›ã€‚",

        intervention_text="Practice 10 minutes of meditation daily"    questions=["æ‚¨ä¼šå°è¯•è¿™ä¸ªæ–¹æ³•å—ï¼Ÿ"]

    ))

]

condition_b = Condition(

# Run longitudinal study    name="ç‰ˆæœ¬B", 

config = LongitudinalStudyConfig(    intervention_text="ç ”ç©¶è¡¨æ˜ï¼Œæ¯å¤©å†¥æƒ³10åˆ†é’Ÿå¯ä»¥é™ä½30%çš„å‹åŠ›æ°´å¹³ã€‚",

    study_id="stress_study",    questions=["æ‚¨ä¼šå°è¯•è¿™ä¸ªæ–¹æ³•å—ï¼Ÿ"]

    study_name="Stress Intervention Study",)

    waves=waves

)# è¿è¡ŒA/Bæµ‹è¯•

ab_manager = ABTestManager()

engine = LongitudinalStudyEngine(llm_client)results = ab_manager.run_test([condition_a, condition_b], personas)

results = engine.run_study(personas, config)```

```

### 2. çºµå‘ç ”ç©¶ï¼ˆå¤šæ³¢æ¬¡è¿½è¸ªï¼‰

For detailed longitudinal study guide, see **[Longitudinal Study Guide](./docs/en/longitudinal/README.md)**

ä½¿ç”¨å¯¹è¯è®°å¿†å®ç°çœŸå®çš„çºµå‘è¿½è¸ªï¼š

### 3. Batch Persona Generation

```python

Generate virtual samples based on real demographic distributions:from src import LongitudinalStudyEngine, WaveConfig, LongitudinalStudyConfig



```python# é…ç½®ç ”ç©¶æ³¢æ¬¡

from src import PersonaGenerator, DistributionConfigwaves = [

    WaveConfig(

# Configure distribution        wave_number=1,

config = DistributionConfig(        wave_name="åŸºçº¿",

    age_distribution={        questions=["æ‚¨ç›®å‰çš„å‹åŠ›æ°´å¹³å¦‚ä½•ï¼Ÿ(1-10)"],

        "18-30": 0.3,        days_from_baseline=0

        "31-50": 0.4,    ),

        "51-70": 0.3    WaveConfig(

    },        wave_number=2,

    gender_distribution={        wave_name="1ä¸ªæœˆå",

        "Male": 0.48,        questions=["æ‚¨ç°åœ¨çš„å‹åŠ›æ°´å¹³å¦‚ä½•ï¼Ÿ(1-10)"],

        "Female": 0.52        days_from_baseline=30,

    }        intervention_text="æ¯å¤©ç»ƒä¹ 10åˆ†é’Ÿå†¥æƒ³"

)    )

]

# Generate 100 personas

generator = PersonaGenerator()# è¿è¡Œçºµå‘ç ”ç©¶

personas = generator.generate_batch(config = LongitudinalStudyConfig(

    count=100,    study_id="stress_study",

    distribution_config=config,    study_name="å‹åŠ›å¹²é¢„ç ”ç©¶",

    llm_client=client    waves=waves

))

```

engine = LongitudinalStudyEngine(llm_client)

### 4. Response Validationresults = engine.run_study(personas, config)

```

Automatically check response quality and consistency:

è¯¦ç»†çš„çºµå‘ç ”ç©¶æŒ‡å—è¯·æŸ¥çœ‹ **[Longitudinal Study Guide](./docs/longitudinal/README.md)**

```python

from src import ResponseValidator, ConsistencyChecker### 3. æ‰¹é‡äººç‰©ç”Ÿæˆ



validator = ResponseValidator()åŸºäºçœŸå®äººå£ç»Ÿè®¡åˆ†å¸ƒç”Ÿæˆè™šæ‹Ÿæ ·æœ¬ï¼š

checker = ConsistencyChecker()

```python

# Validate response formatfrom src import PersonaGenerator, DistributionConfig

is_valid = validator.validate_response(response, question_type)

# é…ç½®åˆ†å¸ƒ

# Check consistencyconfig = DistributionConfig(

metrics = checker.check_consistency(persona_responses)    age_distribution={

print(f"Consistency score: {metrics.consistency_score}")        "18-30": 0.3,

```        "31-50": 0.4,

        "51-70": 0.3

---    },

    gender_distribution={

## ğŸ“š API Documentation        "ç”·": 0.48,

        "å¥³": 0.52

### PersonaManager    }

)

```python

from src import PersonaManager# ç”Ÿæˆ100ä¸ªäººç‰©

generator = PersonaGenerator()

manager = PersonaManager()personas = generator.generate_batch(

    count=100,

# Add persona    distribution_config=config,

manager.add_persona(persona)    llm_client=client

)

# Get all personas```

personas = manager.get_all_personas()

### 5. å“åº”éªŒè¯

# Filter by criteria

young_adults = manager.filter_personas(è‡ªåŠ¨æ£€æŸ¥å“åº”è´¨é‡å’Œä¸€è‡´æ€§ï¼š

    age_range=(18, 30),

    gender="Female"```python

)from src import ResponseValidator, ConsistencyChecker



# Save/loadvalidator = ResponseValidator()

manager.save_to_file("personas.json")checker = ConsistencyChecker()

manager.load_from_file("personas.json")

```# éªŒè¯å“åº”æ ¼å¼

is_valid = validator.validate_response(response, question_type)

### SimulationEngine

# æ£€æŸ¥ä¸€è‡´æ€§

```pythonmetrics = checker.check_consistency(persona_responses)

from src import SimulationEngineprint(f"ä¸€è‡´æ€§å¾—åˆ†: {metrics.consistency_score}")

```

engine = SimulationEngine(

    llm_client=client,---

    cache=cache,

    checkpoint_manager=checkpoint_mgr## ğŸ“š API æ–‡æ¡£

)

### PersonaManager

# Run survey

result = engine.run_survey(```python

    personas=personas,from src import PersonaManager

    questions=questions,

    temperature=0.7,manager = PersonaManager()

    max_tokens=300

)# æ·»åŠ äººç‰©

manager.add_persona(persona)

# Run intervention

result = engine.run_intervention(# è·å–æ‰€æœ‰äººç‰©

    personas=personas,personas = manager.get_all_personas()

    intervention_text="Health intervention text",

    questions=followup_questions# æŒ‰æ¡ä»¶ç­›é€‰

)young_adults = manager.filter_personas(

```    age_range=(18, 30),

    gender="å¥³"

### ResultsStorage)



```python# ä¿å­˜/åŠ è½½

from src import ResultsStoragemanager.save_to_file("personas.json")

manager.load_from_file("personas.json")

storage = ResultsStorage()```



# Save result### SimulationEngine

storage.save_result(simulation_result)

```python

# Load resultsfrom src import SimulationEngine

results = storage.load_all_results()

engine = SimulationEngine(

# Export to CSV    llm_client=client,

storage.export_to_csv(result, "output.csv")    cache=cache,

    checkpoint_manager=checkpoint_mgr

# Export analysis script)

storage.export_analysis_script(result, "analysis.py", language="python")

```# è¿è¡Œè°ƒæŸ¥

result = engine.run_survey(

---    personas=personas,

    questions=questions,

## â“ FAQ    temperature=0.7,

    max_tokens=300

### Q: How many LLM API calls are needed?)



A: Call count = Number of personas Ã— Number of questions. For example:# è¿è¡Œå¹²é¢„

result = engine.run_intervention(

- 10 personas Ã— 9 questions = 90 calls    personas=personas,

- Caching can significantly reduce repeat calls    intervention_text="å¥åº·å¹²é¢„æ–‡æœ¬",

    questions=followup_questions

### Q: How long does simulation take?)

```

A: Depends on:

### ResultsStorage

- **Local model**: ~5-15 seconds/response

- **Online API**: ~1-3 seconds/response```python

- **Parallel execution**: Can reduce time by 50-80%from src import ResultsStorage



### Q: How reliable are the results?storage = ResultsStorage()



A: LLM simulation is an exploratory research tool, suitable for:# ä¿å­˜ç»“æœ

storage.save_result(simulation_result)

- âœ… Rapid prototyping

- âœ… Hypothesis generation# åŠ è½½ç»“æœ

- âœ… Questionnaire pre-testingresults = storage.load_all_results()

- âŒ **Cannot** replace real human research

# å¯¼å‡ºä¸ºCSV

### Q: How to improve response quality?storage.export_to_csv(result, "output.csv")



1. Create detailed, realistic persona backgrounds# å¯¼å‡ºåˆ†æè„šæœ¬

2. Use clear, specific questionsstorage.export_analysis_script(result, "analysis.py", language="python")

3. Choose appropriate temperature parameters```

4. Use more powerful models (e.g., GPT-4)

5. Enable response validation and consistency checks---



### Q: What about costs?## â“ å¸¸è§é—®é¢˜



- **Local LM Studio**: Completely free (requires GPU)### Q: éœ€è¦å¤šå°‘ LLM API è°ƒç”¨ï¼Ÿ

- **DeepSeek API**: ~$0.0001/1k tokens, extremely low cost

- **OpenAI GPT-3.5**: ~$0.002/1k tokensA: è°ƒç”¨æ¬¡æ•° = äººç‰©æ•°é‡ Ã— é—®é¢˜æ•°é‡ã€‚ä¾‹å¦‚ï¼š

- **OpenAI GPT-4**: ~$0.03/1k tokens- 10ä¸ªäººç‰© Ã— 9ä¸ªé—®é¢˜ = 90æ¬¡è°ƒç”¨

- ä½¿ç”¨ç¼“å­˜å¯å¤§å¹…å‡å°‘é‡å¤è°ƒç”¨

### Q: Is my data secure?

### Q: æ¨¡æ‹Ÿéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

- Local mode: Data never leaves your machine

- API mode: Follows each provider's privacy policyA: å–å†³äºï¼š

- Recommendation: Use local mode for sensitive data- **æœ¬åœ°æ¨¡å‹**ï¼šçº¦ 5-15 ç§’/å“åº”

- **åœ¨çº¿API**ï¼šçº¦ 1-3 ç§’/å“åº”

---- **å¹¶è¡Œæ‰§è¡Œ**ï¼šå¯ç¼©çŸ­ 50-80% æ—¶é—´



## ğŸ¤ Contributing### Q: ç»“æœçš„å¯é æ€§å¦‚ä½•ï¼Ÿ



Contributions welcome! See [CONTRIBUTING.md](./docs/en/contributing/README.md) for details.A: LLMæ¨¡æ‹Ÿæ˜¯æ¢ç´¢æ€§ç ”ç©¶å·¥å…·ï¼Œé€‚åˆï¼š

- âœ… å¿«é€ŸåŸå‹æµ‹è¯•

### Development Setup- âœ… å‡è®¾ç”Ÿæˆ

- âœ… é—®å·é¢„æµ‹è¯•

```bash- âŒ **ä¸èƒ½**æ›¿ä»£çœŸå®äººç±»ç ”ç©¶

# Install development dependencies

pip install -r requirements-dev.txt### Q: å¦‚ä½•æé«˜å“åº”è´¨é‡ï¼Ÿ



# Run tests1. åˆ›å»ºè¯¦ç»†ã€çœŸå®çš„äººç‰©èƒŒæ™¯

pytest2. ä½¿ç”¨æ¸…æ™°ã€å…·ä½“çš„é—®é¢˜

3. é€‰æ‹©åˆé€‚çš„æ¸©åº¦å‚æ•°

# Code formatting4. ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰

black src/ tests/5. å¯ç”¨å“åº”éªŒè¯å’Œä¸€è‡´æ€§æ£€æŸ¥

isort src/ tests/

### Q: æˆæœ¬å¦‚ä½•ï¼Ÿ

# Type checking

mypy src/- **æœ¬åœ°LM Studio**ï¼šå®Œå…¨å…è´¹ï¼ˆéœ€è¦GPUï¼‰

```- **DeepSeek API**ï¼š~0.001å…ƒ/åƒtokenï¼Œæä½æˆæœ¬

- **OpenAI GPT-3.5**ï¼š~0.015å…ƒ/åƒtoken

### Report Issues- **OpenAI GPT-4**ï¼š~0.3å…ƒ/åƒtoken



Found a bug or have a feature suggestion? Please [create an issue](https://github.com/jason-jj-li/auto_sim_ai/issues).### Q: æ•°æ®å®‰å…¨å—ï¼Ÿ



---- æœ¬åœ°æ¨¡å¼ï¼šæ•°æ®å®Œå…¨ä¸å‡ºæœ¬åœ°

- APIæ¨¡å¼ï¼šéµå¾ªå„æä¾›å•†çš„éšç§æ”¿ç­–

## ğŸ“„ License- å»ºè®®ï¼šæ•æ„Ÿæ•°æ®ä½¿ç”¨æœ¬åœ°æ¨¡å¼



This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.---



---## ğŸ¤ è´¡çŒ®



## ğŸ™ Acknowledgmentsæ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦æƒ…ã€‚



- [Streamlit](https://streamlit.io/) - Excellent Python web framework### å¼€å‘è®¾ç½®

- [LM Studio](https://lmstudio.ai/) - Local LLM runtime environment

- [OpenAI](https://openai.com/) - API standards```bash

- [DeepSeek](https://www.deepseek.com/) - Cost-effective LLM service# å®‰è£…å¼€å‘ä¾èµ–

pip install -r requirements-dev.txt

---

# è¿è¡Œæµ‹è¯•

## ğŸ“ Contactpytest



- **Maintainer**: Jason Li# ä»£ç æ ¼å¼åŒ–

- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)black src/ tests/

- **Email**: [Contact via GitHub Issues]isort src/ tests/



---# ç±»å‹æ£€æŸ¥

mypy src/

<div align="center">```



**â­ If this project helps you, please give it a star!**### æŠ¥å‘Šé—®é¢˜



Made with â¤ï¸ by Jason Liå‘ç° Bug æˆ–æœ‰åŠŸèƒ½å»ºè®®ï¼Ÿè¯·[åˆ›å»º Issue](https://github.com/jason-jj-li/auto_sim_ai/issues)ã€‚



</div>---


## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ™ è‡´è°¢

- [Streamlit](https://streamlit.io/) - ä¼˜ç§€çš„Python Webæ¡†æ¶
- [LM Studio](https://lmstudio.ai/) - æœ¬åœ°LLMè¿è¡Œç¯å¢ƒ
- [OpenAI](https://openai.com/) - APIæ ‡å‡†
- [DeepSeek](https://www.deepseek.com/) - é«˜æ€§ä»·æ¯”LLMæœåŠ¡

---

## ğŸ“ è”ç³»æ–¹å¼

- **ç»´æŠ¤è€…**: Jason Li
- **GitHub**: [@jason-jj-li](https://github.com/jason-jj-li)
- **Email**: [é€šè¿‡GitHub Issuesè”ç³»]

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸€ä¸ªæ˜Ÿæ ‡ï¼**

Made with â¤ï¸ by Jason Li

</div>
